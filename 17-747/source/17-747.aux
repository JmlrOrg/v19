\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{el2012bayesian}
\citation{rezende2015variational}
\citation{parno2014transport,han2017stein}
\citation{kingma2013auto,goodfellow2014generative}
\citation{tabak2013family,dinh2016density}
\citation{blei2016variational}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\citation{meng2002warp}
\citation{santambrogio2015optimal}
\citation{villani2008optimal}
\citation{douglas1999applications,kantorovich1965best}
\citation{el2012bayesian}
\citation{anderes2012general,heng2015gibbs}
\citation{rezende2015variational,liu2016stein}
\citation{lauritzen1996graphical}
\citation{rezende2015variational,detommaso2018stein}
\citation{morrison2017beyond}
\citation{spantini2017inference}
\@writefile{toc}{\contentsline {section}{\numberline {2}Notation}{3}{section.3}}
\newlabel{sec:notation}{{2}{3}{}{section.3}{}}
\citation{fremlin2000measure}
\citation{rosenblatt1952remarks}
\citation{carlier2010knothe}
\newlabel{eq:smoothPushPull}{{1}{4}{}{equation.4}{}}
\MT@newlabel{eq:smoothPushPull}
\MT@newlabel{eq:smoothPushPull}
\newlabel{eq:cond_density}{{2}{4}{}{equation.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Triangular Transport Maps: a Building Block}{4}{section.6}}
\newlabel{sec:compTransport}{{3}{4}{}{section.6}{}}
\citation{santambrogio2015optimal}
\citation{bogachev2005triangular}
\citation{santambrogio2015optimal}
\citation{marzouk2016introduction}
\newlabel{eq:lowerTri}{{3}{5}{}{section.6}{}}
\newlabel{eq:componentMap}{{3}{5}{}{equation.7}{}}
\MT@newlabel{eq:componentMap}
\newlabel{lem:changeVarTriWeak}{{1}{5}{}{theorem.8}{}}
\newlabel{eq:pullbackDensTri}{{4}{5}{}{equation.9}{}}
\MT@newlabel{eq:pullbackDensTri}
\citation{ramsay1998estimating}
\citation{Shapiro2013,kushner2003stochastic}
\citation{dick2013high}
\citation{asmussen2007stochastic}
\citation{glynn1990likelihood}
\citation{wright1999numerical}
\citation{el2012bayesian}
\newlabel{eq:monotone}{{5}{6}{}{equation.11}{}}
\newlabel{OptimDirect}{{6}{6}{}{equation.12}{}}
\MT@newlabel{OptimDirect}
\MT@newlabel{OptimDirect}
\MT@newlabel{OptimDirect}
\MT@newlabel{eq:monotone}
\MT@newlabel{OptimDirect}
\MT@newlabel{OptimDirect}
\MT@newlabel{OptimDirect}
\newlabel{OptimDirectSAA}{{7}{6}{}{equation.13}{}}
\citation{parno2015transport,parno2014transport}
\citation{tabak2013family,Csillery2010}
\citation{rezende2015variational,bigoni2016monotone,mendoza2018bayesian}
\MT@newlabel{OptimDirectSAA}
\MT@newlabel{OptimDirectSAA}
\newlabel{eq:var_diag}{{8}{7}{}{equation.14}{}}
\newlabel{eq:normConst}{{9}{7}{}{equation.15}{}}
\MT@newlabel{eq:componentMap}
\MT@newlabel{OptimDirect}
\newlabel{eq:pushforDensTri}{{3}{7}{}{equation.15}{}}
\citation{xiu2010numerical}
\citation{xiu2010numerical}
\citation{koller2009probabilistic}
\MT@newlabel{OptimDirect}
\MT@newlabel{eq:monotone}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:movieToyProblem}{{1}{8}{}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\hspace {9pt} $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle T$}\mathaccent "0365{T}_0$}}}{8}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\hspace {9pt} $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle T$}\mathaccent "0365{T}_1$}}}{8}{subfigure.1.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {\hspace {9pt} $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle T$}\mathaccent "0365{T}_2$}}}{8}{subfigure.1.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {\hspace {9pt} $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle T$}\mathaccent "0365{T}_3$}}}{8}{subfigure.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Markov Networks}{8}{section.21}}
\newlabel{sec:markov}{{4}{8}{}{section.21}{}}
\citation{lauritzen1996graphical}
\citation{lauritzen1996graphical}
\citation{hammersley1971markov,lauritzen1996graphical}
\citation{koller2009probabilistic,rue2005gaussian}
\newlabel{eq:globalMarkov}{{10}{9}{}{equation.23}{}}
\MT@newlabel{eq:globalMarkov}
\newlabel{eq:factHammer}{{11}{9}{}{equation.24}{}}
\MT@newlabel{eq:factHammer}
\citation{koller2009probabilistic}
\citation{hyvarinen2005estimation,meinshausen2006high,lin2015high}
\citation{koller2009probabilistic}
\newlabel{lem:pairwiseIndep}{{2}{10}{Pairwise conditional independence}{theorem.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Sparsity of Triangular Transport Maps}{10}{section.27}}
\newlabel{sec:sparse}{{5}{10}{}{section.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Sparsity Bounds}{10}{subsection.28}}
\newlabel{sec:sparsityBounds}{{5.1}{10}{}{subsection.28}{}}
\MT@newlabel{OptimDirect}
\citation{koller2009probabilistic}
\citation{villani2008optimal}
\newlabel{thm:sparsityRosenblatt}{{3}{11}{Sparsity of Knothe--Rosenblatt rearrangements}{theorem.30}{}}
\newlabel{thm:sparsityRosenblatt:inverse}{{1}{11}{Sparsity of Knothe--Rosenblatt rearrangements}{Item.31}{}}
\newlabel{eq:superSetInverse}{{12}{11}{Sparsity of Knothe--Rosenblatt rearrangements}{equation.32}{}}
\newlabel{thm:sparsityRosenblatt:direct}{{2}{11}{Sparsity of Knothe--Rosenblatt rearrangements}{Item.33}{}}
\newlabel{eq:superSetDirect}{{13}{11}{Sparsity of Knothe--Rosenblatt rearrangements}{equation.34}{}}
\newlabel{thm:sparsityRosenblatt:inclusion}{{3}{11}{Sparsity of Knothe--Rosenblatt rearrangements}{Item.35}{}}
\newlabel{eq:inclusion}{{14}{11}{Sparsity of Knothe--Rosenblatt rearrangements}{equation.36}{}}
\citation{rue2005gaussian}
\citation{kim1998stochastic}
\citation{kim1998stochastic}
\citation{rue2005gaussian}
\MT@newlabel{eq:superSetInverse}
\MT@newlabel{eq:superSetDirect}
\MT@newlabel{eq:inclusion}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Connection to Gaussian Markov Random Fields}{12}{subsection.48}}
\newlabel{sec_gmrf}{{5.2}{12}{}{subsection.48}{}}
\citation{rue2005gaussian}
\newlabel{fig:marginalGraph}{{2}{13}{}{figure.caption.37}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\bm {\mathcal {G}}^5$}}}{13}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\bm {\mathcal {G}}^4$}}}{13}{subfigure.2.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\bm {\mathcal {G}}^3$}}}{13}{subfigure.2.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\bm {\mathcal {G}}^2$}}}{13}{subfigure.2.4}}
\MT@newlabel{eq:superSetInverse}
\newlabel{fig:sparsityPatt}{{3}{13}{}{figure.caption.42}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {${\bf P}_{jk}=\partial _{j}S^k$}}}{13}{subfigure.3.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {${\bf P}_{jk}=\partial _{j}T^k$}}}{13}{subfigure.3.2}}
\newlabel{fig:sparsityVolHyper}{{4}{14}{}{figure.caption.45}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {${\bf P}_{jk}=\partial _{j}S^k$}}}{14}{subfigure.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Ordering of Triangular Maps}{14}{subsection.50}}
\newlabel{sec_order}{{5.3}{14}{}{subsection.50}{}}
\citation{george1989evolution,saad2003iterative}
\citation{yannakakis1981computing}
\citation{koller2009probabilistic}
\newlabel{bestOrdering}{{15}{15}{}{equation.51}{}}
\MT@newlabel{bestOrdering}
\MT@newlabel{bestOrdering}
\MT@newlabel{eq:superSetInverse}
\MT@newlabel{bestOrdering}
\newlabel{bestOrderingGraph}{{16}{15}{}{equation.52}{}}
\MT@newlabel{bestOrderingGraph}
\MT@newlabel{bestOrderingGraph}
\MT@newlabel{bestOrderingGraph}
\MT@newlabel{bestOrderingGraph}
\@writefile{toc}{\contentsline {section}{\numberline {6}Decomposability of Transport Maps}{15}{section.58}}
\newlabel{sec:decomp}{{6}{15}{}{section.58}{}}
\newlabel{fig:example_badOrder}{{5}{16}{}{figure.caption.53}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\bm {\mathcal {G}}$}}}{16}{subfigure.5.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {${\bf P}_{ij}=\partial _{i}S^j$}}}{16}{subfigure.5.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\bm {\mathcal {G}}'$}}}{16}{subfigure.5.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {${\bf P}_{ij}'=\partial _{i}S^j$}}}{16}{subfigure.5.4}}
\newlabel{def:lowdim}{{4}{16}{Low-dimensional map with respect to a set}{theorem.59}{}}
\citation{lauritzen1996graphical}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Preliminary Notions}{17}{subsection.62}}
\newlabel{sec:prelnotions}{{6.1}{17}{}{subsection.62}{}}
\newlabel{def:graphDec}{{5}{17}{Proper graph decomposition}{theorem.63}{}}
\newlabel{def:genTri}{{6}{17}{Generalized triangular function}{theorem.64}{}}
\newlabel{eq:genTriMap}{{6.1}{17}{}{theorem.64}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Decomposition and Graph Sparsification}{18}{subsection.65}}
\newlabel{sec:decompMainResults}{{6.2}{18}{}{subsection.65}{}}
\newlabel{thm:decompTrans}{{7}{18}{Decomposition of transport maps}{theorem.66}{}}
\newlabel{thm:decompTrans_partFactorizationPi}{{1}{18}{Decomposition of transport maps}{Item.67}{}}
\newlabel{eq:formFactoriz}{{17}{18}{Decomposition of transport maps}{equation.68}{}}
\newlabel{thm:decompTrans_partT}{{2}{18}{Decomposition of transport maps}{Item.69}{}}
\MT@newlabel{eq:formFactoriz}
\newlabel{eq:formPerm}{{18}{18}{Decomposition of transport maps}{equation.70}{}}
\newlabel{thm:decompTrans_partLeftMap}{{2a}{18}{Decomposition of transport maps}{Item.71}{}}
\newlabel{thm:decompTrans_partRightMap}{{2b}{18}{Decomposition of transport maps}{Item.72}{}}
\newlabel{thm:decompTrans_partRVimap}{{2c}{18}{Decomposition of transport maps}{Item.73}{}}
\newlabel{graphSparsification_partImap}{{2d}{18}{Decomposition of transport maps}{Item.74}{}}
\newlabel{eq:sparsityPatternGenTriMaps}{{19}{19}{}{equation.77}{}}
\MT@newlabel{eq:sparsityPatternGenTriMaps}
\MT@newlabel{eq:sparsityPatternGenTriMaps}
\MT@newlabel{eq:formPerm}
\MT@newlabel{eq:formPerm}
\MT@newlabel{eq:sparsityPatternGenTriMaps}
\MT@newlabel{eq:formPerm}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Recursive Decompositions}{20}{subsection.79}}
\newlabel{sec:rec_dec}{{6.3}{20}{}{subsection.79}{}}
\newlabel{lem:recursiveDecomp}{{8}{20}{Recursive decompositions}{theorem.80}{}}
\newlabel{lem:recursiveDecomp_partPos}{{1}{20}{Recursive decompositions}{Item.82}{}}
\newlabel{lem:recursiveDecomp_partInclusion}{{1a}{20}{Recursive decompositions}{Item.83}{}}
\newlabel{lem:recursiveDecomp_lmap}{{1b}{20}{Recursive decompositions}{Item.84}{}}
\newlabel{lem:recursiveDecomp_rmap}{{1c}{20}{Recursive decompositions}{Item.85}{}}
\MT@newlabel{eq:formFactoriz}
\newlabel{lem:recursiveDecomp_partNeg}{{2}{21}{Recursive decompositions}{Item.86}{}}
\newlabel{eq:longerInclusion}{{20}{21}{}{equation.87}{}}
\newlabel{eq:longerComposition}{{21}{21}{}{equation.88}{}}
\MT@newlabel{eq:longerInclusion}
\MT@newlabel{eq:longerComposition}
\citation{bertsekas1995dynamic}
\citation{koller2009probabilistic}
\citation{lauritzen1996graphical}
\citation{rezende2015variational}
\MT@newlabel{eq:longerComposition}
\MT@newlabel{eq:longerInclusion}
\citation{lauritzen1996graphical}
\citation{lauritzen1996graphical}
\citation{reich2015probabilistic}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Computation of Decomposable Transports}{23}{subsection.89}}
\newlabel{sec:compDecompTrans}{{6.4}{23}{}{subsection.89}{}}
\MT@newlabel{eq:longerComposition}
\MT@newlabel{OptimDirect}
\MT@newlabel{eq:formFactoriz}
\MT@newlabel{eq:formFactoriz}
\MT@newlabel{eq:formFactoriz}
\@writefile{toc}{\contentsline {section}{\numberline {7}Sequential Inference on State-Space Models: Variational Algorithms}{23}{section.96}}
\newlabel{sec:dataAss}{{7}{23}{}{section.96}{}}
\newlabel{fig:graphSparsification}{{6}{24}{}{figure.caption.91}{}}
\citation{oksendal2013stochastic}
\citation{sarkka2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Markov structure of a typical state-space model}}{25}{figure.caption.97}}
\newlabel{fig:dataAssSmoothing}{{7}{25}{Markov structure of a typical state-space model}{figure.caption.97}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Smoothing and Filtering: the Full Bayesian Solution}{25}{subsection.99}}
\newlabel{sec:filt}{{7.1}{25}{}{subsection.99}{}}
\newlabel{eq:dataAssBayes}{{22}{25}{}{equation.100}{}}
\citation{sarkka2013bayesian}
\citation{doucet2009tutorial}
\MT@newlabel{eq:dataAssBayes}
\newlabel{thm:decompSmooth}{{9}{26}{Decomposition theorem for state-space models}{theorem.101}{}}
\newlabel{eq:thm:decompSmooth:upperTriMap}{{23}{26}{Decomposition theorem for state-space models}{equation.102}{}}
\newlabel{thm:decompSmooth:partFilt}{{1}{26}{Decomposition theorem for state-space models}{Item.103}{}}
\newlabel{thm:decompSmooth:partSmot}{{2}{26}{Decomposition theorem for state-space models}{Item.104}{}}
\newlabel{eq:thm:decompSmooth:upperTriMapSmooth}{{24}{26}{Decomposition theorem for state-space models}{equation.105}{}}
\citation{marzouk2016introduction}
\newlabel{thm:decompSmooth:partFull}{{3}{27}{Decomposition theorem for state-space models}{Item.106}{}}
\newlabel{eq:thm:decompSmooth:upperTriMapEmb}{{25}{27}{Decomposition theorem for state-space models}{equation.107}{}}
\newlabel{thm:decompSmooth:partEvidence}{{4}{27}{Decomposition theorem for state-space models}{Item.108}{}}
\newlabel{thm:decompSmooth:evidence}{{26}{27}{Decomposition theorem for state-space models}{equation.109}{}}
\MT@newlabel{eq:thm:decompSmooth:upperTriMapEmb}
\MT@newlabel{eq:thm:decompSmooth:upperTriMap}
\citation{kitagawa1987non,doucet2009tutorial,godsill2012monte}
\citation{evensen2003ensemble,evensen2000ensemble}
\citation{raanes2016ensemble}
\newlabel{eq:backward_cond}{{27}{28}{}{equation.110}{}}
\MT@newlabel{eq:backward_cond}
\citation{spantini2017inference}
\citation{crisan2002survey,del2004feynman,smith2013sequential}
\MT@newlabel{OptimDirectSAA}
\citation{rauch1965maximum}
\citation{bierman2006factorization}
\MT@newlabel{eq:normConst}
\MT@newlabel{thm:decompSmooth:evidence}
\newlabel{cor:genDecompThmChains}{{10}{30}{}{theorem.111}{}}
\MT@newlabel{eq:thm:decompSmooth:upperTriMap}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}The Linear Gaussian Case: Connection with the RTS Smoother}{30}{subsection.112}}
\newlabel{sec:smoothLinGauss}{{7.2}{30}{}{subsection.112}{}}
\newlabel{eq:linearSubMap}{{28}{30}{}{equation.113}{}}
\newlabel{lem:kalmanRec}{{11}{31}{The linear Gaussian case}{theorem.114}{}}
\MT@newlabel{eq:linearSubMap}
\newlabel{eq:recursionKalm}{{29}{31}{The linear Gaussian case}{equation.115}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Sequential Joint Parameter and State Estimation}{31}{subsection.116}}
\newlabel{sec:joint}{{7.3}{31}{}{subsection.116}{}}
\newlabel{eq:dataAssBayesJoint}{{30}{31}{}{equation.117}{}}
\MT@newlabel{eq:dataAssBayesJoint}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Markov structure of a typical state-space model with static parameters}}{31}{figure.caption.118}}
\newlabel{fig:dataAssHyper}{{8}{31}{Markov structure of a typical state-space model with static parameters}{figure.caption.118}{}}
\citation{andrieu2010particle}
\newlabel{thm:decompJoint}{{12}{32}{Decomposition theorem for joint parameter and state estimation}{theorem.120}{}}
\newlabel{eq:thm:decompJoint:genTriMap}{{31}{32}{Decomposition theorem for joint parameter and state estimation}{equation.121}{}}
\newlabel{eq:target_stepmap_0}{{32}{32}{Decomposition theorem for joint parameter and state estimation}{equation.122}{}}
\newlabel{eq:target_stepmap}{{33}{32}{Decomposition theorem for joint parameter and state estimation}{equation.123}{}}
\newlabel{thm:decompJoint:partFilt}{{1}{32}{Decomposition theorem for joint parameter and state estimation}{Item.124}{}}
\newlabel{eq:thm:decompJoint:TriMapFilt}{{34}{32}{Decomposition theorem for joint parameter and state estimation}{equation.125}{}}
\newlabel{thm:decompJoint:partFull}{{2}{32}{Decomposition theorem for joint parameter and state estimation}{Item.126}{}}
\newlabel{eq:thm:decompJoint:genTriMapEmb}{{35}{32}{Decomposition theorem for joint parameter and state estimation}{equation.127}{}}
\citation{kitagawa1998self,liu2001combined}
\citation{polson2008practical}
\citation{jacob2015sequential}
\citation{chopin2013smc2,crisan2013nested,del2017biased}
\citation{kantas2015particle}
\citation{erol2017nearly}
\newlabel{thm:decompJoint:partEvidence}{{3}{33}{Decomposition theorem for joint parameter and state estimation}{Item.128}{}}
\MT@newlabel{thm:decompSmooth:evidence}
\MT@newlabel{eq:thm:decompJoint:genTriMapEmb}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Fixed-Point Smoothing}{33}{subsection.129}}
\newlabel{sec:margSmooth}{{7.4}{33}{}{subsection.129}{}}
\citation{morrison2017beyond}
\citation{bigoni2016monotone}
\citation{kim1998stochastic,rue2009approximate}
\citation{rue2009approximate,durbin2000time}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Online fixed-point smoothing}}{34}{figure.caption.130}}
\newlabel{fig:dataAssMargSmoothing}{{9}{34}{Online fixed-point smoothing}{figure.caption.130}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Numerical Illustration}{34}{section.132}}
\newlabel{sec:numerics}{{8}{34}{}{section.132}{}}
\citation{wright1999numerical}
\citation{robert2013monte}
\citation{jacob2015sequential}
\MT@newlabel{OptimDirect}
\MT@newlabel{eq:monotone}
\MT@newlabel{OptimDirect}
\MT@newlabel{eq:var_diag}
\MT@newlabel{eq:var_diag}
\MT@newlabel{eq:var_diag}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Comparison between the $\{5,95\}$--percentiles (dashed lines) and the mean (solid line) of the numerical approximation of the smoothing marginals $\pi _{ \bm  {Z}_{k} \delimiter "026A30C \bm  {y}_{0:N}}$ via transport maps (red lines) versus a ``reference'' MCMC solution (black lines), for $k=0,\ldots  ,N$. The two solutions are indistinguishable.\relax }}{37}{figure.caption.133}}
\newlabel{fig:stoc-vol:smoothing-vs-unbiased}{{10}{37}{Comparison between the $\{5,95\}$--percentiles (dashed lines) and the mean (solid line) of the numerical approximation of the smoothing marginals $\pi _{ \Zb _{k} \vert \yb _{0:N}}$ via transport maps (red lines) versus a ``reference'' MCMC solution (black lines), for $k=0,\ldots ,N$. The two solutions are indistinguishable.\relax }{figure.caption.133}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces At each time $k$, we illustrate the $\{5,95\}$--percentiles (dotted lines) and the mean (solid line) of the numerical approximation of the filtering distribution $\pi _{ \bm  {Z}_k \delimiter "026A30C \bm  {y}_{0:k}}$. \relax }}{38}{figure.caption.134}}
\newlabel{fig:stoc-vol:filtering}{{11}{38}{At each time $k$, we illustrate the $\{5,95\}$--percentiles (dotted lines) and the mean (solid line) of the numerical approximation of the filtering distribution $\pi _{ \Zb _k \vert \yb _{0:k}}$. \relax }{figure.caption.134}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \emph  {(Horizontal plane)} At each time $k$, we illustrate the $\{5,25,40,60,75,95\}$--percentiles (shaded regions) and the mean (solid line) of the numerical approximation of $\pi _{ \bm  {\phi }\delimiter "026A30C \bm  {y}_{0:k}}$, the posterior marginal of the static parameter $\bm  {\phi }$. \emph  {(Vertical axis)} At several times $k$ we also compare the transport map numerical approximation of $\pi _{ \bm  {\phi }\delimiter "026A30C \bm  {y}_{0:k}}$ (solid lines) with a reference MCMC solution (dashed lines). The two distributions agree remarkably well.\relax }}{38}{figure.caption.135}}
\newlabel{fig:stoc-vol:3d-phi-vs-unbiased}{{12}{38}{\emph {(Horizontal plane)} At each time $k$, we illustrate the $\{5,25,40,60,75,95\}$--percentiles (shaded regions) and the mean (solid line) of the numerical approximation of $\pi _{ \phib \vert \yb _{0:k}}$, the posterior marginal of the static parameter $\phib $. \emph {(Vertical axis)} At several times $k$ we also compare the transport map numerical approximation of $\pi _{ \phib \vert \yb _{0:k}}$ (solid lines) with a reference MCMC solution (dashed lines). The two distributions agree remarkably well.\relax }{figure.caption.135}{}}
\MT@newlabel{eq:var_diag}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Computational effort required to compute a decomposable transport map for different complexities of the transformations $\mathfrak  {M}_k$---linear versus degree seven---and for different inference scenarios---smoothing and static parameter estimation {\it  (top row)} or long-time smoothing without static parameters {\it  (bottom row)}, for the stochastic volatility model of Section \ref  {sec:numerics}. The last column reports the variance diagnostic \MT_extended_eqref:n  {eq:var_diag} for the corresponding \emph  {joint} posterior, not just a few marginals. It highlights a tradeoff between cost and accuracy, typical of the transport map approach to variational inference. For comparison, we also report the cost and accuracy of a simple Laplace approximation, which requires no formal optimization. \relax }}{39}{table.caption.136}}
\MT@newlabel{eq:var_diag}
\newlabel{tab:cost_vs_accuracy}{{1}{39}{Computational effort required to compute a decomposable transport map for different complexities of the transformations $\submap _k$---linear versus degree seven---and for different inference scenarios---smoothing and static parameter estimation {\it (top row)} or long-time smoothing without static parameters {\it (bottom row)}, for the stochastic volatility model of Section \ref {sec:numerics}. The last column reports the variance diagnostic \eqref {eq:var_diag} for the corresponding \emph {joint} posterior, not just a few marginals. It highlights a tradeoff between cost and accuracy, typical of the transport map approach to variational inference. For comparison, we also report the cost and accuracy of a simple Laplace approximation, which requires no formal optimization. \relax }{table.caption.136}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Same as Figure \ref  {fig:stoc-vol:3d-phi-vs-unbiased}, but for the static parameter $\bm  {\mu }$. \relax }}{39}{figure.caption.137}}
\newlabel{fig:stoc-vol:3d-mu-vs-unbiased}{{13}{39}{Same as Figure \ref {fig:stoc-vol:3d-phi-vs-unbiased}, but for the static parameter $\mub $. \relax }{figure.caption.137}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces  Shaded regions represent the $\{5,25,40,60,75,95\}$--percentiles of the marginals of the posterior predictive distribution (conditioning on all the data), along with black dots that represent the observed data $(\bm  {y}_k)_{k=0}^N$. \relax }}{40}{figure.caption.138}}
\newlabel{fig:stoc-vol:post-pred}{{14}{40}{Shaded regions represent the $\{5,25,40,60,75,95\}$--percentiles of the marginals of the posterior predictive distribution (conditioning on all the data), along with black dots that represent the observed data $(\yb _k)_{k=0}^N$. \relax }{figure.caption.138}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  Randomly chosen two-dimensional conditionals of the pullback of $\pi _{\bm  {\Theta }, \bm  {Z}_{0:N} \delimiter "026A30C \bm  {y}_{0:N}}$ through the numerical approximation of $\mathfrak  {T}_{N-1}$. Since we use a standard normal reference distribution, the numerical approximation of $\mathfrak  {T}_{N-1}$ should be deemed satisfactory if the pullback density is close to a standard normal, as it is here. \relax }}{40}{figure.caption.139}}
\newlabel{fig:stoc-vol:pullback}{{15}{40}{Randomly chosen two-dimensional conditionals of the pullback of $\pi _{\vhyp , \Zb _{0:N} \vert \yb _{0:N}}$ through the numerical approximation of $\mathfrak {T}_{N-1}$. Since we use a standard normal reference distribution, the numerical approximation of $\mathfrak {T}_{N-1}$ should be deemed satisfactory if the pullback density is close to a standard normal, as it is here. \relax }{figure.caption.139}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Comparison between the $\{5,95\}$--percentiles (dashed lines) and the mean (solid line) of the transport map numerical approximation of the smoothing marginals $\pi _{ \bm  {Z}_{k} \delimiter "026A30C \bm  {\theta }^*, \bm  {y}_{0:2500}}$, with $\bm  {\theta }^* = {\rm  med}[ \Theta \delimiter "026A30C \bm  {y}_0 ,\ldots  , \bm  {y}_{N} ]$ (red lines), and a reference MCMC solution (black lines). The two solutions are indistinguishable.\relax }}{41}{figure.caption.140}}
\newlabel{fig:stoc-vol:long-smooth-vs-unbiased}{{16}{41}{Comparison between the $\{5,95\}$--percentiles (dashed lines) and the mean (solid line) of the transport map numerical approximation of the smoothing marginals $\pi _{ \Zb _{k} \vert \thetab ^*, \yb _{0:2500}}$, with $\thetab ^* = {\rm med}[ \Theta \vert \yb _0 ,\ldots , \yb _{N} ]$ (red lines), and a reference MCMC solution (black lines). The two solutions are indistinguishable.\relax }{figure.caption.140}{}}
\newlabel{RF1}{42}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Same as Figure \ref  {fig:stoc-vol:long-smooth-vs-unbiased}, but for a longer assimilation window, i.e., $\pi _{ \bm  {Z}_{k} \delimiter "026A30C \bm  {\theta }^*, \bm  {y}_{0:9000}}$. The smoothing approximation remains excellent despite the widening inference horizon. \relax }}{42}{figure.caption.141}}
\newlabel{fig:stoc-vol:very-long-smooth-vs-unbiased}{{17}{42}{Same as Figure \ref {fig:stoc-vol:long-smooth-vs-unbiased}, but for a longer assimilation window, i.e., $\pi _{ \Zb _{k} \vert \thetab ^*, \yb _{0:9000}}$. The smoothing approximation remains excellent despite the widening inference horizon. \relax }{figure.caption.141}{}}
\citation{marsaglia2000ziggurat}
\citation{el2012bayesian,schillings2014scaling,rezende2015variational}
\citation{meng2002warp}
\citation{laparra2011iterative,anderes2012general,stavropoulou2015parametrization}
\citation{parno2014transport,Bardsley2014,oliver2015metropolized}
\citation{parno2015multiscale}
\citation{daum2008particle,chorin2009implicit,reich2013nonparametric}
\citation{villani2008optimal}
\citation{tabak2013family,rezende2015variational,liu2016stein,bigoni2016monotone}
\citation{rue2005gaussian}
\citation{parno2015transport}
\citation{morrison2017beyond}
\citation{spantini2017inference}
\citation{gaspari1999construction,hamill2001distance}
\@writefile{toc}{\contentsline {section}{\numberline {9}Discussion}{43}{section.142}}
\newlabel{sec:discus}{{9}{43}{}{section.142}{}}
\citation{rezende2015variational}
\citation{anderes2012general,liu2016stein,detommaso2018stein}
\citation{tabak2013family,laparra2011iterative}
\citation{johnson2008recursive,jog2015model,cheng2015efficient}
\citation{bigoni2016monotone}
\citation{daum2008particle,liu2016stein}
\citation{sarkka2013bayesian,evensen2007data}
\citation{crisan2002survey,del2004feynman,smith2013sequential}
\citation{parno2014transport}
\citation{heng2015gibbs}
\citation{daum2012particle,yang2013feedback}
\citation{heng2015gibbs}
\citation{chorin2009implicit}
\citation{Morzfeld2012}
\citation{reich2013nonparametric}
\citation{reich2013nonparametric}
\MT@newlabel{eq:var_diag}
\citation{meng2002warp,wang2016warp}
\citation{stuart2010inverse}
\citation{spantini2014optimal,cui2014likelihood,spantini2016goal}
\citation{samarov1993exploring,constantine2014active}
\citation{rosenblatt1952remarks}
\citation{bogachev2005triangular}
\@writefile{toc}{\contentsline {section}{\numberline {A}Generalized Knothe-Rosenblatt Rearrangement}{46}{section.143}}
\newlabel{sec:genKR}{{A}{46}{}{section.143}{}}
\citation{santambrogio2015optimal}
\citation{bogachev2005triangular}
\citation{bogachev2005triangular}
\citation{bogachev2005triangular}
\citation{bogachev2005triangular}
\newlabel{def:increasRearr}{{13}{47}{Increasing rearrangement on $\re $}{theorem.144}{}}
\newlabel{def:KRrearr}{{14}{47}{Knothe-Rosenblatt rearrangement}{theorem.145}{}}
\MT@newlabel{eq:cond_density}
\MT@newlabel{eq:componentMap}
\newlabel{lem:GenchangeVarTriWeak}{{15}{47}{}{theorem.146}{}}
\newlabel{eq:GenpullbackDensTri}{{36}{47}{}{equation.147}{}}
\MT@newlabel{eq:GenpullbackDensTri}
\citation{lauritzen1996graphical}
\citation{lauritzen1996graphical}
\newlabel{def:genKRrearr}{{16}{48}{Generalized Knothe-Rosenblatt rearrangement}{theorem.148}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proofs of the Main Results}{48}{section.151}}
\newlabel{sec:proofs}{{B}{48}{}{section.151}{}}
\citation{lauritzen1996graphical}
\newlabel{lem:productTarget}{{18}{49}{}{theorem.152}{}}
\newlabel{lem:productTarget_partI}{{1}{49}{}{Item.153}{}}
\newlabel{lem:productTarget_partII}{{2}{49}{}{Item.154}{}}
\citation{lauritzen1996graphical}
\newlabel{eq:factPiDecomp}{{37}{50}{}{equation.155}{}}
\MT@newlabel{eq:factPiDecomp}
\newlabel{eq:factPsi}{{38}{50}{}{equation.156}{}}
\MT@newlabel{eq:factPsi}
\MT@newlabel{eq:formPerm}
\newlabel{eq:factPullback}{{39}{51}{}{equation.157}{}}
\MT@newlabel{eq:factPsi}
\MT@newlabel{eq:factPullback}
\MT@newlabel{eq:factPullback}
\newlabel{cor:lowdim_lmap}{{19}{51}{}{theorem.158}{}}
\MT@newlabel{eq:formPerm}
\MT@newlabel{eq:formFactoriz}
\MT@newlabel{eq:formPerm}
\MT@newlabel{eq:formFactoriz}
\MT@newlabel{eq:thm:decompSmooth:upperTriMap}
\newlabel{thm:decompSmooth:c0}{{40}{53}{}{equation.159}{}}
\newlabel{thm:decompSmooth:ck}{{41}{53}{}{equation.160}{}}
\MT@newlabel{thm:decompSmooth:ck}
\MT@newlabel{eq:thm:decompSmooth:upperTriMapEmb}
\MT@newlabel{eq:thm:decompSmooth:upperTriMapEmb}
\MT@newlabel{thm:decompSmooth:ck}
\newlabel{thm:decompSmooth:pullbacks}{{B}{54}{}{equation.160}{}}
\MT@newlabel{eq:thm:decompSmooth:upperTriMapSmooth}
\newlabel{thm:decompSmooth:data}{{42}{54}{}{equation.161}{}}
\MT@newlabel{thm:decompSmooth:c0}
\MT@newlabel{thm:decompSmooth:ck}
\citation{koller2009probabilistic}
\MT@newlabel{eq:recursionKalm}
\MT@newlabel{eq:thm:decompJoint:genTriMap}
\MT@newlabel{eq:thm:decompJoint:TriMapFilt}
\newlabel{thm:decompJoint:ck}{{43}{56}{}{equation.162}{}}
\newlabel{thm:decompJoint:changeVar}{{44}{56}{}{equation.163}{}}
\MT@newlabel{thm:decompJoint:changeVar}
\MT@newlabel{thm:decompJoint:changeVar}
\MT@newlabel{thm:decompJoint:ck}
\MT@newlabel{eq:thm:decompJoint:genTriMapEmb}
\MT@newlabel{eq:thm:decompJoint:genTriMapEmb}
\MT@newlabel{thm:decompJoint:ck}
\MT@newlabel{thm:decompJoint:ck}
\MT@newlabel{thm:decompSmooth:data}
\@writefile{toc}{\contentsline {section}{\numberline {C}Algorithms for Inference on State-Space Models}{58}{section.164}}
\newlabel{sec:algo}{{C}{58}{}{section.164}{}}
\MT@newlabel{eq:monotone}
\MT@newlabel{OptimDirectSAA}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces  {\bf  (Computation of a monotone map)}   Given an unnormalized target density $\mathaccentV {bar}016{\pi }$ and a parametric triangular monotone map $T[{\bf  c}]$ of the form \MT_extended_eqref:n  {eq:monotone}, defined by an arbitrary set of coefficients ${\bf  c}\in \mathbb  {R}^N$, find the optimal coefficients ${\bf  c}^\star $ according to \MT_extended_eqref:n  {OptimDirectSAA}. \relax }}{58}{algorithm.165}}
\newlabel{alg:computeMap}{{1}{58}{{\bf (Computation of a monotone map)} \\ Given an unnormalized target density $\bar {\pi }$ and a parametric triangular monotone map $T[{\bf c}]$ of the form \eqref {eq:monotone}, defined by an arbitrary set of coefficients ${\bf c}\in \mathbb {R}^N$, find the optimal coefficients ${\bf c}^\star $ according to \eqref {OptimDirectSAA}. \relax }{algorithm.165}{}}
\MT@newlabel{eq:monotone}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  {\bf  (Regression of a monotone map)}   Given a map $M$ and a parametric triangular monotone map $T[{\bf  c}]$ of the form \MT_extended_eqref:n  {eq:monotone} , defined by an arbitrary set of coefficients ${\bf  c}\in \mathbb  {R}^N$, find the coefficients ${\bf  c}^\star $ minimizing the discrete $L^2$ norm between the two maps. \relax }}{58}{algorithm.166}}
\newlabel{alg:regresMap}{{2}{58}{{\bf (Regression of a monotone map)} \\ Given a map $M$ and a parametric triangular monotone map $T[{\bf c}]$ of the form \eqref {eq:monotone} , defined by an arbitrary set of coefficients ${\bf c}\in \mathbb {R}^N$, find the coefficients ${\bf c}^\star $ minimizing the discrete $L^2$ norm between the two maps. \relax }{algorithm.166}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces  {\bf  (Joint parameter and state inference)}   Given observations $({\bm  {y}}_i)_{i=0}^{k+1}$, construct a transport map approximation of the smoothing distribution $\pi _{\bm  {\Theta },\bm  {Z}_{0},\ldots  , \bm  {Z}_{k+1} \delimiter "026A30C \bm  {y}_{0},\ldots  , \bm  {y}_{k+1} }$ in terms of a list of maps $(\frak  {M}_j)_{j=0}^{k}$. \relax }}{59}{algorithm.167}}
\newlabel{alg:assimilate}{{3}{59}{{\bf (Joint parameter and state inference)} \\ Given observations $({\bm y}_i)_{i=0}^{k+1}$, construct a transport map approximation of the smoothing distribution $\pi _{\vhyp ,\Zb _{0},\ldots , \Zb _{k+1} \vert \yb _{0},\ldots , \yb _{k+1} }$ in terms of a list of maps $(\frak {M}_j)_{j=0}^{k}$. \relax }{algorithm.167}{}}
\MT@newlabel{eq:target_stepmap_0}
\MT@newlabel{eq:target_stepmap}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces  {\bf  (Sample the smoothing distribution)}   Generate a sample from the smoothing distribution $\pi _{\bm  {\Theta },\bm  {Z}_{0},\ldots  , \bm  {Z}_{k+1} \delimiter "026A30C \bm  {y}_{0},\ldots  , \bm  {y}_{k+1} }$using the maps computed in Algorithm \ref  {alg:assimilate}. \relax }}{59}{algorithm.168}}
\newlabel{alg:SampleSmoothing}{{4}{59}{{\bf (Sample the smoothing distribution)} \\ Generate a sample from the smoothing distribution $\pi _{\vhyp ,\Zb _{0},\ldots , \Zb _{k+1} \vert \yb _{0},\ldots , \yb _{k+1} }$using the maps computed in Algorithm \ref {alg:assimilate}. \relax }{algorithm.168}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces  {\bf  (Sample the filtering distribution)}   Generate a sample from the marginal distribution $\pi _{ \bm  {\Theta },\bm  {Z}_{k+1} \delimiter "026A30C \bm  {y}_{0}, \ldots  , \bm  {y}_{k+1}}$ using the maps computed in Algorithm \ref  {alg:assimilate}. \relax }}{60}{algorithm.169}}
\newlabel{alg:SampleFiltering}{{5}{60}{{\bf (Sample the filtering distribution)} \\ Generate a sample from the marginal distribution $\pi _{ \vhyp ,\Zb _{k+1} \vert \yb _{0}, \ldots , \yb _{k+1}}$ using the maps computed in Algorithm \ref {alg:assimilate}. \relax }{algorithm.169}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Additional Results for the Stochastic Volatility Model}{60}{section.170}}
\newlabel{sec:add_res}{{D}{60}{}{section.170}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  Same as Figure \ref  {fig:stoc-vol:smoothing-vs-unbiased}, but using linear maps. Compared to a high-order map, there seems to be only a minimal loss of accuracy, more prominent at earlier times. \relax }}{60}{figure.caption.171}}
\newlabel{fig:stoc-vol:smoothing-vs-unbiased-linear}{{18}{60}{Same as Figure \ref {fig:stoc-vol:smoothing-vs-unbiased}, but using linear maps. Compared to a high-order map, there seems to be only a minimal loss of accuracy, more prominent at earlier times. \relax }{figure.caption.171}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  Comparison of the $\{5,95\}$--percentiles (dashed lines) and the mean (solid line) of the numerical approximation of the filtering marginals using {\it  linear} transport maps (blue lines) with those of a ``reference'' solution obtained via seventh-order maps (as shown in Figure \ref  {fig:stoc-vol:filtering}). The two solutions look remarkably similar despite the enormous difference in computational cost (see Table \ref  {tab:cost_vs_accuracy}). \relax }}{61}{figure.caption.172}}
\newlabel{fig:stoc-vol:filtering-vs-unbiased-linear}{{19}{61}{Comparison of the $\{5,95\}$--percentiles (dashed lines) and the mean (solid line) of the numerical approximation of the filtering marginals using {\it linear} transport maps (blue lines) with those of a ``reference'' solution obtained via seventh-order maps (as shown in Figure \ref {fig:stoc-vol:filtering}). The two solutions look remarkably similar despite the enormous difference in computational cost (see Table \ref {tab:cost_vs_accuracy}). \relax }{figure.caption.172}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  Same as Figure \ref  {fig:stoc-vol:3d-phi-vs-unbiased}, but using linear maps. Here, the loss of accuracy is more dramatic than for the smoothing distribution of the state in Figure \ref  {fig:stoc-vol:smoothing-vs-unbiased-linear}. Even though the approximate marginal captures the bulk of the true parameter marginals, for this specific problem of static parameter inference, a linear map is largely inadequate; hence the need for a higher-order nonlinear transformation. \relax }}{61}{figure.caption.173}}
\newlabel{fig:stoc-vol:3d-phi-vs-unbiased-linear}{{20}{61}{Same as Figure \ref {fig:stoc-vol:3d-phi-vs-unbiased}, but using linear maps. Here, the loss of accuracy is more dramatic than for the smoothing distribution of the state in Figure \ref {fig:stoc-vol:smoothing-vs-unbiased-linear}. Even though the approximate marginal captures the bulk of the true parameter marginals, for this specific problem of static parameter inference, a linear map is largely inadequate; hence the need for a higher-order nonlinear transformation. \relax }{figure.caption.173}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces The horizontal plane of Figure \ref  {fig:stoc-vol:3d-phi-vs-unbiased-linear} ({\it  black lines}) overlaid with a selected number of box-and-whisker plots associated with the marginals of a ``reference'' MCMC solution. The ends of the whiskers represent the $\{5,95\}$--percentiles, while the green dots correspond to the means of the reference distribution. Linear maps are insufficient to correctly characterize the parameter marginals, especially the transition at time 74 (cf.\ Figures \ref  {fig:stoc-vol:3d-phi-vs-unbiased} and \ref  {fig:stoc-vol:3d-phi-vs-unbiased-linear})  \relax }}{62}{figure.caption.174}}
\newlabel{fig:stoc-vol:2d-phi-vs-unbiased-linear}{{21}{62}{The horizontal plane of Figure \ref {fig:stoc-vol:3d-phi-vs-unbiased-linear} ({\it black lines}) overlaid with a selected number of box-and-whisker plots associated with the marginals of a ``reference'' MCMC solution. The ends of the whiskers represent the $\{5,95\}$--percentiles, while the green dots correspond to the means of the reference distribution. Linear maps are insufficient to correctly characterize the parameter marginals, especially the transition at time 74 (cf.\ Figures \ref {fig:stoc-vol:3d-phi-vs-unbiased} and \ref {fig:stoc-vol:3d-phi-vs-unbiased-linear})  \relax }{figure.caption.174}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  Same as Figure \ref  {fig:stoc-vol:3d-mu-vs-unbiased}, but using linear maps. Once again, the linear map provides plausible, but somewhat inaccurate, results for sequential parameter inference. A nonlinear transformation is better suited for this problem. \relax }}{62}{figure.caption.175}}
\newlabel{fig:stoc-vol:3d-mu-vs-unbiased-linear}{{22}{62}{Same as Figure \ref {fig:stoc-vol:3d-mu-vs-unbiased}, but using linear maps. Once again, the linear map provides plausible, but somewhat inaccurate, results for sequential parameter inference. A nonlinear transformation is better suited for this problem. \relax }{figure.caption.175}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  The horizontal plane of Figure \ref  {fig:stoc-vol:3d-mu-vs-unbiased-linear} ({\it  black lines}) overlaid with a selected number of box-and-whisker plots associated with the marginals of a ``reference'' MCMC solution. See Figure \ref  {fig:stoc-vol:2d-phi-vs-unbiased-linear} caption for more details. \relax }}{63}{figure.caption.176}}
\newlabel{fig:stoc-vol:2d-mu-vs-unbiased-linear}{{23}{63}{The horizontal plane of Figure \ref {fig:stoc-vol:3d-mu-vs-unbiased-linear} ({\it black lines}) overlaid with a selected number of box-and-whisker plots associated with the marginals of a ``reference'' MCMC solution. See Figure \ref {fig:stoc-vol:2d-phi-vs-unbiased-linear} caption for more details. \relax }{figure.caption.176}{}}
\newlabel{RF2}{64}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces  Same as Figure \ref  {fig:stoc-vol:very-long-smooth-vs-unbiased}, but using linear maps. Long-time smoothing with no static parameters via linear maps yields accurate characterizations of the marginal distributions across all times, at a fraction of the cost of a high-order nonlinear transformation (see Table \ref  {tab:cost_vs_accuracy}). \relax }}{64}{figure.caption.177}}
\newlabel{fig:stoc-vol:very-long-smooth-vs-unbiased-linear}{{24}{64}{Same as Figure \ref {fig:stoc-vol:very-long-smooth-vs-unbiased}, but using linear maps. Long-time smoothing with no static parameters via linear maps yields accurate characterizations of the marginal distributions across all times, at a fraction of the cost of a high-order nonlinear transformation (see Table \ref {tab:cost_vs_accuracy}). \relax }{figure.caption.177}{}}
\bibdata{mapBib}
\bibcite{anderes2012general}{{1}{2012}{{Anderes and Coram}}{{}}}
\bibcite{andrieu2010particle}{{2}{2010}{{Andrieu et~al.}}{{Andrieu, Doucet, and Holenstein}}}
\bibcite{asmussen2007stochastic}{{3}{2007}{{Asmussen and Glynn}}{{}}}
\bibcite{Bardsley2014}{{4}{2014}{{Bardsley et~al.}}{{Bardsley, Solonen, Haario, and Laine}}}
\bibcite{bertsekas1995dynamic}{{5}{1995}{{Bertsekas}}{{}}}
\bibcite{bierman2006factorization}{{6}{2006}{{Bierman}}{{}}}
\bibcite{bigoni2016monotone}{{7}{2019}{{Bigoni et~al.}}{{Bigoni, Spantini, and Marzouk}}}
\bibcite{blei2016variational}{{8}{2016}{{Blei et~al.}}{{Blei, Kucukelbir, and McAuliffe}}}
\bibcite{bogachev2005triangular}{{9}{2005}{{Bogachev et~al.}}{{Bogachev, Kolesnikov, and Medvedev}}}
\bibcite{carlier2010knothe}{{10}{2010}{{Carlier et~al.}}{{Carlier, Galichon, and Santambrogio}}}
\bibcite{cheng2015efficient}{{11}{2015}{{Cheng et~al.}}{{Cheng, Cheng, Liu, Peng, and Teng}}}
\bibcite{chopin2013smc2}{{12}{2013}{{Chopin et~al.}}{{Chopin, Jacob, and Papaspiliopoulos}}}
\bibcite{chorin2009implicit}{{13}{2009}{{Chorin and Tu}}{{}}}
\bibcite{constantine2014active}{{14}{2014}{{Constantine et~al.}}{{Constantine, Dow, and Wang}}}
\bibcite{crisan2002survey}{{15}{2002}{{Crisan and Doucet}}{{}}}
\bibcite{crisan2013nested}{{16}{2013}{{Crisan and Miguez}}{{}}}
\bibcite{Csillery2010}{{17}{2010}{{Csill\'{e}ry et~al.}}{{Csill\'{e}ry, Blum, Gaggiotti, and Fran\c {c}ois}}}
\bibcite{cui2014likelihood}{{18}{2014}{{Cui et~al.}}{{Cui, Martin, Marzouk, Solonen, and Spantini}}}
\bibcite{daum2008particle}{{19}{2008}{{Daum and Huang}}{{}}}
\bibcite{daum2012particle}{{20}{2012}{{Daum and Huang}}{{}}}
\bibcite{del2004feynman}{{21}{2004}{{Del~Moral}}{{}}}
\bibcite{del2017biased}{{22}{2017}{{Del~Moral et~al.}}{{Del~Moral, Jasra, and Zhou}}}
\bibcite{detommaso2018stein}{{23}{2018}{{Detommaso et~al.}}{{Detommaso, Cui, Marzouk, Scheichl, and Spantini}}}
\bibcite{dick2013high}{{24}{2013}{{Dick et~al.}}{{Dick, Kuo, and Sloan}}}
\bibcite{dinh2016density}{{25}{2016}{{Dinh et~al.}}{{Dinh, Sohl-Dickstein, and Bengio}}}
\bibcite{doucet2009tutorial}{{26}{2009}{{Doucet and Johansen}}{{}}}
\bibcite{douglas1999applications}{{27}{1999}{{Douglas}}{{}}}
\bibcite{durbin2000time}{{28}{2000}{{Durbin and Koopman}}{{}}}
\bibcite{erol2017nearly}{{29}{2017}{{Erol et~al.}}{{Erol, Wu, Li, and Russell}}}
\bibcite{evensen2003ensemble}{{30}{2003}{{Evensen}}{{}}}
\bibcite{evensen2007data}{{31}{2007}{{Evensen}}{{}}}
\bibcite{evensen2000ensemble}{{32}{2000}{{Evensen and Van~Leeuwen}}{{}}}
\bibcite{fremlin2000measure}{{33}{2000}{{Fremlin}}{{}}}
\bibcite{gaspari1999construction}{{34}{1999}{{Gaspari and Cohn}}{{}}}
\bibcite{george1989evolution}{{35}{1989}{{George and Liu}}{{}}}
\bibcite{glynn1990likelihood}{{36}{1990}{{Glynn}}{{}}}
\bibcite{godsill2012monte}{{37}{2004}{{Godsill et~al.}}{{Godsill, Doucet, and West}}}
\bibcite{goodfellow2014generative}{{38}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{hamill2001distance}{{39}{2001}{{Hamill et~al.}}{{Hamill, Whitaker, and Snyder}}}
\bibcite{hammersley1971markov}{{40}{1971}{{Hammersley and Clifford}}{{}}}
\bibcite{han2017stein}{{41}{2017}{{Han and Liu}}{{}}}
\bibcite{heng2015gibbs}{{42}{2015}{{Heng et~al.}}{{Heng, Doucet, and Pokern}}}
\bibcite{hyvarinen2005estimation}{{43}{2005}{{Hyv{\"a}rinen}}{{}}}
\bibcite{jacob2015sequential}{{44}{2015}{{Jacob}}{{}}}
\bibcite{jog2015model}{{45}{2015}{{Jog and Loh}}{{}}}
\bibcite{johnson2008recursive}{{46}{2008}{{Johnson and Willsky}}{{}}}
\bibcite{kantas2015particle}{{47}{2015}{{Kantas et~al.}}{{Kantas, Doucet, Singh, Maciejowski, and Chopin}}}
\bibcite{kantorovich1965best}{{48}{1965}{{Kantorovich}}{{}}}
\bibcite{kim1998stochastic}{{49}{1998}{{Kim et~al.}}{{Kim, Shephard, and Chib}}}
\bibcite{kingma2013auto}{{50}{2013}{{Kingma and Welling}}{{}}}
\bibcite{kitagawa1987non}{{51}{1987}{{Kitagawa}}{{}}}
\bibcite{kitagawa1998self}{{52}{1998}{{Kitagawa}}{{}}}
\bibcite{koller2009probabilistic}{{53}{2009}{{Koller and Friedman}}{{}}}
\bibcite{kushner2003stochastic}{{54}{2003}{{Kushner and Yin}}{{}}}
\bibcite{laparra2011iterative}{{55}{2011}{{Laparra et~al.}}{{Laparra, Camps-Valls, and Malo}}}
\bibcite{lauritzen1996graphical}{{56}{1996}{{Lauritzen}}{{}}}
\bibcite{lin2015high}{{57}{2015}{{Lin et~al.}}{{Lin, Drton, and Shojaie}}}
\bibcite{liu2001combined}{{58}{2001}{{Liu and West}}{{}}}
\bibcite{liu2016stein}{{59}{2016}{{Liu and Wang}}{{}}}
\bibcite{marsaglia2000ziggurat}{{60}{2000}{{Marsaglia and Tsang}}{{}}}
\bibcite{marzouk2016introduction}{{61}{2016}{{Marzouk et~al.}}{{Marzouk, Moselhy, Parno, and Spantini}}}
\bibcite{meinshausen2006high}{{62}{2006}{{Meinshausen and B{\"u}hlmann}}{{}}}
\bibcite{mendoza2018bayesian}{{63}{2018}{{Mendoza et~al.}}{{Mendoza, Allegra, and Coleman}}}
\bibcite{meng2002warp}{{64}{2002}{{Meng and Schilling}}{{}}}
\bibcite{morrison2017beyond}{{65}{2017}{{Morrison et~al.}}{{Morrison, Baptista, and Marzouk}}}
\bibcite{Morzfeld2012}{{66}{2012}{{Morzfeld et~al.}}{{Morzfeld, Tu, Atkins, and Chorin}}}
\bibcite{el2012bayesian}{{67}{2012}{{Moselhy and Marzouk}}{{}}}
\bibcite{oksendal2013stochastic}{{68}{2013}{{Oksendal}}{{}}}
\bibcite{oliver2015metropolized}{{69}{2015}{{Oliver}}{{}}}
\bibcite{parno2015transport}{{70}{2015}{{Parno}}{{}}}
\bibcite{parno2014transport}{{71}{2018}{{Parno and Marzouk}}{{}}}
\bibcite{parno2015multiscale}{{72}{2016}{{Parno et~al.}}{{Parno, Moselhy, and Marzouk}}}
\bibcite{polson2008practical}{{73}{2008}{{Polson et~al.}}{{Polson, Stroud, and M{\"u}ller}}}
\bibcite{raanes2016ensemble}{{74}{2016}{{Raanes}}{{}}}
\bibcite{ramsay1998estimating}{{75}{1998}{{Ramsay}}{{}}}
\bibcite{rauch1965maximum}{{76}{1965}{{Rauch et~al.}}{{Rauch, Striebel, and Tung}}}
\bibcite{reich2013nonparametric}{{77}{2013}{{Reich}}{{}}}
\bibcite{reich2015probabilistic}{{78}{2015}{{Reich and Cotter}}{{}}}
\bibcite{rezende2015variational}{{79}{2015}{{Rezende and Mohamed}}{{}}}
\bibcite{robert2013monte}{{80}{2013}{{Robert and Casella}}{{}}}
\bibcite{rosenblatt1952remarks}{{81}{1952}{{Rosenblatt}}{{}}}
\bibcite{rue2005gaussian}{{82}{2005}{{Rue and Held}}{{}}}
\bibcite{rue2009approximate}{{83}{2009}{{Rue et~al.}}{{Rue, Martino, and Chopin}}}
\bibcite{saad2003iterative}{{84}{2003}{{Saad}}{{}}}
\bibcite{samarov1993exploring}{{85}{1993}{{Samarov}}{{}}}
\bibcite{santambrogio2015optimal}{{86}{2015}{{Santambrogio}}{{}}}
\bibcite{sarkka2013bayesian}{{87}{2013}{{S{\"a}rkk{\"a}}}{{}}}
\bibcite{schillings2014scaling}{{88}{2016}{{Schillings and Schwab}}{{}}}
\bibcite{Shapiro2013}{{89}{2013}{{Shapiro}}{{}}}
\bibcite{smith2013sequential}{{90}{2013}{{Smith et~al.}}{{Smith, Doucet, de~Freitas, and Gordon}}}
\bibcite{spantini2017inference}{{91}{2017}{{Spantini}}{{}}}
\bibcite{spantini2014optimal}{{92}{2015}{{Spantini et~al.}}{{Spantini, Solonen, Cui, Martin, Tenorio, and Marzouk}}}
\bibcite{spantini2016goal}{{93}{2017}{{Spantini et~al.}}{{Spantini, Cui, Willcox, Tenorio, and Marzouk}}}
\bibcite{stavropoulou2015parametrization}{{94}{2015}{{Stavropoulou and M{\"u}ller}}{{}}}
\bibcite{stuart2010inverse}{{95}{2010}{{Stuart}}{{}}}
\bibcite{tabak2013family}{{96}{2013}{{Tabak and Turner}}{{}}}
\bibcite{villani2008optimal}{{97}{2008}{{Villani}}{{}}}
\bibcite{wang2016warp}{{98}{2016}{{Wang and Meng}}{{}}}
\bibcite{wright1999numerical}{{99}{1999}{{Wright and Nocedal}}{{}}}
\bibcite{xiu2010numerical}{{100}{2010}{{Xiu}}{{}}}
\bibcite{yang2013feedback}{{101}{2013}{{Yang et~al.}}{{Yang, Mehta, and Meyn}}}
\bibcite{yannakakis1981computing}{{102}{1981}{{Yannakakis}}{{}}}
\newlabel{LastPage}{{}{71}{}{page.71}{}}
\xdef\lastpage@lastpage{71}
\xdef\lastpage@lastpageHy{71}
