\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{ogata1998space}
\citation{brillinger1988maximum}
\citation{stoyan2000recent}
\citation{cox1955some}
\citation{moller1998log,brix2001spatiotemporal,cunningham2008inferring}
\citation{rasmussen2006gaussian}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{hensman2015mcmc}
\citation{lloyd2015variational}
\citation{samo2015scalable}
\citation{lloyd2015variational}
\citation{flaxman2017poisson}
\citation{walder17fast}
\citation{adams2009tractable}
\citation{gunter2014efficient}
\citation{teh2011gaussian}
\citation{kirichenko2015optimality}
\citation{polson2013bayesian}
\citation{linderman2015dependent,wenzel2017scalable}
\citation{linderman2017bayesian}
\citation{donner2017inverse}
\citation{lloyd2016latent}
\citation{csato2002sparse,csato2002phd,titsias2009variational}
\citation{hensman2015mcmc}
\citation{lloyd2015variational}
\citation{konstantopoulos2011radon}
\citation{cox1955some}
\citation{adams2009tractable}
\citation{rasmussen2006gaussian}
\citation{matthews2016sparse}
\citation{murray2006mcmc}
\@writefile{toc}{\contentsline {section}{\numberline {2}The Inference problem}{3}{section.5}}
\newlabel{sec:inference problem}{{2}{3}{}{section.5}{}}
\newlabel{eq:poisson_likelihood}{{2}{3}{}{section.5}{}}
\newlabel{eq:sigmoid poisson likelihood}{{1}{3}{}{equation.6}{}}
\newlabel{eq:posterior density}{{2}{3}{}{equation.7}{}}
\MT@newlabel{eq:posterior density}
\MT@newlabel{eq:sigmoid poisson likelihood}
\MT@newlabel{eq:posterior density}
\MT@newlabel{eq:sigmoid poisson likelihood}
\MT@newlabel{eq:sigmoid poisson likelihood}
\citation{adams2009tractable}
\citation{kingman1993poisson}
\citation{adams2009tractable}
\citation{polson2013bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data augmentation I: Latent Poisson process}{4}{subsection.8}}
\MT@newlabel{eq:sigmoid poisson likelihood}
\newlabel{eq:exponent integral}{{3}{4}{}{equation.10}{}}
\newlabel{eq:poisson characteristic functional}{{4}{4}{}{equation.11}{}}
\MT@newlabel{eq:poisson characteristic functional}
\MT@newlabel{eq:exponent integral}
\MT@newlabel{eq:poisson characteristic functional}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Data augmentation II: P\'olya--Gamma variables and marked Poisson process}{4}{subsection.12}}
\newlabel{eq:polya-gamma}{{5}{4}{}{equation.13}{}}
\newlabel{eq:tilted PG}{{6}{4}{}{equation.14}{}}
\MT@newlabel{eq:polya-gamma}
\MT@newlabel{eq:tilted PG}
\MT@newlabel{eq:polya-gamma}
\citation{kingman1993poisson}
\citation{lewis1979simulation,adams2009tractable}
\newlabel{eq:PG sigmoid}{{7}{5}{}{equation.15}{}}
\MT@newlabel{eq:exponent integral}
\MT@newlabel{eq:PG sigmoid}
\newlabel{eq:marked poisson}{{8}{5}{}{equation.17}{}}
\MT@newlabel{eq:poisson characteristic functional}
\MT@newlabel{eq:marked poisson}
\MT@newlabel{eq:marked poisson}
\MT@newlabel{eq:sigmoid poisson likelihood}
\newlabel{eq:augmented likelihood}{{9}{5}{}{equation.19}{}}
\MT@newlabel{eq:sigmoid poisson likelihood}
\citation{bishop2006pattern}
\@writefile{toc}{\contentsline {section}{\numberline {3}Inference in the augmented space}{6}{section.20}}
\newlabel{sec:inference}{{3}{6}{}{section.20}{}}
\newlabel{eq:posterior}{{10}{6}{}{equation.22}{}}
\MT@newlabel{eq:posterior}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Variational mean--field approximation}{6}{subsection.23}}
\newlabel{sec:variational}{{3.1}{6}{}{subsection.23}{}}
\newlabel{eq:variational posterior measure}{{11}{6}{}{equation.24}{}}
\newlabel{eq:lower bound}{{12}{6}{}{equation.25}{}}
\MT@newlabel{eq:variational posterior measure}
\MT@newlabel{eq:variational posterior measure}
\newlabel{eq:optimal first factor}{{13}{6}{}{equation.26}{}}
\newlabel{eq:optimalq2}{{14}{7}{}{equation.27}{}}
\MT@newlabel{eq:lower bound}
\newlabel{eq:factorizing form1}{{15}{7}{}{equation.28}{}}
\newlabel{eq:factorizing form2}{{16}{7}{}{equation.29}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimal P\'olya--Gamma density}{7}{equation.29}}
\MT@newlabel{eq:optimal first factor}
\MT@newlabel{eq:factorizing form1}
\MT@newlabel{eq:tilted PG}
\newlabel{eq:polya gamma obs}{{17}{7}{}{equation.30}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimal Poisson process}{7}{equation.30}}
\MT@newlabel{eq:optimal first factor}
\MT@newlabel{eq:factorizing form1}
\newlabel{eq:optimal marked process}{{18}{7}{}{equation.32}{}}
\MT@newlabel{eq:optimal marked process}
\newlabel{eq:posterior mean measure}{{19}{7}{}{equation.34}{}}
\citation{rasmussen2006gaussian}
\citation{solin2016stochastic}
\citation{csato2002sparse,csato2002phd,titsias2009variational}
\citation{batz2018approximate,matthews2016sparse}
\@writefile{toc}{\contentsline {paragraph}{Optimal Gaussian process}{8}{equation.34}}
\MT@newlabel{eq:optimalq2}
\MT@newlabel{eq:factorizing form2}
\newlabel{eq:U function}{{20}{8}{}{equation.35}{}}
\MT@newlabel{eq:U function}
\citation{lloyd2015variational,hensman2015mcmc}
\citation{lloyd2015variational}
\@writefile{toc}{\contentsline {paragraph}{Optimal sparse Gaussian process}{9}{equation.35}}
\newlabel{eq:sparse GP model}{{21}{9}{}{equation.36}{}}
\newlabel{eq:expected g}{{22}{9}{}{equation.37}{}}
\newlabel{eq:expected squared g}{{23}{9}{}{equation.38}{}}
\MT@newlabel{eq:U function}
\MT@newlabel{eq:expected g}
\MT@newlabel{eq:expected squared g}
\newlabel{eq:GP posterior}{{24}{9}{}{equation.39}{}}
\newlabel{eq:posterior GP cov}{{25}{9}{}{equation.40}{}}
\newlabel{eq:posterior GP mean}{{26}{9}{}{equation.41}{}}
\citation{kingma2014adam}
\citation{press1989numerical}
\citation{bishop2006pattern}
\MT@newlabel{eq:sparse GP model}
\MT@newlabel{eq:GP posterior}
\newlabel{eq:pred GP mean}{{27}{10}{}{equation.42}{}}
\newlabel{eq:pred GP var}{{28}{10}{}{equation.43}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimal density for maximal intensity $\lambda $}{10}{equation.43}}
\MT@newlabel{eq:optimalq2}
\newlabel{eq:optimal lambda density}{{29}{10}{}{equation.44}{}}
\@writefile{toc}{\contentsline {paragraph}{Hyperparameters}{10}{equation.44}}
\MT@newlabel{eq:lower bound}
\citation{rasmussen2006gaussian}
\newlabel{alg:VB}{{1}{11}{}{algocfline.45}{}}
\MT@newlabel{eq:polya gamma obs}
\MT@newlabel{eq:posterior mean measure}
\MT@newlabel{eq:posterior GP cov}
\MT@newlabel{eq:posterior GP mean}
\MT@newlabel{eq:pred GP mean}
\MT@newlabel{eq:pred GP var}
\MT@newlabel{eq:optimal lambda density}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Variational Bayes algorithm for sigmoidal Gaussian Cox process.}}{11}{algocf.56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Laplace approximation}{11}{subsection.57}}
\newlabel{subsec:laplace}{{3.2}{11}{}{subsection.57}{}}
\@writefile{toc}{\contentsline {paragraph}{Obtaining the MAP estimate}{11}{subsection.57}}
\newlabel{eq:MAP problem}{{3.2}{11}{}{subsection.57}{}}
\newlabel{eq:Qfunction}{{30}{12}{}{equation.58}{}}
\newlabel{eq:MAP log likelihood}{{3.2}{12}{}{equation.58}{}}
\MT@newlabel{eq:U function}
\newlabel{eq:sparse Qfunction}{{31}{12}{}{equation.59}{}}
\citation{rasmussen2006gaussian}
\citation{flaxman2017poisson}
\MT@newlabel{eq:sparse Qfunction}
\newlabel{eq:Mstep}{{3.2}{13}{}{equation.59}{}}
\MT@newlabel{eq:pred GP mean}
\MT@newlabel{eq:Qfunction}
\@writefile{toc}{\contentsline {paragraph}{Sparse Laplace posterior}{13}{equation.59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Predictive density}{13}{subsection.60}}
\citation{john2018large}
\citation{lewis1979simulation,adams2009tractable}
\citation{adams2009tractable}
\citation{murray2010elliptical}
\citation{hensman2015mcmc}
\newlabel{eq:log predictive likelihood}{{32}{14}{}{equation.62}{}}
\MT@newlabel{eq:log predictive likelihood}
\newlabel{eq:expandend likelihood}{{33}{14}{}{equation.64}{}}
\MT@newlabel{eq:sigmoid poisson likelihood}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{14}{section.65}}
\newlabel{sec:results}{{4}{14}{}{section.65}{}}
\@writefile{toc}{\contentsline {paragraph}{Generating data from the model}{14}{section.65}}
\citation{GPflow2017}
\@writefile{toc}{\contentsline {paragraph}{Benchmarks for sigmoidal Gaussian Cox process inference}{15}{equation.66}}
\MT@newlabel{eq:sigmoid poisson likelihood}
\@writefile{toc}{\contentsline {paragraph}{Experiments on data from generative model}{15}{equation.66}}
\MT@newlabel{eq:expandend likelihood}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\bf  Inference on 1D dataset.} {\bf  (a)}--{\bf  (d)} Inference result for sampler, mean field algorithm, Laplace approximation, and variational Gauss. Solid coloured lines denote the mean intensity function, shaded areas mean $\pm $ standard deviation, and dashed black lines the true rate functions. Vertical bars are observations ${\cal  D}$. {\bf  (e)} Convergence of mean field and EM algorithm. Objective functions (Lower bound for mean--field and log likelihood for EM algorithm, shifted such that convergence is at $0$) as function of run time (triangle marks one finished iteration of the respective algorithm). {\bf  (f)} Inferred posterior densities over the maximal intensity $\lambda $. Variational Gauss provides only a point estimate. Black vertical bar denotes the true $\lambda $.}}{16}{figure.68}}
\newlabel{fig:fig1}{{1}{16}{{\bf Inference on 1D dataset.} {\bf (a)}--{\bf (d)} Inference result for sampler, mean field algorithm, Laplace approximation, and variational Gauss. Solid coloured lines denote the mean intensity function, shaded areas mean $\pm $ standard deviation, and dashed black lines the true rate functions. Vertical bars are observations $\dataset $. {\bf (e)} Convergence of mean field and EM algorithm. Objective functions (Lower bound for mean--field and log likelihood for EM algorithm, shifted such that convergence is at $0$) as function of run time (triangle marks one finished iteration of the respective algorithm). {\bf (f)} Inferred posterior densities over the maximal intensity $\lambda $. Variational Gauss provides only a point estimate. Black vertical bar denotes the true $\lambda $}{figure.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces {\bf  Inference on 2D dataset.} {\bf  (a)} Ground truth intensity function $\Lambda (\boldsymbol  {x})$ with observed dataset ${\cal  D}$ (red dots).{\bf  (b)}--{\bf  (e)} Mean posterior intensity of the sampler, mean field algorithm, Laplace, and variational Gauss are shown. $100$ inducing points on a regular grid (shown as coloured points) and $2500$ integration points/bins are used.}}{17}{figure.69}}
\newlabel{fig:fig2}{{2}{17}{{\bf Inference on 2D dataset.} {\bf (a)} Ground truth intensity function $\Lambda (\bx )$ with observed dataset $\dataset $ (red dots).{\bf (b)}--{\bf (e)} Mean posterior intensity of the sampler, mean field algorithm, Laplace, and variational Gauss are shown. $100$ inducing points on a regular grid (shown as coloured points) and $2500$ integration points/bins are used}{figure.69}{}}
\MT@newlabel{eq:log predictive likelihood}
\MT@newlabel{eq:expandend likelihood}
\MT@newlabel{eq:expandend likelihood}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {\bf  Evaluation of inference.} {\bf  (a)} The log expected predictive likelihood averaged over ten test sets as a function of the number of inducing points. Number of integration points/bins is fixed to $2500$. Results for sampler in (red), mean field (blue), Laplace (orange), and variational Gauss (purple) algorithm. Solid line denotes mean over five fits (same data), and shaded area denotes min. and max. result. Dashed blue line shows the approximated log expected predictive likelihood for the mean field algorithm. {\bf  (b)} Same as (a), but as function of number of integration points. Number of inducing points is fixed to $10\times 10$. Below: Run time of the different algorithms as function of number of inducing points {\bf  (c)} and number of integration points {\bf  (d)}. Data are the same as in Figure\nobreakspace  {}\ref  {fig:fig2}.}}{18}{figure.70}}
\newlabel{fig:fig3}{{3}{18}{{\bf Evaluation of inference.} {\bf (a)} The log expected predictive likelihood averaged over ten test sets as a function of the number of inducing points. Number of integration points/bins is fixed to $2500$. Results for sampler in (red), mean field (blue), Laplace (orange), and variational Gauss (purple) algorithm. Solid line denotes mean over five fits (same data), and shaded area denotes min. and max. result. Dashed blue line shows the approximated log expected predictive likelihood for the mean field algorithm. {\bf (b)} Same as (a), but as function of number of integration points. Number of inducing points is fixed to $10\times 10$. Below: Run time of the different algorithms as function of number of inducing points {\bf (c)} and number of integration points {\bf (d)}. Data are the same as in Figure~\ref {fig:fig2}}{figure.70}{}}
\citation{lloyd2015variational}
\citation{adams2009tractable}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\bf  Performance on different artificial datasets.} The sampler (S), the mean field algorithm (MF), the Laplace (L), and variational Gauss (VG) are compared on five different datasets with $d$--dimensions and $N$ observations (one column corresponds to one dataset). Top row: Log expected test likelihood of the different inference results. The star denotes the approximated test likelihood of the variational algorithm. Center row: The approximated root mean squared error (normalised by true maximal intensity rate $\lambda $). Bottom row: Run time in seconds. The dataset {\bf  (e)} is intractable for the sampler due to the many observations. Data in Figure\nobreakspace  {}\ref  {fig:fig1} and\nobreakspace  {}\ref  {fig:fig2} correspond to {\bf  (a)} and {\bf  (c)}.}}{19}{figure.71}}
\newlabel{fig:fig4}{{4}{19}{{\bf Performance on different artificial datasets.} The sampler (S), the mean field algorithm (MF), the Laplace (L), and variational Gauss (VG) are compared on five different datasets with $d$--dimensions and $N$ observations (one column corresponds to one dataset). Top row: Log expected test likelihood of the different inference results. The star denotes the approximated test likelihood of the variational algorithm. Center row: The approximated root mean squared error (normalised by true maximal intensity rate $\lambda $). Bottom row: Run time in seconds. The dataset {\bf (e)} is intractable for the sampler due to the many observations. Data in Figure~\ref {fig:fig1} and~\ref {fig:fig2} correspond to {\bf (a)} and {\bf (c)}}{figure.71}{}}
\@writefile{toc}{\contentsline {paragraph}{General data sets and comparison to the approach of Lloyd et al.}{19}{figure.71}}
\citation{gridcelldata,sargolini06}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces {\bf  1D example.} Observations (black bars) are sampled from the same function (black line) scaled by {\bf  (a)} 1, {\bf  (b)} 10, and {\bf  (c)} 100. Blue and green line show the mean posterior of the sigmoidal and squared Gaussian Cox process, respectively. Shaded area denotes mean $\pm $ standard deviation.}}{20}{figure.73}}
\newlabel{fig:fig5}{{5}{20}{{\bf 1D example.} Observations (black bars) are sampled from the same function (black line) scaled by {\bf (a)} 1, {\bf (b)} 10, and {\bf (c)} 100. Blue and green line show the mean posterior of the sigmoidal and squared Gaussian Cox process, respectively. Shaded area denotes mean $\pm $ standard deviation}{figure.73}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces {\bf  Benchmarks for Figure\nobreakspace  {}\ref  {fig:fig5}} The mean and standard deviation of runtime, RMSE, and log expected test likelihood for Figure\nobreakspace  {}\ref  {fig:fig5}{\bf  (a)}--{\bf  (c)} obtained from $5$ fits. Note that the RMSE for $\Lambda (\boldsymbol  {x})=g^2(\boldsymbol  {x})$ has no standard deviation, because the inference algorithm is deterministic.}}{20}{table.74}}
\newlabel{tab:table1}{{1}{20}{{\bf Benchmarks for Figure~\ref {fig:fig5}} The mean and standard deviation of runtime, RMSE, and log expected test likelihood for Figure~\ref {fig:fig5}{\bf (a)}--{\bf (c)} obtained from $5$ fits. Note that the RMSE for $\Lambda (\bx )=g^2(\bx )$ has no standard deviation, because the inference algorithm is deterministic}{table.74}{}}
\citation{moreira2013predicting}
\citation{john2018large}
\citation{john2018large}
\citation{lloyd2015variational}
\citation{lloyd2015variational,flaxman2017poisson,john2018large}
\citation{lloyd2015variational}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion and Outlook}{21}{section.78}}
\newlabel{sec:discussion}{{5}{21}{}{section.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces {\bf  Fits to real world data sets.} {\bf  (a)} Position of the mouse while the recorded neuron spiked. {\bf  (b)} Posterior mean obtained by the variational mean--field algorithm for the sigmoidal Gaussian Cox process. {\bf  (c)} Same as in (b) for the variational approximation of the squared Gaussian Cox process. {\bf  (d)} Log expected test--likelihood $\ell _{\rm  test}$ and runtime as function of number of integration points for both algorithms. The dotted line is obtained by first fitting the sigmoidal model with $1000$ integration points and then with the number that is indicated on the x-axis. Shaded area is mean $\pm $ standard deviation obtained in $5$ repeated fits. {\bf  (e)}--{\bf  (h)} Same as (a)--(d), but for a data set, where the observations are positions of taxi pick--ups in the city of Porto.}}{22}{figure.77}}
\newlabel{fig:fig6}{{6}{22}{{\bf Fits to real world data sets.} {\bf (a)} Position of the mouse while the recorded neuron spiked. {\bf (b)} Posterior mean obtained by the variational mean--field algorithm for the sigmoidal Gaussian Cox process. {\bf (c)} Same as in (b) for the variational approximation of the squared Gaussian Cox process. {\bf (d)} Log expected test--likelihood $\ell _{\rm test}$ and runtime as function of number of integration points for both algorithms. The dotted line is obtained by first fitting the sigmoidal model with $1000$ integration points and then with the number that is indicated on the x-axis. Shaded area is mean $\pm $ standard deviation obtained in $5$ repeated fits. {\bf (e)}--{\bf (h)} Same as (a)--(d), but for a data set, where the observations are positions of taxi pick--ups in the city of Porto}{figure.77}{}}
\citation{hensman2015mcmc}
\citation{donner2017inverse}
\citation{knollmuller2017inference,john2018large}
\citation{hawkes1971spectra}
\citation{embrechts2011multivariate}
\citation{ogata1998space}
\citation{donner2017inverse}
\citation{dunn2015correlations}
\citation{murray2009gaussian}
\citation{donner2018efficient}
\citation{kingman1993poisson}
\citation{polson2013bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {A}Poisson processes}{25}{section.79}}
\newlabel{app:poisson process}{{A}{25}{}{section.79}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition of a Poisson process}{25}{section.79}}
\@writefile{toc}{\contentsline {paragraph}{Campbell's Theorem}{25}{Item.81}}
\newlabel{eq:characteristic functional}{{36}{25}{}{equation.82}{}}
\MT@newlabel{eq:characteristic functional}
\@writefile{toc}{\contentsline {paragraph}{Marked Poisson process}{25}{equation.82}}
\newlabel{eq:marked characteristic functional}{{37}{25}{}{equation.83}{}}
\citation{konstantopoulos2011radon}
\@writefile{toc}{\contentsline {section}{\numberline {B}The P\'olya-Gamma density}{26}{section.84}}
\newlabel{app:polya gamma}{{B}{26}{}{section.84}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Variational inference for stochastic processes}{26}{section.85}}
\newlabel{app:variational inference}{{C}{26}{}{section.85}{}}
\@writefile{toc}{\contentsline {paragraph}{Densities for random processes}{26}{section.85}}
\newlabel{eq:radon nikodym}{{38}{26}{}{equation.86}{}}
\MT@newlabel{eq:radon nikodym}
\@writefile{toc}{\contentsline {paragraph}{Poisson process density}{26}{equation.86}}
\MT@newlabel{eq:augmented likelihood}
\citation{kingman1993poisson}
\newlabel{eq:poisson expectation}{{39}{27}{}{equation.87}{}}
\MT@newlabel{eq:poisson expectation}
\MT@newlabel{eq:marked characteristic functional}
\@writefile{toc}{\contentsline {paragraph}{Kullback-Leibler divergence}{27}{equation.87}}
\@writefile{toc}{\contentsline {section}{\numberline {D}The posterior point process is a marked Poisson process}{27}{section.88}}
\newlabel{app:optimal posterior poisson}{{D}{27}{}{section.88}{}}
\MT@newlabel{eq:optimal marked process}
\MT@newlabel{eq:optimal marked process}
\MT@newlabel{eq:marked characteristic functional}
\@writefile{toc}{\contentsline {section}{\numberline {E}Sparse Gaussian process approximation}{28}{section.89}}
\newlabel{app:sparse GP}{{E}{28}{}{section.89}{}}
\newlabel{eq:app sparse gp}{{40}{28}{}{equation.90}{}}
\newlabel{eq:app posterior measure}{{E}{28}{}{equation.90}{}}
\MT@newlabel{eq:app sparse gp}
\newlabel{eq:app sparse kl}{{E}{28}{}{equation.90}{}}
\newlabel{eq:U function app}{{E}{28}{}{equation.90}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Lower bound \& hyperparameter optimization}{29}{section.91}}
\newlabel{app:lower bound}{{F}{29}{}{section.91}{}}
\MT@newlabel{eq:lower bound}
\bibdata{bib}
\bibcite{adams2009tractable}{{1}{2009}{{Adams et~al.}}{{Adams, Murray, and MacKay}}}
\bibcite{batz2018approximate}{{2}{2018}{{Batz et~al.}}{{Batz, Ruttor, and Opper}}}
\bibcite{bishop2006pattern}{{3}{2006}{{Bishop}}{{}}}
\bibcite{brillinger1988maximum}{{4}{1988}{{Brillinger}}{{}}}
\bibcite{brix2001spatiotemporal}{{5}{2001}{{Brix and Diggle}}{{}}}
\bibcite{cox1955some}{{6}{1955}{{Cox}}{{}}}
\bibcite{csato2002phd}{{7}{2002}{{Csat{\'o}}}{{}}}
\bibcite{csato2002sparse}{{8}{2002}{{Csat{\'o} and Opper}}{{}}}
\bibcite{cunningham2008inferring}{{9}{2008}{{Cunningham et~al.}}{{Cunningham, Yu, Shenoy, and Sahani}}}
\bibcite{matthews2016sparse}{{10}{2016}{{de~G.~Matthews et~al.}}{{de~G.~Matthews, Hensman, Turner, and Ghahramani}}}
\bibcite{donner2017inverse}{{11}{2017}{{Donner and Opper}}{{}}}
\bibcite{donner2018efficient}{{12}{2018}{{Donner and Opper}}{{}}}
\bibcite{dunn2015correlations}{{13}{2015}{{Dunn et~al.}}{{Dunn, Mørreaunet, and Roudi}}}
\bibcite{embrechts2011multivariate}{{14}{2011}{{Embrechts et~al.}}{{Embrechts, Liniger, and Lin}}}
\bibcite{flaxman2017poisson}{{15}{2017}{{Flaxman et~al.}}{{Flaxman, Teh, and Sejdinovic}}}
\bibcite{gridcelldata}{{16}{2014}{{For The Biology Of~Memory and Sargolini}}{{}}}
\bibcite{gunter2014efficient}{{17}{2014}{{Gunter et~al.}}{{Gunter, Lloyd, Osborne, and Roberts}}}
\bibcite{hawkes1971spectra}{{18}{1971}{{Hawkes}}{{}}}
\bibcite{hensman2015mcmc}{{19}{2015}{{Hensman et~al.}}{{Hensman, Matthews, Filippone, and Ghahramani}}}
\bibcite{john2018large}{{20}{2018}{{John and Hensman}}{{}}}
\bibcite{kingma2014adam}{{21}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kingman1993poisson}{{22}{1993}{{Kingman}}{{}}}
\bibcite{kirichenko2015optimality}{{23}{2015}{{Kirichenko and van Zanten}}{{}}}
\bibcite{knollmuller2017inference}{{24}{2017}{{{Knollm{\"u}ller} et~al.}}{{{Knollm{\"u}ller}, {Steininger}, and {En{\ss }lin}}}}
\bibcite{konstantopoulos2011radon}{{25}{2011}{{Konstantopoulos et~al.}}{{Konstantopoulos, Zerakidze, and Sokhadze}}}
\bibcite{lewis1979simulation}{{26}{1979}{{Lewis and Shedler}}{{}}}
\bibcite{linderman2015dependent}{{27}{2015}{{Linderman et~al.}}{{Linderman, Johnson, and Adams}}}
\bibcite{linderman2017bayesian}{{28}{2017}{{Linderman et~al.}}{{Linderman, Johnson, Miller, Adams, Blei, and Paninski}}}
\bibcite{lloyd2015variational}{{29}{2015}{{Lloyd et~al.}}{{Lloyd, Gunter, Osborne, and Roberts}}}
\bibcite{lloyd2016latent}{{30}{2016}{{Lloyd et~al.}}{{Lloyd, Gunter, Osborne, Roberts, and Nickson}}}
\bibcite{GPflow2017}{{31}{2017}{{Matthews et~al.}}{{Matthews, {van der Wilk}, Nickson, Fujii, {Boukouvalas}, {Le{‘o}n-Villagr{‘a}}, Ghahramani, and Hensman}}}
\bibcite{moller1998log}{{32}{1998}{{M{\o }ller et~al.}}{{M{\o }ller, Syversveen, and Waagepetersen}}}
\bibcite{moreira2013predicting}{{33}{2013}{{Moreira-Matias et~al.}}{{Moreira-Matias, Gama, Ferreira, Mendes-Moreira, and Damas}}}
\bibcite{murray2006mcmc}{{34}{2006}{{Murray et~al.}}{{Murray, Ghahramani, and MacKay}}}
\bibcite{murray2009gaussian}{{35}{2009}{{Murray et~al.}}{{Murray, MacKay, and Adams}}}
\bibcite{murray2010elliptical}{{36}{2010}{{Murray et~al.}}{{Murray, Adams, and MacKay}}}
\bibcite{ogata1998space}{{37}{1998}{{Ogata}}{{}}}
\bibcite{polson2013bayesian}{{38}{2013}{{Polson et~al.}}{{Polson, Scott, and Windle}}}
\bibcite{press1989numerical}{{39}{2007}{{Press et~al.}}{{Press, Flannery, Teukolsky, Vetterling, et~al.}}}
\bibcite{rasmussen2006gaussian}{{40}{2006}{{Rasmussen and Williams}}{{}}}
\bibcite{samo2015scalable}{{41}{2015}{{Samo and Roberts}}{{}}}
\bibcite{sargolini06}{{42}{2006}{{Sargolini et~al.}}{{Sargolini, Fyhn, Hafting, McNaughton, Witter, Moser, and Moser}}}
\bibcite{solin2016stochastic}{{43}{2016}{{Solin}}{{}}}
\bibcite{stoyan2000recent}{{44}{2000}{{Stoyan and Penttinen}}{{}}}
\bibcite{teh2011gaussian}{{45}{2011}{{Teh and Rao}}{{}}}
\bibcite{titsias2009variational}{{46}{2009}{{Titsias}}{{}}}
\bibcite{walder17fast}{{47}{2017}{{Walder and Bishop}}{{}}}
\bibcite{wenzel2017scalable}{{48}{2017}{{Wenzel et~al.}}{{Wenzel, Galy-Fajou, Donner, Kloft, and Opper}}}
\newlabel{LastPage}{{}{34}{}{page.34}{}}
\xdef\lastpage@lastpage{34}
\xdef\lastpage@lastpageHy{34}
