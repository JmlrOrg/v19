\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{neyshabur2014search,neyshabur2015path,Zhang2016,Keskar2016,Neyshabur2017a,Wilson2017}
\citation{Hardt2015a}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Kingma2015}
\@writefile{toc}{\contentsline {section}{\numberline {2}Main Results}{2}{section.2}}
\newlabel{eq: general loss functions}{{1}{2}{}{equation.3}{}}
\@writefile{loe}{\contentsline {assm}{\numberline {1}Assumption}{2}{assm.5}}
\newlabel{assum: Linear sepereability}{{1}{2}{}{assm.5}{}}
\@writefile{loe}{\contentsline {assm}{\numberline {2}Assumption}{2}{assm.7}}
\newlabel{assum: loss properties}{{2}{2}{}{assm.7}{}}
\citation{Ganti2015}
\newlabel{eq: gradient descent linear}{{2}{3}{}{equation.10}{}}
\newlabel{lem: convergence of linear classifiers}{{1}{3}{}{thm.11}{}}
\newlabel{def: exponential tail}{{2}{3}{}{thm.12}{}}
\@writefile{loe}{\contentsline {assm}{\numberline {3}Assumption}{3}{assm.14}}
\newlabel{assum: exponential tail}{{3}{3}{}{assm.14}{}}
\newlabel{thmt@@LRasymptotic@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )2.\@arabic {\c@equation }}\setcounter {equation}{2}}{3}{}{assm.14}{}}
\citation{Soudry2017a}
\@writefile{loe}{\contentsline {thmR}{\numberline {3}Theorem}{4}{thmR.16}}
\newlabel{thmt@@LRasymptotic}{{3}{4}{}{thmR.16}{}}
\newlabel{thm: main theorem}{{3}{4}{}{thmR.16}{}}
\newlabel{eq: asymptotic form}{{3}{4}{}{equation.17}{}}
\newlabel{eq: max margin vector}{{4}{4}{}{equation.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Proof Sketch}{4}{equation.18}}
\newlabel{eq: gradient exp}{{5}{4}{}{equation.19}{}}
\newlabel{eq:kkt}{{6}{4}{}{equation.20}{}}
\citation{Ji2018}
\citation{schapire1998boosting}
\citation{zhang2005boosting,telgarsky2013margins}
\citation{telgarsky2013margins}
\citation{Rosset2004}
\citation{Rosset2004}
\citation{gunasekar2018characterizing}
\@writefile{toc}{\contentsline {paragraph}{Degenerate and Non-Degenerate Data Sets}{5}{equation.20}}
\@writefile{toc}{\contentsline {paragraph}{Parallel Work on the Degenerate Case}{5}{equation.20}}
\@writefile{toc}{\contentsline {paragraph}{More Refined Analysis of the Residual}{5}{equation.20}}
\newlabel{thmt@@LRasymptotic2@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )2.\@arabic {\c@equation }}\setcounter {equation}{6}}{5}{}{equation.20}{}}
\@writefile{loe}{\contentsline {thmR}{\numberline {4}Theorem}{5}{thmR.22}}
\newlabel{thmt@@LRasymptotic2}{{4}{5}{}{thmR.22}{}}
\newlabel{thm: refined Theorem}{{4}{5}{}{thmR.22}{}}
\newlabel{eq: w tilde}{{7}{5}{}{equation.23}{}}
\@writefile{toc}{\contentsline {paragraph}{Analogies with Boosting}{5}{equation.23}}
\citation{Rosset2004,telgarsky2013margins}
\@writefile{toc}{\contentsline {paragraph}{Non-homogeneous linear predictors}{6}{equation.23}}
\newlabel{sec: convergence rates}{{3}{6}{}{section.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implications: Rates of convergence}{6}{section.25}}
\newlabel{thmt@@ratetheorem@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )3.\@arabic {\c@equation }}\setcounter {equation}{7}}{6}{}{section.25}{}}
\@writefile{loe}{\contentsline {thmR}{\numberline {5}Theorem}{6}{thmR.27}}
\newlabel{thmt@@ratetheorem}{{5}{6}{}{thmR.27}{}}
\newlabel{thm: rates}{{5}{6}{}{thmR.27}{}}
\newlabel{eq: normalized weight vector}{{8}{6}{}{equation.28}{}}
\newlabel{eq: angle}{{9}{6}{}{equation.29}{}}
\newlabel{eq: margin}{{10}{6}{}{equation.30}{}}
\newlabel{eq: logistic loss convergence}{{11}{6}{}{equation.31}{}}
\newlabel{eq:poploss}{{12}{7}{}{equation.32}{}}
\@writefile{loe}{\contentsline {corR}{\numberline {6}Corollary}{7}{corR.34}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visualization of or main results on a synthetic dataset in which the $L_{2}$ max margin vector $\mathaccentV {hat}05E{\mathbf  {w}}$ is precisely known. \textbf  {(A)} The dataset (positive and negatives samples ($y=\pm 1$) are respectively denoted by $'+'$ and $'\circ '$), max margin separating hyperplane (black line), and the asymptotic solution of GD (dashed blue). For both GD and GD with momentum (GDMO), we show: \textbf  {(B) }The norm of $\mathbf  {w}\left (t\right )$, normalized so it would equal to $1$ at the last iteration, to facilitate comparison. As expected (eq. \ref  {eq: asymptotic form}), the norm increases logarithmically; \textbf  {(C) }the training loss. As expected, it decreases as $t^{-1}$ (eq. \ref  {eq: logistic loss convergence}); and \textbf  {(D\&E) }the angle and margin gap of $\mathbf  {w}\left (t\right )$ from $\mathaccentV {hat}05E{\mathbf  {w}}$ (eqs. \ref  {eq: angle} and \ref  {eq: margin}). As expected, these are logarithmically decreasing to zero. \textbf  {Implementation details:} The dataset includes four support vectors: $\mathbf  {x}_{1}=\left (0.5,1.5\right ),\mathbf  {x}_{2}=\left (1.5,0.5\right )$ with $y_{1}=y_{2}=1$, and $\mathbf  {x}_{3}=-\mathbf  {x}_{1}$, $\mathbf  {x}_{4}=-\mathbf  {x}_{2}$ with $y_{3}=y_{4}=-1$ (the $L_{2}$ normalized max margin vector is then $\mathaccentV {hat}05E{\mathbf  {w}}=\left (1,1\right )/\sqrt  {2}$ with margin equal to $\sqrt  {2}$ ), and $12$ other random datapoints ($6$ from each class), that are not on the margin. We used a learning rate $\eta =1/\sigma ^2_{\qopname  \relax m{max}}\left (\mathbf  {X}\right )$, where $\sigma ^2_{\qopname  \relax m{max}}\left (\mathbf  {X}\right )$ is the maximal singular value of $\mathbf  {X}$, momentum $\gamma =0.9$ for GDMO, and initialized at the origin. }}{8}{figure.36}}
\newlabel{fig:Synthetic-dataset}{{1}{8}{Visualization of or main results on a synthetic dataset in which the $L_{2}$ max margin vector $\hat {\mathbf {w}}$ is precisely known. \textbf {(A)} The dataset (positive and negatives samples ($y=\pm 1$) are respectively denoted by $'+'$ and $'\circ '$), max margin separating hyperplane (black line), and the asymptotic solution of GD (dashed blue). For both GD and GD with momentum (GDMO), we show: \textbf {(B) }The norm of $\mathbf {w}\left (t\right )$, normalized so it would equal to $1$ at the last iteration, to facilitate comparison. As expected (eq. \ref {eq: asymptotic form}), the norm increases logarithmically; \textbf {(C) }the training loss. As expected, it decreases as $t^{-1}$ (eq. \ref {eq: logistic loss convergence}); and \textbf {(D\&E) }the angle and margin gap of $\mathbf {w}\left (t\right )$ from $\hat {\mathbf {w}}$ (eqs. \ref {eq: angle} and \ref {eq: margin}). As expected, these are logarithmically decreasing to zero. \textbf {Implementation details:} The dataset includes four support vectors: $\mathbf {x}_{1}=\left (0.5,1.5\right ),\mathbf {x}_{2}=\left (1.5,0.5\right )$ with $y_{1}=y_{2}=1$, and $\mathbf {x}_{3}=-\mathbf {x}_{1}$, $\mathbf {x}_{4}=-\mathbf {x}_{2}$ with $y_{3}=y_{4}=-1$ (the $L_{2}$ normalized max margin vector is then $\hat {\mathbf {w}}=\left (1,1\right )/\sqrt {2}$ with margin equal to $\sqrt {2}$ ), and $12$ other random datapoints ($6$ from each class), that are not on the margin. We used a learning rate $\eta =1/\sigma ^2_{\max }\left (\mathbf {X}\right )$, where $\sigma ^2_{\max }\left (\mathbf {X}\right )$ is the maximal singular value of $\mathbf {X}$, momentum $\gamma =0.9$ for GDMO, and initialized at the origin}{figure.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Extensions}{8}{section.37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Multi-Class Classification with Cross-Entropy Loss}{8}{subsection.38}}
\newlabel{eq: multi loss}{{13}{8}{}{equation.39}{}}
\citation{Hubara2016}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training of a convolutional neural network on CIFAR10 using stochastic gradient descent with constant learning rate and momentum, softmax output and a cross entropy loss, where we achieve $8.3\%$ final validation error. We observe that, approximately: (1) The training loss decays as a $t^{-1}$, (2) the $L_{2}$ norm of last weight layer increases logarithmically, (3) after a while, the validation loss starts to increase, and (4) in contrast, the validation (classification) error slowly improves. }}{9}{figure.46}}
\newlabel{fig: DNN results}{{2}{9}{Training of a convolutional neural network on CIFAR10 using stochastic gradient descent with constant learning rate and momentum, softmax output and a cross entropy loss, where we achieve $8.3\%$ final validation error. We observe that, approximately: (1) The training loss decays as a $t^{-1}$, (2) the $L_{2}$ norm of last weight layer increases logarithmically, (3) after a while, the validation loss starts to increase, and (4) in contrast, the validation (classification) error slowly improves}{figure.46}{}}
\newlabel{thmt@@mainMulti@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.\@arabic {\c@equation }}\setcounter {equation}{13}}{9}{}{equation.39}{}}
\@writefile{loe}{\contentsline {thmR}{\numberline {7}Theorem}{9}{thmR.41}}
\newlabel{thmt@@mainMulti}{{7}{9}{}{thmR.41}{}}
\newlabel{thm:mainMulti}{{7}{9}{}{thmR.41}{}}
\newlabel{eq:5}{{14}{9}{}{equation.42}{}}
\newlabel{eq: K-class SVM}{{15}{9}{}{equation.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Deep networks}{9}{subsection.44}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Sample values from various epochs in the experiment depicted in Fig. \ref  {fig: DNN results}. }}{10}{table.49}}
\newlabel{tab:Sample-value-dnn}{{1}{10}{Sample values from various epochs in the experiment depicted in Fig. \ref {fig: DNN results}}{table.49}{}}
\@writefile{loe}{\contentsline {corR}{\numberline {8}Corollary}{10}{corR.51}}
\newlabel{eq: DNN_output}{{16}{10}{}{equation.52}{}}
\citation{Gunasekar2018b}
\citation{Gunasekar2018b}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Same as Fig. \ref  {fig:Synthetic-dataset}, except we multiplied all $x_{2}$ values in the dastaset by $20$, and also train using ADAM. The final weight vector produced after $2\cdot 10^{6}$ epochs of optimization using ADAM (red dashed line) does not converge to L2 max margin solution (black line), in contrast to GD (blue dashed line), or GDMO.}}{11}{figure.55}}
\newlabel{fig:Synthetic-dataset-adam}{{3}{11}{Same as Fig. \ref {fig:Synthetic-dataset}, except we multiplied all $x_{2}$ values in the dastaset by $20$, and also train using ADAM. The final weight vector produced after $2\cdot 10^{6}$ epochs of optimization using ADAM (red dashed line) does not converge to L2 max margin solution (black line), in contrast to GD (blue dashed line), or GDMO}{figure.55}{}}
\newlabel{sec:other_opt}{{4.3}{11}{}{subsection.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Other optimization methods }{11}{subsection.53}}
\citation{Nacson2018b}
\citation{duchi2011adaptive}
\citation{Kingma2015}
\citation{Hoffer2017a}
\citation{Wilson2017}
\citation{Wilson2017}
\citation{gunasekar2018characterizing}
\citation{gunasekar2018characterizing}
\citation{Nacson2018}
\citation{Gunasekar2017}
\citation{Gunasekar2017}
\citation{Gunasekar2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Other loss functions}{12}{subsection.56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Matrix Factorization}{12}{subsection.57}}
\citation{gunasekar2018characterizing}
\@writefile{toc}{\contentsline {section}{\numberline {5}Summary}{13}{section.58}}
\newlabel{sec:proof}{{A}{15}{\newpage {}Appendix}{section.64}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proof of Theorems \ref  {thm: main theorem} and \ref  {thm: refined Theorem} for almost every dataset}{15}{section.64}}
\newlabel{thmt@@LRasymptotic_ae@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )1.\@arabic {\c@equation }}\setcounter {equation}{16}}{15}{\newpage {}Appendix}{section.64}{}}
\@writefile{loe}{\contentsline {thmR}{\numberline {9}Theorem}{15}{thmR.66}}
\newlabel{thmt@@LRasymptotic_ae}{{9}{15}{}{thmR.66}{}}
\newlabel{thm: main theorem almost everywhere}{{9}{15}{}{thmR.66}{}}
\newlabel{eq:w_tilde2}{{18}{15}{\newpage {}Appendix}{equation.68}{}}
\newlabel{eq: v1 SVM}{{19}{15}{\newpage {}Appendix}{equation.69}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Simple proof of Theorem \ref  {thm: main theorem almost everywhere}}{16}{subsection.71}}
\newlabel{eq: r definition}{{20}{16}{\newpage {}Appendix}{equation.72}{}}
\newlabel{eq: r dot}{{21}{16}{\newpage {}Appendix}{equation.73}{}}
\newlabel{eq: dr^2/dt}{{22}{16}{\newpage {}Appendix}{equation.74}{}}
\newlabel{eq: dr/dt S}{{23}{16}{\newpage {}Appendix}{equation.75}{}}
\newlabel{eq: dr/dt non-S}{{24}{16}{\newpage {}Appendix}{equation.76}{}}
\newlabel{sec:Proof-of-Theorem}{{A.2}{17}{\newpage {}Appendix}{subsection.77}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Complete proof of Theorem \ref  {thm: main theorem almost everywhere} }{17}{subsection.77}}
\newlabel{thmt@@GDconvergence@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )1.\@arabic {\c@equation }}\setcounter {equation}{24}}{17}{\newpage {}Appendix}{subsection.77}{}}
\@writefile{loe}{\contentsline {lemR}{\numberline {10}Lemma}{17}{lemR.79}}
\newlabel{thmt@@GDconvergence}{{10}{17}{}{lemR.79}{}}
\newlabel{lem: GD convergence}{{10}{17}{}{lemR.79}{}}
\newlabel{eq: gradient descent}{{25}{17}{}{equation.80}{}}
\newlabel{thmt@@correlation@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )1.\@arabic {\c@equation }}\setcounter {equation}{25}}{17}{\newpage {}Appendix}{equation.80}{}}
\@writefile{loe}{\contentsline {lemR}{\numberline {11}Lemma}{17}{lemR.82}}
\newlabel{thmt@@correlation}{{11}{17}{}{lemR.82}{}}
\newlabel{lem: correlation bound}{{11}{17}{}{lemR.82}{}}
\newlabel{eq: general case}{{26}{17}{}{equation.83}{}}
\newlabel{eq: bounded r(t)}{{27}{17}{}{equation.84}{}}
\newlabel{eq: bounded cases}{{28}{17}{}{equation.85}{}}
\newlabel{eq: r recursion}{{29}{17}{\newpage {}Appendix}{equation.86}{}}
\newlabel{eq: square r difference}{{30}{17}{\newpage {}Appendix}{equation.87}{}}
\newlabel{eq: norm grad squared 1/t}{{32}{18}{\newpage {}Appendix}{equation.89}{}}
\newlabel{eq: square norm of r difference-1}{{33}{18}{\newpage {}Appendix}{equation.90}{}}
\newlabel{eq: norm difference convergence}{{34}{18}{\newpage {}Appendix}{equation.91}{}}
\newlabel{eq: correlation general case}{{35}{18}{\newpage {}Appendix}{equation.92}{}}
\newlabel{subsec:Proof-of-refined-Theorem}{{A.3}{18}{\newpage {}Appendix}{subsection.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Proof of Theorem \ref  {thm: refined Theorem} }{18}{subsection.93}}
\citation{Ganti2015}
\newlabel{eq: r(t) bound}{{36}{19}{\newpage {}Appendix}{equation.94}{}}
\newlabel{sec:Proof of GD convergence}{{A.4}{19}{\newpage {}Appendix}{subsection.95}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Proof of Lemma \ref  {lem: GD convergence}}{19}{subsection.95}}
\@writefile{loe}{\contentsline {lemR}{\numberline {10}Lemma}{19}{lemR.97}}
\newlabel{eq: propery of beta smoothness}{{37}{20}{\newpage {}Appendix}{equation.99}{}}
\newlabel{sec:Proof-of-Lemma correlation}{{A.5}{20}{\newpage {}Appendix}{subsection.100}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Proof of Lemma \ref  {lem: correlation bound}}{20}{subsection.100}}
\@writefile{loe}{\contentsline {lemR}{\numberline {11}Lemma}{20}{lemR.102}}
\newlabel{eq: exp bound top}{{38}{21}{\newpage {}Appendix}{equation.106}{}}
\newlabel{eq: exp bound bottom}{{39}{21}{\newpage {}Appendix}{equation.107}{}}
\newlabel{eq: norm r dot}{{40}{21}{\newpage {}Appendix}{equation.108}{}}
\newlabel{eq: w_hat r bound 1}{{41}{21}{\newpage {}Appendix}{equation.109}{}}
\newlabel{eq: two factor inequality}{{42}{21}{\newpage {}Appendix}{equation.110}{}}
\newlabel{eq: bound on non-SV gradient}{{43}{22}{\newpage {}Appendix}{equation.111}{}}
\newlabel{eq: minimum over S1 tilde}{{44}{22}{\newpage {}Appendix}{equation.112}{}}
\newlabel{eq: positive case}{{45}{22}{\newpage {}Appendix}{equation.113}{}}
\newlabel{eq: positive case a}{{46}{22}{\newpage {}Appendix}{equation.115}{}}
\newlabel{eq: positive case b}{{47}{22}{\newpage {}Appendix}{equation.117}{}}
\newlabel{eq: positive case c}{{48}{23}{\newpage {}Appendix}{equation.119}{}}
\newlabel{eq: negative case a}{{49}{23}{\newpage {}Appendix}{equation.121}{}}
\newlabel{eq: case 2 2}{{50}{23}{\newpage {}Appendix}{equation.123}{}}
\newlabel{eq: M bound 1}{{51}{23}{\newpage {}Appendix}{equation.124}{}}
\newlabel{eq: M bound 2}{{52}{23}{\newpage {}Appendix}{equation.125}{}}
\newlabel{eq: negative case b}{{53}{23}{\newpage {}Appendix}{equation.126}{}}
\newlabel{eq: negative case c}{{54}{24}{\newpage {}Appendix}{equation.128}{}}
\newlabel{eq: epsilon 2}{{55}{24}{\newpage {}Appendix}{equation.130}{}}
\newlabel{sec:alpha}{{B}{24}{\newpage {}Appendix}{section.132}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Generic solutions of the KKT conditions in eq. \ref  {eq:kkt} }{24}{section.132}}
\newlabel{lem: alpha}{{12}{24}{\newpage {}Appendix}{thm.133}{}}
\newlabel{eq: dual KKT}{{56}{25}{\newpage {}Appendix}{equation.134}{}}
\newlabel{eq: inverted dual KKT}{{57}{25}{\newpage {}Appendix}{equation.135}{}}
\newlabel{sec: proof of degenerate case}{{C}{25}{\newpage {}Appendix}{section.136}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Completing the proof of Theorem \ref  {thm: main theorem} for zero measure cases }{25}{section.136}}
\newlabel{eq:hatwm}{{58}{25}{\newpage {}Appendix}{equation.137}{}}
\newlabel{eq:sm}{{59}{26}{\newpage {}Appendix}{equation.138}{}}
\newlabel{theorem: main2}{{13}{26}{\newpage {}Appendix}{thm.140}{}}
\newlabel{eq: asymptotic form-1}{{61}{26}{\newpage {}Appendix}{equation.141}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Auxiliary notation}{26}{subsection.142}}
\newlabel{eq: w tilde-1}{{62}{27}{\newpage {}Appendix}{equation.143}{}}
\newlabel{eq: w tilde constraints}{{63}{27}{\newpage {}Appendix}{equation.144}{}}
\newlabel{eq: w check}{{64}{27}{\newpage {}Appendix}{equation.145}{}}
\newlabel{eq: w check constraints}{{65}{27}{\newpage {}Appendix}{equation.146}{}}
\newlabel{eq: w_hat full decomposition}{{66}{27}{\newpage {}Appendix}{equation.147}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Proof of Theorem \ref  {theorem: main2}}{27}{subsection.148}}
\newlabel{eq: define r(t) degenrate}{{67}{27}{\newpage {}Appendix}{equation.149}{}}
\newlabel{eq: norm r(t+1) degenerate}{{68}{27}{\newpage {}Appendix}{equation.150}{}}
\newlabel{eq: norm(r(t+1)-r(t) degenerate}{{69}{28}{\newpage {}Appendix}{equation.151}{}}
\newlabel{eq: derL converge degenerate}{{71}{28}{\newpage {}Appendix}{equation.153}{}}
\newlabel{eq: sum norm r bound}{{72}{28}{\newpage {}Appendix}{equation.154}{}}
\newlabel{thmt@@Correlation@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )3.\@arabic {\c@equation }}\setcounter {equation}{72}}{28}{\newpage {}Appendix}{equation.154}{}}
\@writefile{loe}{\contentsline {lemR}{\numberline {14}Lemma}{28}{lemR.156}}
\newlabel{thmt@@Correlation}{{14}{28}{}{lemR.156}{}}
\newlabel{lem: r correlation}{{14}{28}{}{lemR.156}{}}
\newlabel{eq: general case-1}{{73}{28}{}{equation.157}{}}
\newlabel{thmt@@PhiSum@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )3.\@arabic {\c@equation }}\setcounter {equation}{73}}{28}{\newpage {}Appendix}{equation.157}{}}
\@writefile{loe}{\contentsline {lemR}{\numberline {15}Lemma}{28}{lemR.159}}
\newlabel{thmt@@PhiSum}{{15}{28}{}{lemR.159}{}}
\newlabel{lem: PhiSum}{{15}{28}{}{lemR.159}{}}
\newlabel{eq: rho bound}{{74}{28}{}{equation.160}{}}
\newlabel{eq: rho sum bound}{{75}{28}{}{equation.161}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Proof of Lemma \ref  {lem: r correlation}}{29}{subsection.162}}
\newlabel{thmt@@logtsum@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )3.\@arabic {\c@equation }}\setcounter {equation}{75}}{29}{\newpage {}Appendix}{subsection.162}{}}
\@writefile{loe}{\contentsline {lemR}{\numberline {16}Lemma}{29}{lemR.164}}
\newlabel{thmt@@logtsum}{{16}{29}{}{lemR.164}{}}
\newlabel{lem:logtsum}{{16}{29}{}{lemR.164}{}}
\newlabel{lem:int}{{16}{29}{}{lemR.164}{}}
\newlabel{eq:logsum}{{76}{29}{\newpage {}Appendix}{equation.165}{}}
\newlabel{eq:int-byparts}{{77}{30}{\newpage {}Appendix}{equation.166}{}}
\@writefile{loe}{\contentsline {lemR}{\numberline {14}Lemma}{30}{lemR.168}}
\newlabel{eq: define r(t) degenrate 2}{{78}{30}{\newpage {}Appendix}{equation.170}{}}
\newlabel{eq: q(t)}{{79}{30}{\newpage {}Appendix}{equation.171}{}}
\newlabel{eq: h_m}{{80}{30}{\newpage {}Appendix}{equation.172}{}}
\newlabel{eq: q second derivative}{{81}{30}{\newpage {}Appendix}{equation.173}{}}
\newlabel{eq: q dot}{{82}{30}{\newpage {}Appendix}{equation.174}{}}
\newlabel{eq: h bound}{{83}{30}{\newpage {}Appendix}{equation.175}{}}
\newlabel{eq: h dot bound}{{84}{31}{\newpage {}Appendix}{equation.176}{}}
\newlabel{eq: r correlation}{{85}{31}{\newpage {}Appendix}{equation.177}{}}
\newlabel{eq: r correlation 2}{{86}{31}{\newpage {}Appendix}{equation.178}{}}
\newlabel{eq: bound on non-SV gradient-1}{{87}{32}{\newpage {}Appendix}{equation.179}{}}
\newlabel{eq:gamma}{{88}{32}{\newpage {}Appendix}{equation.180}{}}
\newlabel{eq: temp main term, degenerate}{{89}{33}{\newpage {}Appendix}{equation.181}{}}
\newlabel{eq:psi}{{90}{33}{\newpage {}Appendix}{equation.182}{}}
\newlabel{eq: psi bound}{{91}{33}{\newpage {}Appendix}{equation.183}{}}
\newlabel{eq: main term, degenerate}{{92}{33}{\newpage {}Appendix}{equation.184}{}}
\newlabel{eq: the last term}{{94}{35}{\newpage {}Appendix}{equation.186}{}}
\newlabel{eq:case1}{{95}{35}{\newpage {}Appendix}{equation.187}{}}
\newlabel{eq.case1a}{{96}{36}{\newpage {}Appendix}{equation.190}{}}
\newlabel{eq:tmp}{{97}{37}{\newpage {}Appendix}{equation.195}{}}
\newlabel{eq:case2b}{{98}{38}{\newpage {}Appendix}{equation.200}{}}
\newlabel{subsec: existence 1}{{C.4}{38}{\newpage {}Appendix}{subsection.202}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}Proof of the existence and uniqueness of the solution to eqs. \ref  {eq: w tilde-1}-\ref  {eq: w tilde constraints} }{38}{subsection.202}}
\newlabel{eq: w tilde-1-1}{{100}{38}{\newpage {}Appendix}{equation.203}{}}
\newlabel{eq: w tilde constraints-1}{{101}{38}{\newpage {}Appendix}{equation.204}{}}
\newlabel{eq: w_tilde equation}{{102}{39}{\newpage {}Appendix}{equation.206}{}}
\newlabel{lem: existence of solutions}{{17}{39}{\newpage {}Appendix}{equation.206}{}}
\newlabel{eq: v no zero eigen value}{{103}{39}{\newpage {}Appendix}{equation.207}{}}
\newlabel{eq: null space}{{104}{39}{\newpage {}Appendix}{equation.208}{}}
\newlabel{eq: s equation}{{105}{39}{\newpage {}Appendix}{equation.209}{}}
\newlabel{eq: sum v full}{{106}{39}{\newpage {}Appendix}{equation.210}{}}
\newlabel{eq: s1 solution}{{107}{39}{\newpage {}Appendix}{equation.211}{}}
\newlabel{eq: sum v i>1}{{108}{39}{\newpage {}Appendix}{equation.212}{}}
\newlabel{eq: alpha existence-1}{{109}{40}{\newpage {}Appendix}{equation.213}{}}
\newlabel{eq: s v alpha}{{110}{40}{\newpage {}Appendix}{equation.214}{}}
\newlabel{subsec: existence 2}{{C.5}{41}{\newpage {}Appendix}{subsection.215}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5}Proof of the existence and uniqueness of the solution to eqs. \ref  {eq: w check}-\ref  {eq: w check constraints}}{41}{subsection.215}}
\newlabel{eq: w check proof}{{111}{41}{\newpage {}Appendix}{equation.217}{}}
\newlabel{eq: w check constraints-1}{{112}{41}{\newpage {}Appendix}{equation.218}{}}
\newlabel{eq: identity}{{113}{41}{\newpage {}Appendix}{equation.219}{}}
\newlabel{eq: w wheck re-param}{{114}{41}{\newpage {}Appendix}{equation.220}{}}
\newlabel{eq: rank W}{{115}{41}{\newpage {}Appendix}{equation.221}{}}
\newlabel{eq: u equation}{{116}{41}{\newpage {}Appendix}{equation.222}{}}
\newlabel{eq: rank check}{{117}{42}{\newpage {}Appendix}{equation.223}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.6}Proof of Lemma \ref  {lem: PhiSum}}{42}{subsection.224}}
\newlabel{subsec: PhiSumProof}{{C.6}{42}{\newpage {}Appendix}{subsection.224}{}}
\@writefile{loe}{\contentsline {lemR}{\numberline {15}Lemma}{42}{lemR.226}}
\newlabel{sec:Calculation-of-convergence rates}{{D}{44}{\newpage {}Appendix}{section.229}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Calculation of convergence rates }{44}{section.229}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Proof of Theorem \ref  {thm: rates}}{44}{subsection.230}}
\newlabel{eq: normalized weight vector full}{{118}{44}{\newpage {}Appendix}{equation.231}{}}
\newlabel{eq: margin full}{{119}{45}{\newpage {}Appendix}{equation.232}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Validation error lower bound}{46}{subsection.234}}
\newlabel{sec:Softmax-output-with-cross-entropy-loss}{{E}{46}{\newpage {}Appendix}{section.235}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Softmax output with cross-entropy loss}{46}{section.235}}
\newlabel{eq: cross-entropy loss}{{121}{47}{\newpage {}Appendix}{equation.236}{}}
\@writefile{loe}{\contentsline {assm}{\numberline {4}Assumption}{47}{assm.238}}
\newlabel{assum: multi-class seperability}{{4}{47}{}{assm.238}{}}
\newlabel{lem: convergence of softmax-cross-entropy}{{19}{47}{\newpage {}Appendix}{thm.239}{}}
\@writefile{loe}{\contentsline {thmR}{\numberline {7}Theorem}{47}{thmR.241}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}Notations and Definitions}{48}{subsection.244}}
\newlabel{eq: what def with alpha for CE}{{123}{48}{\newpage {}Appendix}{equation.246}{}}
\newlabel{eq:rdef}{{124}{48}{\newpage {}Appendix}{equation.247}{}}
\newlabel{eq:1}{{125}{48}{\newpage {}Appendix}{equation.248}{}}
\newlabel{eq:alpha_def}{{126}{48}{\newpage {}Appendix}{equation.249}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}Auxiliary Lemma}{49}{subsection.250}}
\newlabel{thmt@@multiAuxlemma@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )5.\@arabic {\c@equation }}\setcounter {equation}{126}}{49}{\newpage {}Appendix}{subsection.250}{}}
\@writefile{loe}{\contentsline {lemR}{\numberline {20}Lemma}{49}{lemR.252}}
\newlabel{thmt@@multiAuxlemma}{{20}{49}{}{lemR.252}{}}
\newlabel{lemma:1}{{20}{49}{}{lemR.252}{}}
\newlabel{eq:lemma}{{127}{49}{}{equation.253}{}}
\newlabel{eq:P_ge_epsilon1}{{128}{49}{}{equation.254}{}}
\newlabel{eq:lemmaImproved}{{129}{49}{}{equation.255}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.3}Proof of Theorem \ref  {thm:mainMulti}}{49}{subsection.256}}
\newlabel{eq: r(t+1) norm, CE}{{130}{49}{\newpage {}Appendix}{equation.257}{}}
\newlabel{eq: square r difference, CE case}{{131}{49}{\newpage {}Appendix}{equation.258}{}}
\newlabel{eq: norm grad squared 1/t, CE case}{{133}{49}{\newpage {}Appendix}{equation.260}{}}
\newlabel{eq: square norm of r difference-1, CE}{{134}{49}{\newpage {}Appendix}{equation.261}{}}
\newlabel{eq:6}{{136}{50}{\newpage {}Appendix}{equation.263}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.4}Proof of Lemma \ref  {lemma:1}}{50}{subsection.264}}
\newlabel{appendix: proof of auxilary Lemma CE}{{E.4}{50}{\newpage {}Appendix}{subsection.264}{}}
\@writefile{loe}{\contentsline {lemR}{\numberline {20}Lemma}{50}{lemR.266}}
\newlabel{eq: (r(t+1)-r(t))r(t) bound Multi}{{137}{50}{\newpage {}Appendix}{equation.270}{}}
\newlabel{eq: w_hat r bound 1 Multi}{{139}{51}{\newpage {}Appendix}{equation.272}{}}
\newlabel{eq: second term}{{141}{52}{\newpage {}Appendix}{equation.274}{}}
\newlabel{eq: g(t) 2 terms sum, n s.v. and xr not bounded}{{145}{54}{\newpage {}Appendix}{equation.278}{}}
\newlabel{eq: second term improved bound}{{150}{55}{\newpage {}Appendix}{equation.283}{}}
\newlabel{sec:Additional-Figures}{{F}{55}{\newpage {}Appendix}{section.285}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F}An experiment with stochastic gradient descent }{55}{section.285}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Same as Fig. \ref  {fig:Synthetic-dataset}, except stochastic gradient decent is used (with mini-batch of size 4), instead of GD. }}{55}{figure.287}}
\newlabel{fig: SGD}{{4}{55}{Same as Fig. \ref {fig:Synthetic-dataset}, except stochastic gradient decent is used (with mini-batch of size 4), instead of GD}{figure.287}{}}
\bibcite{Nacson2018b}{{1}{2018}{{Nacson et~al.}}{{ Nacson, Srebro, Soudry}}}
\bibcite{Gunasekar2018b}{{2}{2018b}{{Gunasekar et~al.}}{{ Gunasekar, Lee, Soudry, Srebro}}}
\bibcite{duchi2011adaptive}{{3}{2011}{{Duchi et~al.}}{{Duchi, Hazan, and Singer}}}
\bibcite{duchi2011adaptive}{{4}{2011}{{Duchi et~al.}}{{Duchi, Hazan, and Singer}}}
\bibcite{Ganti2015}{{5}{2015}{{Ganti}}{{}}}
\bibcite{Gunasekar2017}{{6}{2017}{{Gunasekar et~al.}}{{Gunasekar, Woodworth, Bhojanapalli, Neyshabur, and Srebro}}}
\bibcite{gunasekar2018characterizing}{{7}{2018}{{Gunasekar et~al.}}{{Gunasekar, Lee, Soudry, and Srebro}}}
\bibcite{Hardt2015a}{{8}{2016}{{Hardt et~al.}}{{Hardt, Recht, and Singer}}}
\bibcite{Hoffer2017a}{{9}{2017}{{Hoffer et~al.}}{{Hoffer, Hubara, and Soudry}}}
\bibcite{Hubara2016}{{10}{2018}{{Hubara et~al.}}{{Hubara, Courbariaux, Soudry, El-yaniv, and Bengio}}}
\bibcite{Ji2018}{{11}{2018}{{Ji and Telgarsky}}{{}}}
\bibcite{Keskar2016}{{12}{2017}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{Kingma2015}{{13}{2015}{{Kingma and Ba}}{{}}}
\bibcite{Nacson2018}{{14}{2018}{{Nacson et~al.}}{{Nacson, Lee, Gunasekar, Srebro, and Soudry}}}
\bibcite{neyshabur2014search}{{15}{2014}{{Neyshabur et~al.}}{{Neyshabur, Tomioka, and Srebro}}}
\bibcite{neyshabur2015path}{{16}{2015}{{Neyshabur et~al.}}{{Neyshabur, Salakhutdinov, and Srebro}}}
\bibcite{Neyshabur2017a}{{17}{2017}{{Neyshabur et~al.}}{{Neyshabur, Bhojanapalli, McAllester, and Srebro}}}
\bibcite{Rosset2004}{{18}{2004}{{Rosset et~al.}}{{Rosset, Zhu, and Hastie}}}
\bibcite{schapire1998boosting}{{19}{1998}{{Schapire et~al.}}{{Schapire, Freund, Bartlett, Lee, et~al.}}}
\bibcite{Soudry2017a}{{20}{2018}{{Soudry et~al.}}{{Soudry, Hoffer, {Shpigel Nacson}, and Srebro}}}
\bibcite{telgarsky2013margins}{{21}{2013}{{Telgarsky}}{{}}}
\bibcite{Wilson2017}{{22}{2017}{{Wilson et~al.}}{{Wilson, Roelofs, Stern, Srebro, and Recht}}}
\bibcite{Zhang2016}{{23}{2017}{{Zhang et~al.}}{{Zhang, Bengio, Hardt, Recht, and Vinyals}}}
\bibcite{zhang2005boosting}{{24}{2005}{{Zhang et~al.}}{{Zhang, Yu, et~al.}}}
\newlabel{LastPage}{{}{57}{}{page.57}{}}
\xdef\lastpage@lastpage{57}
\xdef\lastpage@lastpageHy{57}
