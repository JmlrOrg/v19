@article{gunasekar2018characterizing,
    title={Characterizing Implicit Bias in Terms of Optimization Geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1802.08246},
  year={2018}
}
@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}

@unpublished{Ji2018,
author = {Ji, Ziwei and Telgarsky, Matus},
title = {{Risk and parameter convergence of logistic regression}},
year = {2018},
note = {Communicated by the authors}
}

@article{Nacson2018b,
archivePrefix = {arXiv},
arxivId = {1806.01796},
author = {Nacson, Mor Shpigel and Srebro, Nathan and Soudry, Daniel},
eprint = {1806.01796},
file = {:C$\backslash$:/Users/danie/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nacson, Srebro, Soudry - 2018 - Stochastic Gradient Descent on Separable Data Exact Convergence with a Fixed Learning Rate.pdf:pdf},
month = {jun},
title = {{Stochastic Gradient Descent on Separable Data: Exact Convergence with a Fixed Learning Rate}},
url = {http://arxiv.org/abs/1806.01796},
year = {2018}
}


@inproceedings{Gunasekar2018b,
archivePrefix = {arXiv},
arxivId = {1806.00468},
author = {Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
booktitle = {NIPS},
eprint = {1806.00468},
file = {:C$\backslash$:/Users/danie/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gunasekar et al. - 2018 - Implicit Bias of Gradient Descent on Linear Convolutional Networks.pdf:pdf},
month = {jun},
title = {{Implicit Bias of Gradient Descent on Linear Convolutional Networks}},
url = {http://arxiv.org/abs/1806.00468},
year = {2018}
}


@inproceedings{telgarsky2013margins,
  title={Margins, shrinkage and boosting},
  author={Telgarsky, Matus},
  booktitle={Proceedings of the 30th International Conference on International Conference on Machine Learning-Volume 28},
  pages={II--307},
  year={2013},
  organization={JMLR. org}
}

@article{Nacson2018,
archivePrefix = {arXiv},
arxivId = {1803.01905},
author = {Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Srebro, Nathan and Soudry, Daniel},
eprint = {1803.01905},
file = {:C$\backslash$:/Users/danie/Downloads/1803.01905 (2).pdf:pdf},
journal = {arXiv},
pages = {1--45},
title = {{Convergence of Gradient Descent on Separable Data}},
year = {2018}
}


@inproceedings{Soudry2017a,
author = {Soudry, Daniel and Hoffer, Elad and {Shpigel Nacson}, Mor and Srebro, N},
booktitle = {ICLR},
title = {{The Implicit Bias of Gradient Descent on Separable Data}},
year = {2018}
}


@inproceedings{neyshabur2015path,
  title={Path-sgd: Path-normalized optimization in deep neural networks},
  author={Neyshabur, Behnam and Salakhutdinov, Ruslan R and Srebro, Nati},
  booktitle={NIPS},
  year={2015}
}

@misc{Ganti2015,
author = {Ganti, RadhaKrishna},
title = {{EE6151, Convex optimization algorithms. Unconstrained minimization: Gradient descent algorithm}},
url = {https://rkganti.wordpress.com/2015/08/20/unconstrained-minimization-gradient-descent-algorithm/},
year = {2015}
}



@Article{Shalev-Shwartz2010boosting,
author="Shalev-Shwartz, Shai
and Singer, Yoram",
title="On the equivalence of weak learnability and linear separability: new relaxations and efficient boosting algorithms",
journal="Machine Learning",
year="2010",
month="Sep",
day="01",
volume="80",
number="2",
pages="141--163",
abstract="Boosting algorithms build highly accurate prediction mechanisms from a collection of low-accuracy predictors. To do so, they employ the notion of weak-learnability. The starting point of this paper is a proof which shows that weak learnability is equivalent to linear separability with l                1 margin. The equivalence is a direct consequence of von Neumann's minimax theorem. Nonetheless, we derive the equivalence directly using Fenchel duality. We then use our derivation to describe a family of relaxations to the weak-learnability assumption that readily translates to a family of relaxations of linear separability with margin. This alternative perspective sheds new light on known soft-margin boosting algorithms and also enables us to derive several new relaxations of the notion of linear separability. Last, we describe and analyze an efficient boosting framework that can be used for minimizing the loss functions derived from our family of relaxations. In particular, we obtain efficient boosting algorithms for maximizing hard and soft versions of the l                1 margin.",
issn="1573-0565",
doi="10.1007/s10994-010-5173-z",

}


@article{schapire1998boosting,
  title={Boosting the margin: A new explanation for the effectiveness of voting methods},
  author={Schapire, Robert E and Freund, Yoav and Bartlett, Peter and Lee, Wee Sun and others},
  journal={The annals of statistics},
  volume={26},
  number={5},
  pages={1651--1686},
  year={1998},
  publisher={Institute of Mathematical Statistics}
}

@article{rosset2004boosting,
  title={Boosting as a regularized path to a maximum margin classifier},
  author={Rosset, Saharon and Zhu, Ji and Hastie, Trevor},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Aug},
  pages={941--973},
  year={2004}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2014}
}


@article{zhang2005boosting,
  title={Boosting with early stopping: Convergence and consistency},
  author={Zhang, Tong and Yu, Bin and others},
  journal={The Annals of Statistics},
  volume={33},
  number={4},
  pages={1538--1579},
  year={2005},
  publisher={Institute of Mathematical Statistics}
}