\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{DBLP:conf/nips/YiPCC16}
\citation{Epstein95,Ho03}
\citation{ASI:ASI1}
\citation{Price2006}
\citation{robust_pca09,Chandrasekaran_Sanghavi_Parrilo_Willsky_2009,Clarkson:2013:LRA:2488608.2488620,Frieze:2004:FMA:1039488.1039494,Bhojanapalli:2015:TLA:2722129.2722191,DBLP:conf/nips/YiPCC16,ChenWainwright2015,conf/aistats/GuWL16,DBLP:journals/corr/CherapanamjeriG16,NIPS2014_5430}
\citation{Li_backgroundsubtraction}
\citation{Basri03}
\citation{robust_pca09}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{robust_pca09,Chandrasekaran_Sanghavi_Parrilo_Willsky_2009}
\citation{5934412}
\citation{robust_pca09}
\citation{robust_pca09}
\citation{6319655}
\citation{NIPS2014_5430}
\citation{ChenWainwright2015}
\citation{conf/aistats/GuWL16}
\citation{DBLP:conf/nips/YiPCC16}
\citation{NIPS2011_4486,7035075,7809181,DBLP:journals/corr/CherapanamjeriG16}
\citation{DBLP:conf/nips/YiPCC16}
\citation{conf/aistats/GuWL16}
\citation{DBLP:conf/nips/YiPCC16}
\citation{ruhe1974numerical}
\citation{Burer2003,Burer2005}
\citation{7755794}
\citation{doi:10.1137/16M105808X}
\citation{pmlr-v49-bandeira16}
\citation{Jain:2013:LMC:2488608.2488693}
\citation{DBLP:conf/icml/TuBSSR16}
\citation{ChenWainwright2015,pmlr-v54-wang17b,Park2016,pmlr-v54-wang17b,pmlr-v54-park17a}
\citation{DeSa:2015:GCS:3045118.3045366}
\newlabel{eq:convex}{{1}{2}{}{equation.2}{}}
\citation{DBLP:conf/nips/YiPCC16}
\citation{DBLP:conf/nips/YiPCC16}
\citation{DBLP:conf/nips/YiPCC16}
\citation{DBLP:conf/nips/YiPCC16}
\citation{doi:10.1137/15M1025153}
\@writefile{toc}{\contentsline {section}{\numberline {2}Algorithm}{3}{section.3}}
\newlabel{sec:alg}{{2}{3}{}{section.3}{}}
\newlabel{eq:objective}{{2}{3}{}{equation.4}{}}
\newlabel{eq:threshold}{{3}{4}{}{equation.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient descent on the manifold under the fully observed setting.}}{4}{algorithm.6}}
\newlabel{alg:gradient}{{1}{4}{}{algorithm.6}{}}
\newlabel{eq:threshold2}{{4}{4}{}{equation.7}{}}
\newlabel{eq:objective2}{{5}{4}{}{equation.8}{}}
\citation{Vandereycken2013,Shalit:2012:OLE:2503308.2188399,absil2009optimization}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Gradient descent on the manifold under the partially observed setting.}}{5}{algorithm.9}}
\newlabel{alg:gradient2}{{2}{5}{}{algorithm.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Derivation of the Proposed Algorithms}{5}{section.10}}
\newlabel{sec:review_algo}{{3}{5}{}{section.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Manifold optimization}{5}{subsection.11}}
\newlabel{sec:background}{{3.1}{5}{}{subsection.11}{}}
\citation{Vandereycken2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The visualization of gradient descent algorithms on the manifold $\mathcal  {M}$. The black solid line is the Euclidean gradient. The blue solid line is the projection of the Euclidean gradient to the tangent space. The red solid line represents the orthographic retraction, while the red dashed line represents the projective retraction.}}{6}{figure.13}}
\newlabel{fig:visualization}{{1}{6}{The visualization of gradient descent algorithms on the manifold $\mathcal {M}$. The black solid line is the Euclidean gradient. The blue solid line is the projection of the Euclidean gradient to the tangent space. The red solid line represents the orthographic retraction, while the red dashed line represents the projective retraction}{figure.13}{}}
\newlabel{eq:gradient_manifold}{{6}{6}{}{equation.12}{}}
\citation{Absil2015}
\citation{Absil2015}
\citation{Absil2015}
\citation{Absil2015}
\citation{Shalit:2012:OLE:2503308.2188399,Vandereycken2013}
\citation{Absil2015}
\citation{Absil2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The geometry of the manifold of low-rank matrices}{7}{subsection.14}}
\newlabel{sec:geometry}{{3.2}{7}{}{subsection.14}{}}
\newlabel{eq:projection}{{7}{7}{}{equation.15}{}}
\newlabel{eq:projective}{{8}{7}{}{equation.16}{}}
\newlabel{eq:projective2}{{9}{7}{}{equation.17}{}}
\newlabel{eq:ortho_retraction}{{10}{7}{}{equation.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Derivation of the proposed algorithms}{7}{subsection.19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Derivation of Algorithm 1}{7}{subsubsection.20}}
\newlabel{sec:derivation1}{{3.3.1}{7}{}{subsubsection.20}{}}
\newlabel{eq:gradient}{{11}{7}{}{equation.21}{}}
\citation{absil2009optimization}
\newlabel{eq:derivative}{{12}{8}{}{equation.22}{}}
\newlabel{eq:alg_projective}{{13}{8}{}{equation.23}{}}
\newlabel{eq:ortho_retraction1}{{14}{8}{}{equation.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Derivation of Algorithm 2}{8}{subsubsection.25}}
\newlabel{sec:partial}{{3.3.2}{8}{}{subsubsection.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Basic convergence properties of Algorithms\nobreakspace  {}\ref  {alg:gradient} and\nobreakspace  {}\ref  {alg:gradient2}}{8}{subsubsection.26}}
\citation{Vandereycken2013,Shalit:2012:OLE:2503308.2188399,absil2009optimization}
\citation{absil2009optimization}
\citation{5466511,Vandereycken2013,doi:10.1137/15M1050525}
\citation{doi:10.1137/15M1025153}
\citation{doi:10.1137/15M1025153}
\citation{doi:10.1137/15M1050525}
\citation{doi:10.1137/15M1050525}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Prior works on manifold optimization}{9}{subsection.27}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Theoretical Analysis}{9}{section.28}}
\newlabel{sec:theory}{{4}{9}{}{section.28}{}}
\newlabel{eq:ass1}{{15}{9}{}{equation.30}{}}
\newlabel{eq:ass2}{{16}{9}{}{equation.32}{}}
\citation{DBLP:conf/nips/YiPCC16}
\citation{DBLP:conf/nips/YiPCC16}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Analysis of Algorithm\nobreakspace  {}\ref  {alg:gradient}}{10}{subsection.33}}
\newlabel{thm:main}{{1}{10}{Linear convergence rate, fully observed case}{theorem.34}{}}
\newlabel{thm:initialization}{{4}{10}{Initialization, fully observed case}{theorem.37}{}}
\newlabel{thm:noisy}{{5}{11}{Stability, fully observed case}{theorem.38}{}}
\newlabel{eq:prob}{{17}{11}{Stability, fully observed case}{equation.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Analysis of Algorithm\nobreakspace  {}\ref  {alg:gradient2}}{11}{subsection.40}}
\newlabel{thm:main2}{{6}{11}{Linear convergence rate, partially observed case}{theorem.41}{}}
\newlabel{eq:main2}{{18}{11}{Linear convergence rate, partially observed case}{equation.42}{}}
\newlabel{eq:requirement}{{19}{11}{}{equation.44}{}}
\citation{DBLP:conf/nips/YiPCC16}
\citation{DBLP:conf/nips/YiPCC16}
\newlabel{thm:initialization2}{{9}{12}{Initialization, partially observed case}{theorem.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Comparison with Alternating Gradient Descent}{12}{subsection.47}}
\newlabel{sec:compare}{{4.3}{12}{}{subsection.47}{}}
\citation{DBLP:conf/nips/YiPCC16}
\citation{DBLP:conf/nips/YiPCC16}
\citation{DBLP:conf/nips/YiPCC16}
\citation{NIPS2014_5430}
\citation{NIPS2014_5430}
\citation{conf/aistats/GuWL16}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces {\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {}{Comparison of the theoretical guarantees in our work and the alternating gradient descent algorithm in \cite  {DBLP:conf/nips/YiPCC16}. The four criteria are explained in details in Section\nobreakspace  {}\ref  {sec:compare}. }}}}{13}{table.52}}
\newlabel{tab:compare}{{1}{13}{\small {}{Comparison of the theoretical guarantees in our work and the alternating gradient descent algorithm in \cite {DBLP:conf/nips/YiPCC16}. The four criteria are explained in details in Section~\ref {sec:compare}. \label {tab:compare}}}{table.52}{}}
\newlabel{eq:factor}{{20}{13}{}{equation.53}{}}
\citation{Shen_anaccelerated}
\citation{NIPS2014_5430,conf/aistats/GuWL16}
\citation{NIPS2014_5430}
\citation{conf/aistats/GuWL16}
\citation{DBLP:conf/nips/YiPCC16}
\citation{NIPS2014_5430}
\citation{conf/aistats/GuWL16}
\citation{DBLP:conf/nips/YiPCC16}
\citation{NIPS2014_5430}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Comparison with other robust PCA algorithms}{14}{subsection.54}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces {\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {}{Comparison of the theoretical guarantees in our work and some other robust PCA algorithms. }}}}{14}{table.55}}
\newlabel{tab:compare2}{{2}{14}{\small {}{Comparison of the theoretical guarantees in our work and some other robust PCA algorithms. \label {tab:compare2}}}{table.55}{}}
\citation{DBLP:conf/nips/YiPCC16}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulations}{15}{section.56}}
\newlabel{sec:simu}{{5}{15}{}{section.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Choice of parameters}{15}{subsection.57}}
\newlabel{sec:simu1}{{5.1}{15}{}{subsection.57}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Performance of the proposed algorithm}{15}{subsection.60}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The dependence of the estimation error on the number of iterations for different step sizes $\eta $ (a) Algorithm\nobreakspace  {}\ref  {alg:gradient} (Option 1); (b) Algorithm\nobreakspace  {}\ref  {alg:gradient} (Option 2); (c) Algorithm\nobreakspace  {}\ref  {alg:gradient2} (Option 1); (d) Algorithm\nobreakspace  {}\ref  {alg:gradient2} (Option 2).}}{16}{figure.58}}
\newlabel{fig:options}{{2}{16}{The dependence of the estimation error on the number of iterations for different step sizes $\eta $ (a) Algorithm~\ref {alg:gradient} (Option 1); (b) Algorithm~\ref {alg:gradient} (Option 2); (c) Algorithm~\ref {alg:gradient2} (Option 1); (d) Algorithm~\ref {alg:gradient2} (Option 2)}{figure.58}{}}
\newlabel{fig:alpha}{{5.1}{16}{}{figure.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The convergence of the algorithm depending on the choice of $\gamma $. (a) fully observed setting; (b) partially observed setting.}}{16}{figure.59}}
\citation{doi:10.1137/15M1025153}
\citation{DBLP:conf/nips/YiPCC16}
\citation{Shen_anaccelerated}
\citation{Shen_anaccelerated}
\citation{robust_pca09}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Comparison with other robust PCA algorithms}{17}{subsection.62}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Dependence of the estimation error on the number of iterations for different (a) Overall ratios of corrupted entries $q$ (Algorithm 1); (b) Ranks $r$ (Algorithm 1); (c) Condition numbers $\kappa $ (Algorithm 1); (d) Matrix sizes $[n_{1},n_{2}]$ (Algorithm 1); (e) Subsampling ratio $p$ (Algorithm 2).}}{18}{figure.61}}
\newlabel{fig:performance}{{4}{18}{Dependence of the estimation error on the number of iterations for different (a) Overall ratios of corrupted entries $q$ (Algorithm 1); (b) Ranks $r$ (Algorithm 1); (c) Condition numbers $\kappa $ (Algorithm 1); (d) Matrix sizes $[n_{1},n_{2}]$ (Algorithm 1); (e) Subsampling ratio $p$ (Algorithm 2)}{figure.61}{}}
\citation{DBLP:conf/nips/YiPCC16}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The comparison of the performance of the algorithms under the fully observed setting. The running time is measured in seconds.}}{19}{figure.63}}
\newlabel{fig:comparison}{{5}{19}{The comparison of the performance of the algorithms under the fully observed setting. The running time is measured in seconds}{figure.63}{}}
\citation{DBLP:conf/nips/YiPCC16}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The comparison of the performances of the algorithms under the partially observed setting. The running time is measured in seconds.}}{20}{figure.64}}
\newlabel{fig:partial}{{6}{20}{The comparison of the performances of the algorithms under the partially observed setting. The running time is measured in seconds}{figure.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The performance of Algorithms\nobreakspace  {}\ref  {alg:gradient} and\nobreakspace  {}\ref  {alg:gradient2} in video background subtraction, with three rows representing three frames in the video sequence. For Algorithm\nobreakspace  {}\ref  {alg:gradient2}, a subsampling ratio of $p=0.5$ is used.}}{21}{figure.66}}
\newlabel{fig:DD3}{{7}{21}{The performance of Algorithms~\ref {alg:gradient} and~\ref {alg:gradient2} in video background subtraction, with three rows representing three frames in the video sequence. For Algorithm~\ref {alg:gradient2}, a subsampling ratio of $p=0.5$ is used}{figure.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The relative error of Algorithms\nobreakspace  {}\ref  {alg:gradient}, \ref  {alg:gradient2}, and AGD with respect to the iterations, for both fully observed case and partially observed case in the experiment of background subtraction.}}{21}{figure.67}}
\newlabel{fig:DD4}{{8}{21}{The relative error of Algorithms~\ref {alg:gradient}, \ref {alg:gradient2}, and AGD with respect to the iterations, for both fully observed case and partially observed case in the experiment of background subtraction}{figure.67}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{22}{section.68}}
\newlabel{sec:discussion}{{6}{22}{}{section.68}{}}
\newlabel{eq:inter}{{22}{23}{Verification of \eqref {eq:ortho_retraction1}}{equation.77}{}}
\newlabel{eq:dv}{{23}{24}{Verification of \eqref {eq:ortho_retraction1}}{equation.78}{}}
\newlabel{eq:projection2}{{24}{24}{Verification of \eqref {eq:ortho_retraction1}}{equation.79}{}}
\newlabel{eq:goal1}{{25}{24}{B. Proof of Theorem~\ref {thm:main}}{equation.81}{}}
\newlabel{lemma:bD}{{10}{24}{B. Proof of Theorem~\ref {thm:main}}{theorem.82}{}}
\newlabel{eq:second_bound}{{26}{24}{B. Proof of Theorem~\ref {thm:main}}{equation.83}{}}
\newlabel{eq:second_bound_noisy}{{27}{24}{B. Proof of Theorem~\ref {thm:main}}{equation.84}{}}
\newlabel{lemma:approximation}{{11}{24}{B. Proof of Theorem~\ref {thm:main}}{theorem.85}{}}
\newlabel{eq:control1}{{28}{24}{B. Proof of Theorem~\ref {thm:main}}{equation.86}{}}
\newlabel{eq:control2}{{29}{24}{B. Proof of Theorem~\ref {thm:main}}{equation.87}{}}
\newlabel{lemma:retraction}{{12}{24}{B. Proof of Theorem~\ref {thm:main}}{theorem.88}{}}
\newlabel{eq:condition1}{{30}{25}{B. Proof of Theorem~\ref {thm:main}}{equation.89}{}}
\newlabel{eq:condition1.5}{{31}{25}{B. Proof of Theorem~\ref {thm:main}}{equation.90}{}}
\newlabel{eq:condition1.1}{{32}{25}{B. Proof of Theorem~\ref {thm:main}}{equation.91}{}}
\newlabel{eq:condition1.2}{{33}{25}{B. Proof of Theorem~\ref {thm:main}}{equation.92}{}}
\newlabel{eq:condition3.5}{{34}{25}{B. Proof of Theorem~\ref {thm:main}}{equation.93}{}}
\newlabel{eq:condition4}{{35}{25}{B. Proof of Theorem~\ref {thm:main}}{equation.94}{}}
\citation{DBLP:conf/nips/YiPCC16}
\citation{DBLP:conf/nips/YiPCC16}
\citation{DBLP:conf/nips/YiPCC16}
\newlabel{lemma:prob}{{13}{26}{C. Proof of Theorem~\ref {thm:noisy}}{theorem.96}{}}
\newlabel{eq:event_noisy}{{36}{26}{C. Proof of Theorem~\ref {thm:noisy}}{equation.97}{}}
\newlabel{lemma:concentration}{{14}{26}{D. Proof of Theorem~\ref {thm:main2}}{theorem.99}{}}
\newlabel{lemma:sampling}{{15}{26}{D. Proof of Theorem~\ref {thm:main2}}{theorem.100}{}}
\newlabel{lemma:bD2}{{16}{27}{D. Proof of Theorem~\ref {thm:main2}}{theorem.101}{}}
\newlabel{eq:second_bound2}{{37}{27}{D. Proof of Theorem~\ref {thm:main2}}{equation.102}{}}
\newlabel{eq:condition1c}{{38}{27}{D. Proof of Theorem~\ref {thm:main2}}{equation.103}{}}
\newlabel{eq:aa}{{39}{27}{D. Proof of Theorem~\ref {thm:main2}}{equation.104}{}}
\newlabel{eq:condition1c2}{{40}{27}{D. Proof of Theorem~\ref {thm:main2}}{equation.105}{}}
\newlabel{eq:estimation1}{{41}{29}{Lemma~\ref {lemma:bD}(a)}{equation.108}{}}
\newlabel{eq:med1}{{42}{29}{Lemma~\ref {lemma:bD}(a)}{equation.109}{}}
\newlabel{eq:med2}{{43}{29}{Lemma~\ref {lemma:bD}(a)}{equation.110}{}}
\newlabel{eq:estimation2}{{44}{29}{Lemma~\ref {lemma:bD}(a)}{equation.111}{}}
\newlabel{eq:estimation3}{{45}{30}{Lemma~\ref {lemma:bD}(a)}{equation.112}{}}
\newlabel{eq:estimation4}{{46}{30}{Lemma~\ref {lemma:bD}(a)}{equation.113}{}}
\newlabel{eq:temp}{{47}{31}{Lemma~\ref {lemma:approximation}}{equation.116}{}}
\citation{ledoux1991probability}
\citation{doi:10.1137/0707001}
\citation{vershynin12}
\citation{ledoux1991probability}
\newlabel{eq:claim}{{48}{32}{Lemma~\ref {lemma:prob}}{equation.119}{}}
\newlabel{eq:intermediate1}{{49}{32}{Lemma~\ref {lemma:prob}}{equation.120}{}}
\newlabel{eq:intermediate2}{{50}{32}{Lemma~\ref {lemma:prob}}{equation.121}{}}
\newlabel{eq:covering1}{{51}{33}{Lemma~\ref {lemma:prob}}{equation.122}{}}
\newlabel{eq:covering2}{{52}{33}{Lemma~\ref {lemma:prob}}{equation.123}{}}
\newlabel{eq:covering3}{{53}{33}{Lemma~\ref {lemma:prob}}{equation.124}{}}
\bibdata{bib-online}
\bibcite{Absil2015}{{1}{2015}{{Absil and Oseledets}}{{}}}
\newlabel{eq:convering4}{{54}{34}{Lemma~\ref {lemma:prob}}{equation.125}{}}
\bibcite{absil2009optimization}{{2}{2009}{{Absil et~al.}}{{Absil, Mahony, and Sepulchre}}}
\bibcite{pmlr-v49-bandeira16}{{3}{2016}{{Bandeira et~al.}}{{Bandeira, Boumal, and Voroninski}}}
\bibcite{Basri03}{{4}{2003}{{Basri and Jacobs}}{{}}}
\bibcite{Bhojanapalli:2015:TLA:2722129.2722191}{{5}{2015}{{Bhojanapalli et~al.}}{{Bhojanapalli, Jain, and Sanghavi}}}
\bibcite{doi:10.1137/16M105808X}{{6}{2016}{{Boumal}}{{}}}
\bibcite{Burer2003}{{7}{2003}{{Burer and Monteiro}}{{}}}
\bibcite{Burer2005}{{8}{2005}{{Burer and Monteiro}}{{}}}
\bibcite{doi:10.1137/15M1025153}{{9}{2016}{{Cambier and Absil}}{{}}}
\bibcite{robust_pca09}{{10}{2011}{{Cand\`{e}s et~al.}}{{Cand\`{e}s, Li, Ma, and Wright}}}
\bibcite{Chandrasekaran_Sanghavi_Parrilo_Willsky_2009}{{11}{2011}{{Chandrasekaran et~al.}}{{Chandrasekaran, Sanghavi, Parrilo, and Willsky}}}
\bibcite{ChenWainwright2015}{{12}{2015}{{Chen and Wainwright}}{{}}}
\bibcite{DBLP:journals/corr/CherapanamjeriG16}{{13}{2016}{{Cherapanamjeri et~al.}}{{Cherapanamjeri, Gupta, and Jain}}}
\bibcite{Clarkson:2013:LRA:2488608.2488620}{{14}{2013}{{Clarkson and Woodruff}}{{}}}
\bibcite{doi:10.1137/0707001}{{15}{1970}{{Davis and Kahan}}{{}}}
\bibcite{DeSa:2015:GCS:3045118.3045366}{{16}{2015}{{De~Sa et~al.}}{{De~Sa, Olukotun, and R{\'e}}}}
\bibcite{ASI:ASI1}{{17}{1990}{{Deerwester et~al.}}{{Deerwester, Dumais, Furnas, Landauer, and Harshman}}}
\bibcite{Epstein95}{{18}{1995}{{Epstein et~al.}}{{Epstein, Hallinan, and Yuille}}}
\bibcite{Frieze:2004:FMA:1039488.1039494}{{19}{2004}{{Frieze et~al.}}{{Frieze, Kannan, and Vempala}}}
\bibcite{conf/aistats/GuWL16}{{20}{2016}{{Gu et~al.}}{{Gu, Wang, and Liu}}}
\bibcite{Ho03}{{21}{2003}{{Ho et~al.}}{{Ho, Yang, Lim, Lee, and Kriegman}}}
\bibcite{5934412}{{22}{2011}{{Hsu et~al.}}{{Hsu, Kakade, and Zhang}}}
\bibcite{Jain:2013:LMC:2488608.2488693}{{23}{2013}{{Jain et~al.}}{{Jain, Netrapalli, and Sanghavi}}}
\bibcite{5466511}{{24}{2010}{{Keshavan et~al.}}{{Keshavan, Montanari, and Oh}}}
\bibcite{6319655}{{25}{2012}{{Kyrillidis and Cevher}}{{}}}
\bibcite{ledoux1991probability}{{26}{1991}{{Ledoux and Talagrand}}{{}}}
\bibcite{Li_backgroundsubtraction}{{27}{2004}{{Li et~al.}}{{Li, Huang, Gu, and Tian}}}
\bibcite{7035075}{{28}{2015}{{Li and Haupt}}{{}}}
\bibcite{NIPS2011_4486}{{29}{2011}{{Mackey et~al.}}{{Mackey, Jordan, and Talwalkar}}}
\bibcite{NIPS2014_5430}{{30}{2014}{{Netrapalli et~al.}}{{Netrapalli, U~N, Sanghavi, Anandkumar, and Jain}}}
\bibcite{Park2016}{{31}{2016}{{Park et~al.}}{{Park, Kyrillidis, Bhojanapalli, Caramanis, and Sanghavi}}}
\bibcite{pmlr-v54-park17a}{{32}{2017}{{Park et~al.}}{{Park, Kyrillidis, Carmanis, and Sanghavi}}}
\bibcite{Price2006}{{33}{2006}{{Price et~al.}}{{Price, Patterson, Plenge, Weinblatt, Shadick, and Reich}}}
\bibcite{7809181}{{34}{2017}{{Rahmani and Atia}}{{}}}
\bibcite{ruhe1974numerical}{{35}{1974}{{Ruhe}}{{}}}
\bibcite{Shalit:2012:OLE:2503308.2188399}{{36}{2012}{{Shalit et~al.}}{{Shalit, Weinshall, and Chechik}}}
\bibcite{7755794}{{37}{2017}{{Sun et~al.}}{{Sun, Qu, and Wright}}}
\bibcite{Shen_anaccelerated}{{38}{2010}{{Toh and Yun}}{{}}}
\bibcite{DBLP:conf/icml/TuBSSR16}{{39}{2016}{{Tu et~al.}}{{Tu, Boczar, Simchowitz, Soltanolkotabi, and Recht}}}
\bibcite{Vandereycken2013}{{40}{2013}{{Vandereycken}}{{}}}
\bibcite{vershynin12}{{41}{2012}{{Vershynin}}{{}}}
\bibcite{pmlr-v54-wang17b}{{42}{2017}{{Wang et~al.}}{{Wang, Zhang, and Gu}}}
\bibcite{doi:10.1137/15M1050525}{{43}{2016}{{Wei et~al.}}{{Wei, Cai, Chan, and Leung}}}
\bibcite{DBLP:conf/nips/YiPCC16}{{44}{2016}{{Yi et~al.}}{{Yi, Park, Chen, and Caramanis}}}
\newlabel{LastPage}{{}{39}{}{page.39}{}}
\xdef\lastpage@lastpage{39}
\xdef\lastpage@lastpageHy{39}
