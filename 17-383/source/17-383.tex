\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}
\usepackage{url}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
%\usepackage{amsmath,amsthm,paralist}
\usepackage{amsbsy}
\usepackage{cite}
%\usepackage{citesort}
\usepackage{algorithm,algorithmic}
%\usepackage[margin=1in]{geometry}
\usepackage{afterpage}
\usepackage{multirow}
\usepackage{caption}
\usepackage{bbm}
\usepackage{mathtools}

\usepackage{epstopdf}
\usepackage{xspace} 

\usepackage{comment}

\usepackage{hyperref}
\hypersetup{
    colorlinks=false, %set true if you want colored links
   % linktoc=all,     %set to all if you want both sections and subsections linked
  %  linkcolor=blue,  %choose some color if you want links to stand out
}

\allowdisplaybreaks

\def\real    { \mathbb{R} }
\def\reals   { \mathbb{R} }
\def \remarks {\noindent {\bf Remarks.}\ \ }
\newtheorem{thm}{Theorem}
%\newtheorem{lemma}{Lemma}
%\newtheorem{cor}{Corollary}
%\newtheorem{prop}{Proposition}
%\newtheorem{definition}{Definition}
%\newtheorem{prob}{Problem}
%\newtheorem*{goal}{Plan}
%\newtheorem{theorem}{Theorem}
%\newtheorem{lem}{Lemma}
%\theoremstyle{remark}
%\newtheorem{remark}{Remark}
%\newtheorem{example}{Example}

\renewcommand{\b}{\mathbf b}
\newcommand{\eps}{\epsilon}
\newcommand{\bitem}{\begin{itemize}}
\newcommand{\eitem}{\end{itemize}}
\newcommand{\goto}{\rightarrow}
\newcommand{\poiss}{\mathrm{Poisson}}
\newcommand{\mexp}{\mathrm{Exp}}
\newcommand{\mmax}{\mathrm{max}}
\newcommand{\mmin}{\mathrm{min}}
\newcommand{\msup}{\mathrm{sup}}
\newcommand{\minf}{\mathrm{inf}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\beqn}{\begin{equation}}
\newcommand{\eeqn}{\end{equation}}
\newcommand{\balign}{\begin{align}}
\newcommand{\ealign}{\end{align}}
\newcommand{\lam}{\lambda}
\newcommand{\inner}[1]{\left<#1\right>}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\RF}{membership index\xspace}
\def\x{{\mathbf x}}
\def\y{{\mathbf y}}
\def\n{{\mathbf n}}
\def\a{{\mathbf a}}
\def\s{{\mathbf s}}
\def\k{{\mathbf k}}
\def\u{{\mathbf u}}
\def\v{{\mathbf v}}
\def\z{{\mathbf z}}
\def\e{{\mathbf e}}
\def\w{{\mathbf w}}
\def\z{{\mathbf z}}
\def\balpha{{\mathbf x}}
\def\bA{{\mathbf A}}
\def\A{{\mathbf A}}
\def\B{{\mathbf B}}
\def\F{{\mathbf F}}
\def\H{{\mathbf H}}
\def\S{{\mathbf S}}
\def\bI{{\mathbf I}}
\def\I{{\mathbf I}}
\newcommand{\note}[1]{{\bf [{\em Note:} #1]}}
\newcommand{\dn}[1]{\textcolor{red}{#1 --dn}}
\newcommand{\vct}[1]{#1}
\newcommand{\mtx}[1]{#1}
\newcommand{\oper}[1]{\mathcal{#1}}
\def \R {\mathbb{R}}
\def \C {\mathbb{C}}
\DeclareMathOperator*{\argmin}{arg min}
\DeclareMathOperator*{\argmax}{arg max}
\DeclareMathOperator*{\trace}{tr}
\DeclareMathOperator*{\range}{range}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\sign}{sign}
\pagestyle{plain}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\renewcommand{\ss}{{\vspace*{-1mm}}}
\newcommand{\GamLi}[2]{\Lambda_{#1,#2}}  % \GamLi{\ell}{i}
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
            {-2.5ex\@plus -1ex \@minus -.25ex}%
            {1.25ex \@plus .25ex}%
            {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4} % how many sectioning \edit{level}s to assign numbers to
\setcounter{tocdepth}{4}    % how many sectioning \edit{level}s to show in ToC
\definecolor{orange}{rgb}{1,0.5,0}
\newcommand{\edit}[1]{{{#1}}}
\newcommand{\notate}[1]{\textcolor{Violet}{[{\em \bf #1}]}}
\newcommand{\RSnote}[1]{\textcolor{green}{[{\em {\bf **RS Note:} #1}]}}
\newcommand{\TWnote}[1]{\textcolor{magenta}{[{\em {\bf **TW Note:} #1}]}}
\newcommand{\DNnote}[1]{\textcolor{red}{[{\em {\bf **DN Note:} #1}]}}


\usepackage{lastpage}

\jmlrheading{19}{2018}{1-\pageref{LastPage}}{7/17; Revised
2/18}{10/18}{17-383}{Deanna Needell, Rayan Saab, and Tina Woolf}

\ShortHeadings{Simple Classification Using Binary Data}{Needell, Saab and Woolf}
\firstpageno{1}

\begin{document}

\title{Simple Classification Using Binary Data}

\author{\name Deanna Needell \email deanna@math.ucla.edu\\
   \addr Department of Mathematics\\
             520 Portola Plaza, University of California, Los Angeles, CA 90095\\
\AND
\name Rayan Saab \email rsaab@ucsd.edu\\
   \addr Department of Mathematics\\
             9500 Gilman Drive, University of California, La Jolla, CA 92093\\
\AND
\name Tina Woolf \email tina.woolf@cgu.edu\\
\addr Institute of Mathematical Sciences \\
 150 E. 10th Street, Claremont Graduate University, Claremont CA 91711}

\editor{David Wipf}

\maketitle

\begin{abstract}%
Binary, or one-bit, representations of data arise naturally in many applications, and are appealing in both hardware implementations and algorithm design. In this work, we study the problem of data classification from binary data \edit{obtained from the sign pattern of low-dimensional projections} and propose a framework with low computation and resource costs. We illustrate the utility of the proposed approach through stylized and realistic numerical experiments, and provide a theoretical analysis for a simple case. We hope that our framework and analysis will serve as a foundation for studying similar types of approaches. 
\end{abstract}

%\thanks{DN acknowledges support from the Alfred P. Sloan Foundation, NSF CAREER DMS $\#1348721$ and NSF BIGDATA DMS $\#1740325$.  RS acknowledges support from the NSF under DMS-1517204.}


\begin{keywords}
binary measurements, one-bit representations, classification
\end{keywords}

\section{Introduction}
Our focus is on data classification problems in which only a \textit{binary} representation of the data is available. Such binary representations may arise under a variety of circumstances. In some cases, they may arise naturally due to compressive acquisition. For example, distributed systems may have bandwidth and energy constraints that necessitate extremely coarse quantization of the measurements \citep{fang2014sparse}. 
A binary data representation can also be particularly appealing in hardware implementations because it is inexpensive to compute and promotes a fast hardware device \citep{JacquLBB_Robust,LaskaWYB_Trust}; such benefits have contributed to the success, for example, of 1-bit Sigma-Delta converters \citep{aziz1996overview,candy1962oversampling}. Alternatively, binary, heavily quantized, or compressed representations may be part of the classification algorithm design in the interest of data compression and speed  \citep{BoufoB_1Bit,hunter2010compressive,calderbank2009compressed,davenport2010signal,gupta2010sample,hahn2014adaptive}. The goal of this paper is to present a framework for performing learning inferences, such as classification, from highly quantized data representations---we focus on the extreme case of 1-bit (binary) representations. Let us begin with the mathematical formulation of this problem. 

{\bfseries Problem Formulation.} Let $\{x_i\}_{i=1}^{p}\subset \R^n$ be a point cloud represented via a matrix $$X = [x_1\,\, x_2\,\, \cdots \,\, x_p] \in \R^{n\times p}.$$ Moreover,   let $A: \R^n \to \R^m$ be a linear map, and  denote by $\sign: \R \to \R$ the sign operator given by 
\begin{align*}
\sign(a) = 
\begin{cases}
1 & a\geq 0 \\
-1 & a<0.
\end{cases}
\end{align*}
Without risk of confusion, we overload the above notation so the sign operator can apply to matrices (entrywise). In particular,  for an
$m$ by $p$ matrix $M$, and $(i,j) \in [m]\times [p]$, we define $\sign(M)$ as the $m\times p$ matrix with entries 
\begin{equation*}
(\sign(M))_{i,j} := \sign(M_{i,j}).
\end{equation*}




We consider the setting where a classification algorithm has access to training data of the form $Q=\sign(AX)$,  along with a vector of associated labels 
  $b = (b_1, \,\, \cdots \,\, , b_p )\in\{1,\dots,G\}^p$, indicating the membership of each $x_i$ to exactly one of $G$ classes. Here, $A$ is an $m$ by $n$ matrix. The rows of $A$ define \textit{hyperplanes} in $\R^n$ and the binary sign information tells us which side of the hyperplane each data point lies on. Throughout, we will \edit{primarily} take $A$ to have independent identically distributed standard Gaussian entries \edit{(though experimental results are also included for structured matrices)}. 
Given $Q$ and $b$,
 we wish to train an algorithm that can be used to classify new signals, available only in a similar binary form via the matrix $A$, for which the label is unknown. 
 
 \subsection{Contribution} 
Our contribution is a \textit{framework} for classifying data into a given number of classes using only a binary representation \edit{(obtained as the sign pattern from low-dimensional projections, as described above)} of the data. This framework serves several purposes: (i) it provides mathematical tools that can be used for classification in applications where data is already captured in a simple binary representation, (ii) demonstrates that for general problems, classification can be done effectively using 
low-dimensional measurements, (iii) suggests an approach to use these measurements for classification using 
low computation, (iv) provides a simple technique for classification that can be mathematically analyzed.  We believe this framework can be extended and utilized to build novel algorithmic approaches for many types of learning problems. In this work, we present one method for classification using training data, illustrate its promise on synthetic and real data, and provide a theoretical analysis of the proposed approach in the simple setting of two-dimensional signals and two possible classes. Under mild assumptions, we derive an explicit lower bound on the probability that a new data point gets classified correctly. This analysis serves as a foundation for analyzing the method in more complicated settings, and a framework for studying similar types of approaches.



\subsection{Organization} We proceed next in Section \ref{sec:prior} with a brief overview of related work. Then, in Section \ref{section::algorithm} we propose a two-stage method for classifying data into a given number of classes using only a binary representation of the data. The first stage of the method performs training on data with known class membership, and the second stage is used for classifying new data points with a priori unknown class membership. Next, in Section \ref{section::experiments} we demonstrate the potential of the proposed approach on both synthetically generated data as well as real datasets with application to handwritten digit recognition and facial recognition. Finally, in Section \ref{section::theory} we provide a theoretical analysis of the proposed approach in the simple setting of two-dimensional signals and two classes. We conclude in Section \ref{sec::conclude} with some discussion and future directions.

\subsection{Prior Work}\label{sec:prior}

There is a large body of work on several areas related to the subject of this paper, ranging from classification to compressed sensing, hashing, quantization, and deep learning. Due to the popularity and impact of each of these research areas, any review of prior work that we provide here must necessarily be non-exhaustive. Thus, in what follows, we briefly discuss related prior work, highlighting connections to our work but also stressing the distinctions. 



Support vector machines (SVM) \citep{CristS_Introduction, hearst1998support, joachims1998text, steinwart2008support} have become popular in machine learning, and are often used for classification. Provided a training set of data points and known labels, the SVM problem is to construct the optimal hyperplane (or hyperplanes) separating the data (if the data is linearly separable) or maximizing the geometric margin between the classes (if the data is not linearly separable). Although \edit{loosely} related \edit{(in the sense that at a high level we utilize hyperplanes to separate the data)}, the approach taken in this paper is fundamentally different than in SVM. Instead of searching for the \textit{optimal} separating hyperplane, our proposed algorithm uses many, randomly selected hyperplanes (via the rows of the matrix $A$), and uses the relationship between these hyperplanes and the training data to construct a classification procedure that operates on information between the same hyperplanes and the data to be classified. 

The process of transforming high-dimensional data points into low-dimensional spaces has been studied extensively in related contexts. For example, the pioneering Johnson-Lindenstrauss Lemma states that any set of $p$ points in high dimensional Euclidean space can be (linearly) embedded into $O(\epsilon^{-2} \log(p))$ dimensions, without distorting the distance between any two points by more than a small factor, namely $\epsilon$  \citep{JohnsL_Extensions}. Since the original work of Johnson and Lindenstrauss, much work on Johnson-Lindenstrauss embeddings (often motivated by signal processing and data analysis applications) has focused on randomized embeddings where the matrix associated with the linear embedding is drawn from an appropriate random distribution. Such random embeddings include those based on Gaussian and other subgaussian random variables as well as those that admit fast implementations, usually based on the fast Fourier transform \citep{ailon2006approximate, achlioptas2003database, dasgupta2003elementary}. 

Another important line of related work is \textit{compressed sensing}, in which it has been demonstrated that far fewer linear measurements than dictated by traditional Nyquist sampling can be used to represent high-dimensional data  \citep{CandeRT_Stable,CandeRT_Robust,Donoh_Compressed}. For a signal $x\in\R^n$, one obtains $m<n$ measurements of the form $y = Ax$ (or noisy measurements $y=Ax+z$ for $z\in\R^m$), where $A\in\R^{m\times n}$, and the goal is to recover the signal $x$. By assuming the signal $x$ is $s$-sparse, meaning that $\| x\|_0 = |\supp(x)| = s \ll n$, the recovery problem becomes well-posed under certain conditions on $A$. Indeed, there is now a vast literature describing recovery results and algorithms when $A$, say, is a random matrix drawn from appropriate distributions (including those where the entries of $A$ are independent Gaussian random variables). The relationship between Johnson-Lindenstrauss embeddings and compressed sensing is deep and bi-directional; matrices that yield Johnson-Lindenstrauss embeddings make excellent compressed sensing matrices \citep{baraniuk2006johnson} and conversely, compressed sensing matrices (with minor modifications) yield Johnson-Lindenstrauss embeddings \citep{krahmer2011new}. \edit{Some initial work on performing inference tasks like classification from compressed sensing data shows promising results \citep{BoufoB_1Bit,hunter2010compressive,calderbank2009compressed,davenport2010signal,gupta2010sample,hahn2014adaptive}.}

To allow processing on digital computers, compressive measurements must often be \textit{quantized}, or mapped to discrete values from some finite set. The extreme quantization setting where only the sign bit is acquired is known as \textit{one-bit compressed sensing} and was introduced recently \citep{BoufoB_1Bit}. In this framework, the measurements now take the form $y = \sign(Ax)$, and the objective is still to recover the signal $x$. Several methods have since been developed to recover the signal $x$ (up to normalization) from such simple one-bit measurements  \citep{PlanV_One,PlanV_Robust,gopi2013one,JacquLBB_Robust,yan2012robust,jacques2013quantized}. Although the data we consider in this paper takes a similar form, the overall goal is different; rather than signal \textit{reconstruction}, our interest is data \textit{classification}. 


More recently, there has been growing interest in binary embeddings (embeddings into the binary cube \citep{PlanV_Dimension,yu2014circulant,gong2013iterative,price2015binary,choromanska2016binary, dirksen2016fast}, where it has been observed that using certain linear projections and then applying the sign operator as a nonlinear map largely preserves information about the angular distance between vectors provided one takes sufficiently many measurements. Indeed, the measurement operators used for binary embeddings are Johnson-Lindenstrauss embeddings and thus also similar to those used in compressed sensing, so they again range from random Gaussian and subgaussian matrices to those admitting fast linear transformations, such as random circulant matrices  \citep{dirksen2016fast}, \edit{although there are limitations to such embeddings for subgaussian but non-Gaussian matrices \citep{PlanV_Dimension,PlanV_One}}.
Although we consider a similar binary measurement process, we are not necessarily concerned with geometry preservation in the low-dimensional space, but rather the ability to still perform data classification. 


Deep Learning is an area of machine learning based on learning data representations using multiple \edit{level}s of abstraction, or layers. Each of these layers is essentially a function whose parameters are learned, and the full network is thus a composition of such functions.  Algorithms for such deep neural networks  have recently obtained state of the art results for classification. Their success has been due to the availability of large training data sets coupled with advancements in computing power and the development of new techniques \citep{krizhevsky2012imagenet,simonyan2014very,szegedy2015going,russakovsky2015imagenet}. 
\edit{Randomization in neural networks has again been shown to give computational advantages and even so-called ``shallow" networks with randomization and random initializations of deep neural networks have been shown to obtain results close to deep networks requiring heavy optimization \citep{rahimi2009weighted,giryes2016deep}.  Deep neural networks have also been extended to binary data, where the net represents a set of
Boolean functions that maps all binary inputs to the outputs \citep{kim2016bitwise,courbariaux2015binaryconnect,courbariaux2016binarized}.  Other types of quantizations have been proposed to reduce multiplications in both the input and hidden layers \citep{lin2015neural,marchesi1993fast,simard1994backpropagation,burge1999rising,rastegari2016xnor,hubara2016quantized}. 
We will use randomized non-linear measurements but} consider deep learning and neural networks as motivational to our multi-\edit{level} algorithm design.  \edit{Indeed,} we are not tuning parameters nor \edit{doing any optimization} as is typically done in deep learning, nor do our \edit{level}s necessarily possess the structure typical in deep learning ``architectures"; this makes our approach potentially simpler and easier to work with. 

\edit{Using randomized non-linearities and simpler optimizations appears in several other works \citep{rahimi2009weighted,ozuysal2010fast}. The latter work most closely resembles our approach in that the authors propose a ``score function" using binary tests in the training phase, and then classifies new data based on the maximization of a class probability function. The perspective of this prior approach however is Bayesian rather than geometric, the score functions do not include any balancing terms as ours will below, the measurements are taken as ``binary tests" using components of the data vectors (rather than our compressed sensing style projections), and the approach does not utilize a multi-level approach as ours does. We believe our geometric framework not only lends itself to easily obtained binary data but also a simpler method and analysis. }

\section{The Proposed Classification Algorithm} \label{section::algorithm}
The training phase of our algorithm is detailed in Algorithm \ref{proposed algorithm1}. \edit{Here, the method may take the binary data $Q$ as input directly, or the training data $Q = \sign(AX)$ may be computed as a one-time pre-processing step.  For arbitrary matrices $A$, this step of course may incur a computational cost on the order of $mnp$.  In Section \ref{section::experiments}, we also include experiments using structured matrices that have a fast multiply, reducing this cost to a logarithmic dependence on the dimension $n$.}  Then, the training algorithm proceeds in $L$ ``\edit{level}s". 
In the $\ell$-th \edit{level}, $m$ index sets $\GamLi{\ell}{i} \subset [m]$,  $|\GamLi{\ell}{i}| = \ell$, $i=1,...,m$, are randomly selected,  
so that all elements of $\GamLi{\ell}{i}$ are unique, and $\GamLi{\ell}{i} \neq \GamLi{\ell}{j}$ for $i\neq j$.
This is achieved by selecting the multi-set of $\Lambda_{\ell,i}$'s uniformly at random from a set of cardinality ${{m}\choose{\ell}}\choose m$.
During the $i$-th ``iteration" of the $\ell$-th \edit{level}, the rows of $Q$ indexed by $\GamLi{\ell}{i}$ are used to form the $\ell \times p$ \edit{submatrix of $Q$, the columns of which define the sign patterns $\{\pm 1\}^\ell$ observed by the training data.}
%During the $i$-th ``iteration" of the $\ell$-th \edit{level}, the rows of $Q$ indexed by $\GamLi{\ell}{i}$ are used to form the $\ell \times p$ matrix $Q^{\GamLi{\ell}{i}} \in \{\pm 1\}^{\ell \times p}$, and the unique sign patterns $q \in \{\pm 1\}^\ell$ are extracted from the columns of $Q^{\GamLi{\ell}{i}}$. The number of unique sign patterns (i.e., distinct columns) in $Q^{\GamLi{\ell}{i}}$ is given by $T_{\ell,i}\in \mathbb{N}$.
For example, 
at the first \edit{level} the possible sign patterns are 1 and -1, describing which side of the selected hyperplane the training data points lie on; at the second \edit{level} the possible sign patters are $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$, $\begin{bmatrix} 1 \\ -1 \end{bmatrix}$, $\begin{bmatrix} -1 \\ 1 \end{bmatrix}$, $\begin{bmatrix} -1 \\ -1 \end{bmatrix}$, describing which side of the two selected hyperplanes the training data points lie on, and so on for the subsequent \edit{level}s. \edit{At each level, there are at most $2^\ell$ possible sign patterns. Let $t = t(\ell)\in\{0,1,2,\dots\}$ denote the sign pattern \textit{index} at level $\ell$, where $0\leq t \leq 2^\ell-1$. Then, the binary (i.e., base 2) representation of each $t = (t_\ell \dots t_2 t_1)_{\mbox{bin}} := \sum_{k=1}^\ell t_k 2^{k-1}$ is in one-to-one correspondence with the binary sign pattern it represents, up to the identification of $\{0,1\}$ with the images $\{-1,1\}$ of the sign operator. For example, at level $\ell=2$ the sign pattern index $t=2 = (10)_{\mbox{bin}}$ corresponds to the sign pattern $\begin{bmatrix} 1 \\ -1 \end{bmatrix}$.}

For the $t$-th sign pattern and $g$-th class, a \textit{\RF} parameter $r(\ell,i,t,g)$ that uses knowledge of the number of training points in class $g$ having the $t$-th sign pattern, is calculated for every $\GamLi{\ell}{i}$. Larger values of  $r(\ell,i,t,g)$ suggest that the $t$-th sign pattern is more heavily dominated by class $g$; thus, if a signal with unknown label corresponds to the $t$-th sign pattern, we will be more likely to classify it into the $g$-th class. In this paper, we use the following choice for the \RF parameter $r(\ell,i,t,g)$, which we found to work well experimentally. Below, $P_{g|t}\edit{ = P_{g|t}(\GamLi{\ell}{i})}$ denotes the number of training points from the $g$-th class with the $t$-th sign pattern at the $i$-th set selection in the $\ell$-th \edit{level}: %(i.e., the $t$-th sign pattern determined from the set selection $\GamLi{\ell}{i}$): 
\begin{align} \label{RF3}
r(\ell,i,t,g) &= \frac{P_{g|t}}{\sum_{j=1}^G P_{j|t}} \frac{\sum_{j=1}^G |P_{g|t} - P_{j|t}|}{\sum_{j=1}^G P_{j|t}}.
\end{align}
Let us briefly explain the intuition for this formula. The first fraction in \eqref{RF3} indicates the proportion of training points in class $g$ out of all points with sign pattern $t$ \edit{(at the $\ell$-th level and $i$-th iteration)}. The second fraction in \eqref{RF3} is a balancing term that gives more weight to group $g$ when that group is much different in size than the others with the same sign pattern. 
If $P_{j|t}$ is the same for all classes $j = 1,\dots,G$, then $r(\ell,i,t,g)=0$ for all $g$, and thus no class is given extra weight for the given sign pattern, set selection, and \edit{level}. 
If $P_{g|t}$ is nonzero and $P_{j|t} = 0$ for all other classes, then $r(\ell,i,t,g) = G-1$ 
  and $r(\ell,i,t,j) = 0$ for all $j\neq g$, so that class $g$ receives the largest weight. 
\edit{It is certainly possible that a large number of the sign pattern indices $t$ will have $P_{g|t}=0$ for all groups (i.e., not all binary sign patterns are observed from the training data), in which case $r(\ell,i,t,g) = 0$.}

\edit{
\begin{remark}
Note that in practice the membership index value need not be stored for all $2^\ell$ possible sign pattern indices, but rather only for the unique sign patterns that are actually observed by the training data. In this case, the unique sign patterns at each level $\ell$ and iteration $i$ must be input to the classification phase of the algorithm (Algorithm \ref{proposed algorithm2}). 
\end{remark}}

\begin{algorithm}[ht]
\caption{Training 
} 
\label{proposed algorithm1}
\begin{algorithmic}
%
\STATE \textbf{input:}  training labels $b$, number of classes $G$, number of \edit{level}s $L$, binary training data $Q$ \edit{(or raw training data $X$ and fixed matrix $A$)}
\STATE 
\edit{\textbf{if raw data: } Compute $Q=\sign(AX)$}
%
\FOR{$\ell$ from 1 to $L$, $i$ from 1 to $m$}
%
\STATE
\begin{tabular}{ll}
\textbf{select:} & Randomly select $\GamLi{\ell}{i} \subset [m]$, $|\GamLi{\ell}{i}| = \ell$ \\
%\textbf{determine:} & Determine the $T_{\ell,i}\in\mathbb{N}$ unique column patterns in $Q^{\GamLi{\ell}{i}}$

\end{tabular}
\FOR{$t$ from \edit{0} to \edit{$2^\ell-1$}, $g$ from 1 to $G$}
\STATE
\begin{tabular}{ll}
\textbf{compute:} & Compute $r(\ell,i,t,g)$ by (\ref{RF3})\\
\end{tabular}
\ENDFOR
%
\ENDFOR
\end{algorithmic}
\end{algorithm}

Once the algorithm has been trained, we can use it to classify new signals. Suppose $x\in\R^n$ is a new signal for which the class is unknown, and we have available the quantized measurements $q = \sign(Ax)$. Then Algorithm \ref{proposed algorithm2} is used for the classification of $x$ into one of the $G$ classes. Notice that the number of \edit{level}s $L$, the learned \RF values $r(\ell,i,t,g)$, and the set selections $\GamLi{\ell}{i}$ at each iteration of each \edit{level} are all available from Algorithm \ref{proposed algorithm1}. First, the decision vector $\tilde{r}$ is initialized to the zero vector in $\R^G$.  Then for each \edit{level} $\ell$ and set selection $i$\edit{, the sign pattern, and hence the binary base 2 representation, can be determined using $q$ and $\Lambda_{\ell,i}$. Thus, the corresponding sign pattern index $t^\star = t^\star(\ell,i)\in\{0,1,2,\dots\}$ such that $0\leq t^\star\leq 2^\ell-1$ is identified}.  For each class $g$,  $\tilde{r}(g)$ is updated via $\tilde{r}(g) \leftarrow \tilde{r}(g) + r(\ell,i,t^\star,g)$. %If it happens that the sign pattern for $x$ does not match any sign pattern determined during training, no update to $\tilde{r}$ is performed. 
Finally, after scaling $\tilde{r}$ with respect to the number of \edit{level}s and measurements, the largest entry of $\tilde{r}$ identifies how the estimated label $\widehat{b}_x$ of $x$ is set. This scaling of course does not actually affect the outcome of classification, we use it simply to ensure the quantity does not become unbounded for large problem sizes.  \edit{We note here that especially for large $m$, the bulk of the classification will come from the higher levels (in fact the last level) due to the geometry of the algorithm.  However, we choose to write the testing phase using all levels since the lower levels are cheap to compute with, may still contribute to classification accuracy especially for small $m$, and can be used naturally in other settings such as hierarchical classification and detection (see remarks in Section \ref{sec::conclude}).}

\begin{algorithm}[ht]
\caption{Classification} 
\label{proposed algorithm2}
\begin{algorithmic}
%
\STATE \textbf{input:} binary data $q$, number of classes $G$, number of \edit{level}s $L$, learned parameters $r(\ell,i,t,g)$ and $\GamLi{\ell}{i}$ from Algorithm \ref{proposed algorithm1}
\STATE 
%
\STATE \textbf{initialize:} $\tilde{r}(g) = 0$ for $g = 1,\dots,G$.
%
\FOR{$\ell$ from 1 to $L$, $i$ from 1 to $m$}
%
\STATE
\begin{tabular}{ll}
\textbf{identify:} & Identify the \edit{sign pattern index $t^\star$ using $q$ and $\Lambda_{\ell,i}$} \\%pattern $t^\star\in[T_{\ell,i}]$ to which $q^{\GamLi{\ell}{i}}$ corresponds \\
\end{tabular}
\FOR{$g$ from 1 to $G$} 
\STATE
\begin{tabular}{ll}
\textbf{update:} & $\tilde{r}(g) = \tilde{r}(g) + r(\ell,i,t^\star,g)$ \\
\end{tabular}
\ENDFOR
%
\ENDFOR
\STATE \textbf{scale:} Set $\tilde{r}(g) = \frac{\tilde{r}(g)}{Lm}$ for $g=1,\dots,G$ 
\STATE \textbf{classify:} $\widehat{b}_x = \argmax_{g\in\{1,\dots,G\}}\tilde{r}(g)$
\end{algorithmic}
\end{algorithm}


\section{Experimental Results}\label{section::experiments}
In this section, we provide experimental results of Algorithms \ref{proposed algorithm1} and \ref{proposed algorithm2} for synthetically generated datasets, handwritten digit recognition using the MNIST dataset, and facial recognition using the extended YaleB database. \edit{We note that for the synthetic data, we typically use Gaussian clouds, but note that since our algorthms use hyperplanes to classify data, the results on these type of datasets would be identical to any with the same radial distribution around the origin. We use Gaussian clouds simply because they are easy to visualize and allow for various geometries. Of course, our methods require no particular structure other than being centered around the origin, which can be done as a pre-processing step (and the framework could clearly be extended to remove this property in future work).  The real data like the hand-written digits and faces clearly have more complicated geometries and are harder to visualize. We include both types of data to fully characterize our method's performance.}

\edit{We also remark here that we purposefully choose not to compare to other related methods like SVM for several reasons. First, if the data happens to be linearly separable it is clear that SVM will outperform or match our approach since it is designed precisely for such data.  In the interesting case when the data is not linearly separable, our method will clearly outperform SVM since SVM will fail. To use SVM in this case, one needs an appropriate kernel, and identifying such a kernel is highly non-trivial without understanding the data's geometry, and precisely what our method avoids having to do.}

\edit{Unless otherwise specified}, the matrix $A$ is taken to have i.i.d. standard Gaussian entries. Also, we assume the data is centered. To ensure this, a pre-processing step on the raw data is performed to account for the fact that the data may not be centered around the origin. That is, given the original training data matrix $X$, we calculate $\mu = \frac{1}{p} \sum_{i=1}^p x_i$. Then for each column $x_i$ of $X$, we set $x_i \leftarrow x_i - \mu$. The testing data is adjusted similarly by $\mu$. Note that this assumption can be overcome in future work by using \textit{dithers}---that is, hyperplane dither values may be learned so that $Q = \sign(AX + \tau)$, where $\tau\in\R^m$---or \edit{even with random dithers, as motivated by quantizer results  \citep{exponentialBFNPW14,cambareri2017rare}}.

%\end{document}
%\clearpage
\subsection{Classification of Synthetic Datasets}
In our first stylized experiment, we consider three classes of Gaussian clouds in $\R^2$ (i.e., $n=2$); see Figure \ref{syn:gaussian clouds} for an example training and testing data setup. For each choice of $m\in\{5,7,9,11,13,15,17,19\}$ and $p\in\{75,150,225\}$ with equally sized training data sets for each class (that is, each class is tested with either 25, 50, or 75 training points), 
 we execute Algorithms \ref{proposed algorithm1} and \ref{proposed algorithm2} with a single \edit{level} and 30 trials of generating $A$. We perform classification of 50 test points per group, and report the average correct classification rate \edit{(ACCR)} over all trials. \edit{Note that the ACCR is simply defined as the number of correctly classified testing points divided by the total number of testing points (where the correct class is known either from the generated distribution or the real label for real world data), and then averaged over the trials of generating $A$.  We choose this metric since it captures both false negatives and positives, and since in all experiments we have access to the correct labels.} The right plot of Figure \ref{syn:gaussian clouds} shows that $m\geq 15$ results in nearly perfect classification. 

\begin{figure}[!htbp]
\centering
\begin{tabular}{cc}
%\includegraphics[height=2in]{images/Synthetic/gaussian_g3_d2/ClassifySimRF_PreprocessedData_1layers_3groups_30trials_50n_2d_25p_1preprocess_gaussian_0mutesttrain.eps} &
\includegraphics[height=2in]{images/Synthetic/gaussian_g3_d2/synthetic_1a.eps} &
%\includegraphics[height=2in]{images/Synthetic/gaussian_g3_d2/ClassifySimRF_RateVersusM_1layers_3groups_30trials_50n_2d_1preprocess_gaussian_0mutesttrain_3RFtype.eps} \\
\includegraphics[height=2in]{images/Synthetic/gaussian_g3_d2/synthetic_1b.eps} \\
\end{tabular}
\caption{Synthetic classification experiment with three Gaussian clouds ($G=3$), $L=1$, $n=2$, 50 test points per group, and 30 trials of randomly generating $A$. (Left) Example training and testing data setup. (Right) Average correct classification rate versus $m$ and for the indicated number of training points per class.}
\label{syn:gaussian clouds}
\end{figure}

\begin{figure}[!htbp]
\centering
\begin{tabular}{cc}
%\includegraphics[height=2in]{images/Synthetic/6ball_g2_d2/ClassifySimRF_PreprocessedData_4layers_30trials_50n_2d_25p_1preprocess_manyball_0mutesttrain.eps} &
\includegraphics[height=2in]{images/Synthetic/6ball_g2_d2/synthetic_2a.eps} &
%\includegraphics[height=2in]{images/Synthetic/6ball_g2_d2/ClassifySimRF_RateVersusM_4layers_30trials_50n_2d_1preprocess_manyball_0mutesttrain_3RFtype.eps} \\
\includegraphics[height=2in]{images/Synthetic/6ball_g2_d2/synthetic_2b.eps} \\
\end{tabular}
\caption{Synthetic classification experiment with six Gaussian clouds and two classes ($G=2$), $L=4$, $n=2$, 50 test points per group, and 30 trials of randomly generating $A$. (Left) Example training and testing data setup. (Right) Average correct classification rate versus $m$ and for the indicated number of training points per class.}
\label{syn:gaussian2 alternating6}
\end{figure}

Next, we present a suite of experiments where we again construct the classes as Gaussian clouds in $\R^2$, but utilize \edit{various types of data geometries.} In each case,  we set the number of training data points for each class to be 25, 50, and 75. In Figure \ref{syn:gaussian2 alternating6}, we have two classes forming a total of six Gaussian clouds, and execute Algorithms \ref{proposed algorithm1} and \ref{proposed algorithm2} using four \edit{level}s and $m\in\{10,30,50,70,90,110,130\}$. The classification accuracy increases for larger $m$, with nearly perfect classification for the largest values of $m$ selected. 
 A similar experiment is shown in Figure \ref{syn:gaussian2 alternating8}, where we have two classes forming a total of eight Gaussian clouds, and execute the proposed algorithm using five \edit{level}s. 



In the next two experiments, we display the classification results of Algorithms \ref{proposed algorithm1} and \ref{proposed algorithm2} when using $m\in\{10,30,50,70,90\}$ and one through four \edit{level}s, and see that adding \edit{level}s can be beneficial for more complicated data geometries.
In Figure \ref{syn:gaussian3 alternating8}, we have three classes forming a total of eight Gaussian clouds. We see that from both $L=1$ to $L=2$ and $L=2$ to $L=3$, there are huge gains in classification accuracy. In Figure \ref{syn:gaussian4 alternating8}, we have four classes forming a total of eight Gaussian clouds. Again, from both $L=1$ to $L=2$ and $L=2$ to $L=3$ we see large improvements in classification accuracy, yet still better classification with $L=4$. We note here that in this case it also appears that more training data does not improve the performance (and perhaps even slightly decreases accuracy); this is of course unexpected in practice, but we believe this happens here only because of the construction of the Gaussian clouds---more training data leads to more outliers in each cloud, making the sets harder to separate. 



\begin{figure}[!htbp]
\centering
\begin{tabular}{cc}
%\includegraphics[height=2in]{images/Synthetic/8ball_g2_d2/ClassifySimRF_PreprocessedData_5layers_30trials_50n_2d_25p_1preprocess_manyball_0mutesttrain.eps} &
\includegraphics[height=2in]{images/Synthetic/8ball_g2_d2/synthetic_3a.eps} &
%\includegraphics[height=2in]{images/Synthetic/8ball_g2_d2/ClassifySimRF_RateVersusM_5layers_30trials_50n_2d_1preprocess_manyball_0mutesttrain_3RFtype.eps} \\
\includegraphics[height=2in]{images/Synthetic/8ball_g2_d2/synthetic_3b.eps} \\
\end{tabular}
\caption{Synthetic classification experiment with eight Gaussian clouds and two classes ($G=2$), $L=5$, $n=2$, 50 test points per group, and 30 trials of randomly generating $A$. (Left) Example training and testing data setup. (Right) Average correct classification rate versus $m$ and for the indicated number of training points per class.}
\label{syn:gaussian2 alternating8}
\end{figure}

\begin{figure}[!htbp]
\centering
%\includegraphics[height=2in]{images/Synthetic/8ball_g3_d2/ClassifySimRF_PreprocessedData_1layers_3groups_30trials_50n_2d_25p_1preprocess_manyball_0mutesttrain.eps} 
\includegraphics[height=2in]{images/Synthetic/8ball_g3_d2/synthetic_4a.eps} 
\begin{tabular}{cc}
%\includegraphics[height=2in]{images/Synthetic/8ball_g3_d2/ClassifySimRF_RateVersusM_1layers_3groups_30trials_50n_2d_1preprocess_manyball_0mutesttrain_3RFtype.eps} &
\includegraphics[height=2in]{images/Synthetic/8ball_g3_d2/synthetic_4b.eps} &
%\includegraphics[height=2in]{images/Synthetic/8ball_g3_d2/ClassifySimRF_RateVersusM_2layers_3groups_30trials_50n_2d_1preprocess_manyball_0mutesttrain_3RFtype.eps} \\
\includegraphics[height=2in]{images/Synthetic/8ball_g3_d2/synthetic_4c.eps} \\
(a) $L=1$ & (b) $L=2$ \\
%\includegraphics[height=2in]{images/Synthetic/8ball_g3_d2/ClassifySimRF_RateVersusM_3layers_3groups_30trials_50n_2d_1preprocess_manyball_0mutesttrain_3RFtype.eps} &
\includegraphics[height=2in]{images/Synthetic/8ball_g3_d2/synthetic_4d.eps} &
%\includegraphics[height=2in]{images/Synthetic/8ball_g3_d2/ClassifySimRF_RateVersusM_4layers_3groups_30trials_50n_2d_1preprocess_manyball_0mutesttrain_3RFtype.eps} \\
\includegraphics[height=2in]{images/Synthetic/8ball_g3_d2/synthetic_4e.eps} \\
(c) $L=3$ & (d) $L=4$ \\
\end{tabular}
\caption{Synthetic classification experiment with eight Gaussian clouds and three classes ($G=3$), $L=1,\dots,4$, $n=2$, 50 test points per group, and 30 trials of randomly generating $A$. (Top) Example training and testing data setup. Average correct classification rate versus $m$ and for the indicated number of training points per class for: (middle left) $L=1$, (middle right) $L=2$, (bottom left) $L=3$, (bottom right) $L=4$.}
\label{syn:gaussian3 alternating8}
\end{figure}

\begin{figure}[!htbp]
\centering
%\includegraphics[height=2in]{images/Synthetic/8ball_g4_d2/ClassifySimRF_PreprocessedData_1layers_4groups_30trials_50n_2d_25p_1preprocess_manyball_0mutesttrain.eps} 
\includegraphics[height=2in]{images/Synthetic/8ball_g4_d2/synthetic_5a.eps} 
\begin{tabular}{cc}
%\includegraphics[height=2in]{images/Synthetic/8ball_g4_d2/ClassifySimRF_RateVersusM_1layers_4groups_30trials_50n_2d_1preprocess_manyball_0mutesttrain_3RFtype.eps} &
\includegraphics[height=2in]{images/Synthetic/8ball_g4_d2/synthetic_5b.eps} &
%\includegraphics[height=2in]{images/Synthetic/8ball_g4_d2/ClassifySimRF_RateVersusM_2layers_4groups_30trials_50n_2d_1preprocess_manyball_0mutesttrain_3RFtype.eps} \\
\includegraphics[height=2in]{images/Synthetic/8ball_g4_d2/synthetic_5c.eps} \\
(a) $L=1$ & (b) $L=2$ \\
%\includegraphics[height=2in]{images/Synthetic/8ball_g4_d2/ClassifySimRF_RateVersusM_3layers_4groups_30trials_50n_2d_1preprocess_manyball_0mutesttrain_3RFtype.eps} &
\includegraphics[height=2in]{images/Synthetic/8ball_g4_d2/synthetic_5d.eps} &
%\includegraphics[height=2in]{images/Synthetic/8ball_g4_d2/ClassifySimRF_RateVersusM_4layers_4groups_30trials_50n_2d_1preprocess_manyball_0mutesttrain_3RFtype.eps} \\
\includegraphics[height=2in]{images/Synthetic/8ball_g4_d2/synthetic_5e.eps} \\
(c) $L=3$ & (d) $L=4$ \\
\end{tabular}
\caption{Synthetic classification experiment with eight Gaussian clouds and four classes ($G=4$), $L=1,\dots,4$, $n=2$, 50 test points per group, and 30 trials of randomly generating $A$. (Top) Example training and testing data setup. Average correct classification rate versus $m$ and for the indicated number of training points per class for: (middle left) $L=1$, (middle right) $L=2$, (bottom left) $L=3$, (bottom right) $L=4$.}
\label{syn:gaussian4 alternating8}
\end{figure}

%\clearpage
\subsection{Handwritten Digit Classification}

In this section, we apply Algorithms \ref{proposed algorithm1} and \ref{proposed algorithm2} to the MNIST \citep{MNIST} dataset, which is a benchmark dataset of images of handwritten digits, each with $28 \times 28$ pixels. In total, the dataset has $60,000$ training examples and $10,000$ testing examples. 

First, we apply Algorithms \ref{proposed algorithm1} and \ref{proposed algorithm2} when considering only two digit classes. Figure \ref{mnist:01} shows the correct classification rate for the digits ``0" versus ``1". We set $m\in\{10,30,50,70,90,110\}$, $p\in\{50,100,150\}$ with equally sized training data sets for each class, and classify 50 images per digit class. Notice that the algorithm is performing very well for small $m$ in comparison to $n=28\times 28 =784$ and only a single \edit{level}. Figure \ref{mnist:05} shows the results of a similar setup for the digits ``0" and ``5". In this experiment, we increased  to four \edit{level}s and achieve classification accuracy around $90\%$ at the high end of $m$ values tested. This indicates that the digits ``0" and ``5" are more likely to be mixed up than ``0" and ``1", which is understandable due to the more similar digit shape between ``0" and ``5". \edit{In Figure \ref{mnist:05}, we include the classification performance when the matrix $A$ is constructed using the two-dimensional Discrete Cosine Transform (DCT) in addition to our typical Gaussian matrix $A$ (note one could similarly use the Discrete Fourier Transform instead of the DCT but that requires re-defining the $\sign$ function on complex values). Specifically, to construct $A$ from the $n\times n$ two-dimensional DCT, we select $m$ rows uniformly at random and then apply a random sign (i.e., multiply by $+1$ or -$1$) to the columns. We include these two results to illustrate that there is not much difference when using the DCT and Gaussian constructions of $A$, though we expect analyzing the DCT case to be more challenging and limit the theoretical analysis in this paper to the Gaussian setting.  The advantage of using a structured matrix like the DCT is of course the reduction in computation cost in acquiring the measurements.}

\begin{figure}[!htbp]
\centering
\begin{tabular}{cc}
%\raisebox{1.1\height}{\includegraphics[height=0.7in]{images/MNIST/groups_0_1/ClassifyMNISTRF_TrainingData_1layers_25p_50n_784d.eps}} &
\raisebox{1.1\height}{\includegraphics[height=0.7in]{images/MNIST/groups_0_1/mnist_1a.eps}} &
%\includegraphics[height=2in]{images/MNIST/groups_0_1/ClassifyMNISTRF_RateVersusM_1layers_30trials_50n_0group1_1group2_1preprocess_0mutesttrain_3RFtype.eps} \\
\includegraphics[height=2in]{images/MNIST/groups_0_1/mnist_1b.eps} \\
\end{tabular}
%\includegraphics[height=0.7in]{images/MNIST/groups_0_1/ClassifyMNISTRF_TestingData_1layers_25p_50n_784d.eps} 
\includegraphics[height=0.7in]{images/MNIST/groups_0_1/mnist_1c.eps} 
\caption{Classification experiment using the handwritten ``0" and ``1" digit images from the MNIST dataset, $L=1$, $n=28\times 28 =784$, 50 test points per group, and 30 trials of randomly generating $A$. (Top left) Training data images when $p = 50$. (Top right) Average correct classification rate versus $m$ and for the indicated number of training points per class. (Bottom) Testing data images.}
\label{mnist:01}
\end{figure}

%\begin{figure}[!htbp]
%\centering
%\begin{tabular}{cc}
%%\raisebox{1.1\height}{\includegraphics[height=0.7in]{images/MNIST/groups_0_5/ClassifyMNISTRF_TrainingData_1layers_25p_50n_784d.eps}} &
%\raisebox{1.1\height}{\includegraphics[height=0.7in]{images/MNIST/groups_0_5/mnist_2a.eps}} &
%%\includegraphics[height=2in]{images/MNIST/groups_0_5/ClassifyMNISTRF_RateVersusM_4layers_30trials_50n_0group1_5group2_1preprocess_0mutesttrain_3RFtype.eps}
%\includegraphics[height=2in]{images/MNIST/groups_0_5/mnist_2b.eps}
%\end{tabular}
%%\includegraphics[height=0.7in]{images/MNIST/groups_0_5/ClassifyMNISTRF_TestingData_1layers_25p_50n_784d.eps} 
%\includegraphics[height=0.7in]{images/MNIST/groups_0_5/mnist_2c.eps} 
%\caption{Classification experiment using the handwritten ``0" and ``5" digit images from the MNIST dataset, $L=4$, $n=28\times 28=784$, 50 test points per group, and 30 trials of randomly generating $A$. (Top left) Training data images when $p = 50$. (Top right) Average correct classification rate versus $m$ and for the indicated number of training points per class. (Bottom) Testing data images.}
%\label{mnist:05}
%\end{figure}

\begin{figure}[!htbp]
\centering
%\raisebox{1.1\height}{\includegraphics[height=0.7in]{images/MNIST/groups_0_5/ClassifyMNISTRF_TrainingData_1layers_25p_50n_784d.eps}} &
\includegraphics[height=0.7in]{images/MNIST/groups_0_5/mnist_2a.eps} 
%\includegraphics[height=0.7in]{images/MNIST/groups_0_5/ClassifyMNISTRF_TestingData_1layers_25p_50n_784d.eps} 
\includegraphics[height=0.7in]{images/MNIST/groups_0_5/mnist_2c.eps} 
\begin{tabular}{cc}
%\includegraphics[height=2in]{images/MNIST/groups_0_5/ClassifyMNISTRF_RateVersusM_4layers_30trials_50n_0group1_5group2_1preprocess_0mutesttrain_3RFtype.eps}
\includegraphics[height=2in]{images/MNIST/groups_0_5/mnist_2b.eps} &
\includegraphics[height=2in]{images/MNIST/groups_0_5/mnist_2d.eps} % temp: PUT DCT HERE!
\end{tabular}
\caption{Classification experiment using the handwritten ``0" and ``5" digit images from the MNIST dataset, $L=4$, $n=28\times 28=784$, 50 test points per group, and 30 trials of randomly generating $A$. (Top) Training data images when $p = 50$. (Middle) Testing data images. Average correct classification rate versus $m$ and for the indicated number of training points per class \edit{(bottom left) when using a Gaussian matrix $A$ and (bottom right) when using a DCT matrix $A$.}}
\label{mnist:05}
\end{figure}

\begin{figure}[!htbp]
\centering
%\includegraphics[height=2in]{images/MNIST/10Digits/ClassifyMNISTRF_RateVersusM_5layers_10trials_800n_0group1_1group2_1preprocess_0mutesttrain_3RFtype.eps}
\includegraphics[height=2in]{images/MNIST/10Digits/18layers_v2/mnist_3.eps}
\caption{Correct classification rate versus $m$ when using all ten (0-9) handwritten digits from the MNIST dataset, $L=18$, $n=28\times 28=784$, 1,000, 3,000, and 5,000 training points per group, 800 test points per group (8,000 total), and a single instance of randomly generating $A$.}
\label{mnist:all}
\end{figure}

Next, we apply Algorithms \ref{proposed algorithm1} and \ref{proposed algorithm2} to the MNIST dataset with all ten digits. We utilize $1,000$, $3,000$, and $5,000$ training points per digit class, and perform classification with $800$ test images per class. The classification results using 18 \edit{level}s and $m\in\{100, 200, 400, 600, 800\}$ are shown in Figure \ref{mnist:all}, where it can be seen that with $5,000$ training points per class, above 90\% classification accuracy is achieved for $m\geq 200$. We also see that larger training sets result in slightly improved classification. 

%\clearpage
\subsection{Facial Recognition}

Our last experiment considers facial recognition using the extended YaleB dataset \citep{CHHH07,CHH07b,CHHZ06,HYHNZ05}. This dataset  includes $32 \times 32$ images of 38 individuals with roughly 64 near-frontal images under different illuminations per individual. We select \edit{four} individuals from the dataset, and randomly select images with different illuminations to be included in the training and testing sets (note that the same illumination was included for \textit{each} individual in the training and testing data). We execute Algorithms \ref{proposed algorithm1} and \ref{proposed algorithm2} using four \edit{level}s with $m\in\{10,50,100,150,200,250,300 \}$, $p\in\{20,40,60\}$ with equally sized training data sets for each class, and classify 30 images per class. The results are displayed in Figure \ref{yaleB}. \edit{Above $90\%$ correct classification is achieved for $m\geq 150$ when using the largest training set.}

%%% RESULTS USING TWO FACES %%%
%\begin{figure}[!htbp]
%\centering
%\begin{tabular}{cc}
%%\raisebox{1.1\height}{\includegraphics[height=0.7in,width=2.5in]{images/YaleB/groups_5_6/ClassifyYaleBRF_TrainingData_1layers_10p_30n_1024d.eps}} &
%\raisebox{1.1\height}{\includegraphics[height=0.7in,width=2.5in]{images/YaleB/groups_5_6/yaleb_1a.eps}} &
%%\includegraphics[height=2in]{images/YaleB/groups_5_6/ClassifyYaleBRF_RateVersusM_4layers_30trials_30n_5group1_6group2_1preprocess_0mutesttrain_3RFtype.eps} 
%\includegraphics[height=2in]{images/YaleB/groups_5_6/yaleb_1b.eps} 
%\end{tabular}
%%\includegraphics[height=0.7in]{images/YaleB/groups_5_6/ClassifyYaleBRF_TestingData_1layers_10p_30n_1024d.eps} 
%\includegraphics[height=0.7in]{images/YaleB/groups_5_6/yaleb_1c.eps} 
%\caption{Classification experiment using two individuals from the extended YaleB dataset, $L=4$, $n=32\times 32 = 1024$, 30 test points per group, and 30 trials of randomly generating $A$. (Top left) Training data images when $p = 20$. (Top right) Average correct classification rate versus $m$ and for the indicated number of training points per class. (Bottom) Testing data images.}
%\label{yaleB}
%\end{figure}

%%% RESULTS USING FOUR FACES (FOR REVISION) %%%
\begin{figure}[!htbp]
\centering
\begin{tabular}{cc}
%\raisebox{1.1\height}{\includegraphics[height=0.7in,width=2.5in]{images/YaleB/groups_4_5_6_9/ClassifyYaleBRF_TrainingData_1layers_10p_30n_1024d.eps}} &
\raisebox{0.8\height}{\includegraphics[height=0.9in,width=2.3in]{images/YaleB/groups_4_5_6_9/yaleb_1a.eps}} &
%\includegraphics[height=2in]{images/YaleB/groups_4_5_6_9/ClassifyYaleBRF_RateVersusM_4layers_30trials_30n_5group1_6group2_1preprocess_0mutesttrain_3RFtype.eps} 
\includegraphics[height=2in]{images/YaleB/groups_4_5_6_9/yaleb_1b.eps} 
\end{tabular}
%\includegraphics[height=0.7in]{images/YaleB/groups_4_5_6_9/ClassifyYaleBRF_TestingData_1layers_10p_30n_1024d.eps} 
\includegraphics[height=0.9in]{images/YaleB/groups_4_5_6_9/yaleb_1c.eps} 
\caption{Classification experiment using \edit{four} individuals from the extended YaleB dataset, $L=4$, $n=32\times 32 = 1024$, 30 test points per group, and 30 trials of randomly generating $A$. (Top left) Training data images when $p = 20$. (Top right) Average correct classification rate versus $m$ and for the indicated number of training points per class. (Bottom) Testing data images.}
\label{yaleB}
\end{figure}

\section{Theoretical Analysis for a Simple Case}\label{section::theory}

\subsection{Main Results}
We now provide a theoretical analysis of Algorithms \ref{proposed algorithm1} and \ref{proposed algorithm2} in which we make a series of simplifying assumptions to make the development more tractable.
We focus on the setting where the signals are two-dimensional, belonging to one of two classes, and consider a single \edit{level} (i.e., $\ell=1$, $n=2$, and $G=2$). 
Moreover, we assume the true classes $G_1$ and $G_2$ to be two disjoint \textit{cones} in $\R^2$ and assume that regions of the same angular measure have the same number (or density) of training points. \edit{Of course, the problem of non-uniform densities relates to complicated geometries that may dictate the number of training points required for accurate classification (especially when many levels are needed) and is a great direction for future work. However, we} believe analyzing this simpler setup will provide a foundation for a more generalized analysis in future work. 

Let $A_1$ denote the angular measure of $G_1$, defined by \[A_1 = \max_{x_1, x_2\in G_1} \angle(x_1,x_2),\] where $\angle(x_1,x_2)$ denotes the angle between the vectors $x_1$ and $x_2$; define $A_2$ similarly for $G_2$.  Also, define \[A_{12} = \min_{x_1\in G_1, x_2\in G_2} \angle(x_1,x_2)\] as the angle between classes $G_1$ and $G_2$. Suppose that the test point $x\in G_1$, and  that we classify $x$ using $m$ random hyperplanes. For simplicity, we assume that the hyperplanes can intersect the cones, but only intersect \textit{one} cone at a time. This means we are imposing the condition $A_{12} + A_1 + A_2 \leq \pi$. See Figure \ref{2d cones} for a visualization of the setup for the analysis. Notice that $A_1$ is partitioned into two disjoint pieces, $\theta_1$ and $\theta_2$, where $A_1 = \theta_1+\theta_2$. The angles $\theta_1$ and $\theta_2$ are determined by the location of $x$ within $G_1$. 

\begin{figure}[!htbp]
\centering
\includegraphics[height=3in]{images/Theory/Slide1.eps}
\caption{Visualization of the analysis setup for two classes of two dimensions. If a hyperplane intersects the $\theta_1$ region of $G_1$, then $x$ is not on the same side of the hyperplane as $G_2$. If a hyperplane intersects the $\theta_2$ region of $G_1$, then $x$ is on the same side of the hyperplane as $G_2$. That is, $\theta_1$ and $\theta_2$ are determined by the position of $x$ within $G_1$, and $\theta_1+\theta_2 = A_1$.}
\label{2d cones}
\end{figure}

The \RF parameter (\ref{RF3}) is still used; however, now we have angles instead of numbers of training points. That is,
\begin{align} \label{RF3 continuous}
r(\ell,i,t,g) &= \frac{A_{g|t}}{\sum_{j=1}^G A_{j|t}} \frac{\sum_{j=1}^G |A_{g|t} - A_{j|t}|}{\sum_{j=1}^G A_{j|t}},
\end{align}
where $A_{g|t}\edit{ = A_{g|t}(\GamLi{\ell}{i})}$ denotes the angle of \edit{the part of} class $g$ with the $t$-th sign pattern \edit{index} at the $i$-th set selection in the $\ell$-th \edit{level}.
Throughout, let $t_i^\star$ denote the sign pattern index of the test point $x$ with the $i$-th hyperplane at the first \edit{level}, $\ell=1$; \edit{i.e. $t_i^\star = t_{\Lambda_{\ell,i}}^\star$ with the identification $\Lambda_{\ell, i} = \{i\}$ (since $\ell=1$ implies a single hyperplane is used). }
Letting $\widehat{b}_x$ denote the classification label for $x$ after running the proposed algorithm, Theorem \ref{main theorem} describes the probability that $x$ gets classified correctly with $\widehat{b}_x = 1$. Note that for simplicity, in Theorem \ref{main theorem} we assume the classes $G_1$ and $G_2$ are of the same size (i.e., $A_1=A_2$) and the test point $x$ lies in the middle of class $G_1$ (i.e., $\theta_1 = \theta_2$). These assumptions are for convenience and clarity of presentation only (note that \eqref{RF3 bound multinomial complement} is already quite cumbersome), but the proof follows analogously (albeit without easy simplifications) for the general case; for convenience we leave the computations in Table \ref{table::redness factors for 2d cones} in general form and do not utilize the assumption $\theta_1 = \theta_2$ until the end of the proof.  \edit{We first state a technical result in Theorem \ref{main theorem}, and include two corollaries below that illustrate its usefulness.}
%


\begin{theorem}\label{main theorem}
Let the classes $G_1$ and $G_2$ be two cones in $\R^2$ defined by angular measures $A_1$ and $A_2$, respectively, and suppose regions of the same angular measure have the same density of training points. Suppose $A_1 = A_2$, $\theta_1 = \theta_2$, and $A_{12} + A_1 + A_2 \leq \pi$. Then, the probability that a data point $x\in G_1$ gets classified in class $G_1$ by Algorithms \ref{proposed algorithm1} and \ref{proposed algorithm2} using a single \edit{level} and a measurement matrix $A\in\R^{m\times 2}$ with independent standard Gaussian entries is bounded as follows,
\begin{align}
\mathbb{P}[\widehat{b}_x = 1] &\geq 1 - \hspace{-8mm} \underset{j+k_{1,1}+k_{1,2}+k_2 + k = m, \,\, k_{1,2} \geq 9(j+k_{1,1})}{\sum_{j=0}^m \sum_{k_{1,1}=0}^m \sum_{k_{1,2}=0}^m \sum_{k_2=0}^m \sum_{k=0}^m} \binom{m}{j,k_{1,1},k_{1,2},k_2,k} \left(\frac{A_{12}}{\pi}\right)^j \left(\frac{A_1}{2\pi} \right)^{k_{1,1}+k_{1,2}}   \notag \\
&\quad\quad\times \left(\frac{A_1}{\pi} \right)^{k_2} \left(\frac{\pi-2A_1-A_{12}}{\pi}\right)^k. \label{RF3 bound multinomial complement}
\end{align}
\end{theorem}
Figure \ref{theorem figure} displays the classification probability bound of Theorem \ref{main theorem} compared to the (simulated) true value of $\mathbb{P}[\widehat{b}_x = 1]$. 
Here, $A_1 = A_2 = 15^\circ$, $\theta_1 = \theta_2 = 7.5^\circ$, and $A_{12}$ and $m$ are varied. Most importantly, notice that in all cases, the classification probability is approaching 1 with increasing $m$. Also, the result from Theorem \ref{main theorem} behaves similarly as the simulated true probability, especially as $m$ and $A_{12}$ increase.

The following two corollaries provide asymptotic results for situations where $\mathbb{P}[\widehat{b}_x = 1]$ tends to 1 when $m\rightarrow\infty$. Corollary \ref{Corollary 1} provides this result whenever $A_{12}$ is at least as large as both $A_1$ and $\pi-2A_1-A_{12}$, and Corollary \ref{Corollary 2} provides this result for certain combinations of $A_1$ and $A_{12}$.  \edit{These results of course should match intuition, since as $m$ grows large, our hyperplanes essentially chop up the space into finer and finer wedges. Below, the dependence on the constants on $A_1$, $A_{12}$ is explicit in the proofs.}

\begin{corollary}\label{Corollary 1}
Consider the setup of Theorem \ref{main theorem}. Suppose $A_{12} \geq A_1$ and \edit{$2A_{12} \geq \pi-2A_1$}. %$A_{12} \geq \pi - 2A_1 - A_{12}$. 
Then $\mathbb{P}[\widehat{b}_x = 1] \rightarrow 1$ as $m\rightarrow \infty$.  \edit{In fact, the probability converges to $1$ exponentially, i.e. $\mathbb{P}[\hat{b}_x =1] \geq 1-Ce^{-c m}$ for positive constants $c$ and $C$ that may depend on $A_1, A_{12}$.}
\end{corollary}

\begin{corollary}\label{Corollary 2}
Consider the setup of Theorem \ref{main theorem}. Suppose $A_1 + A_{12} > 0.58 \pi$ and $A_{12} + \frac{3}{4}A_1 \leq \frac{\pi}{2}$. Then $\mathbb{P}[\widehat{b}_x = 1] \rightarrow 1$ as $m\rightarrow \infty$.  \edit{In fact, the probability converges to $1$ exponentially, i.e. $\mathbb{P}[\hat{b}_x =1] \geq 1-Ce^{-c m}$ for positive constants $c$ and $C$ that may depend on $A_1$, $A_{12}$.}
\end{corollary}
%\begin{remark}
%Note that the two conditions in Corollary \ref{Corollary 2} imply the assumption that $A_1 \geq 0.32\pi$ and $A_{12} \leq 0.26 \pi$.
%\end{remark}



\begin{figure}[!htbp]
\centering
%\includegraphics[height=3in]{images/Theory/{ClassificationProbability_CutOneDisk_varythetaG1G2_15.00thetaG1_15.00thetaG2_7.50theta1_7.50theta2_3RF}.eps}
\includegraphics[height=3in]{images/Theory/theory_1.eps}
\caption{$\mathbb{P}[\widehat{b}_x = 1]$ versus the number of hyperplanes $m$ when $A_{12}$ is varied (see legend), $A_1 = A_2 = 15^\circ$, and $\theta_1 = \theta_2 = 7.5^\circ$. The solid lines indicate the probability (\ref{classification with cutting continuous}) with the multinomial probability given by (\ref{multinomial probability}) and the conditional probability (\ref{conditional probability continuous RF3}) simulated over 1000 trials of the uniform random variables. The dashed lines indicate the result (\ref{RF3 bound multinomial complement}) provided in Theorem \ref{main theorem}.}
\label{theorem figure}
\end{figure}
\subsection{Proof of Main Results}

\subsubsection{Proof of Theorem \ref{main theorem}}

\begin{proof}
%\edit{To ease readability, we adopt the simplified notation that $k_{1,1} := k_{1,\theta_1}$ and $k_{1,2} := k_{1,\theta_2}$.}
Using our setup, we have five possibilities for any given hyperplane: (i) the hyperplane completely separates the two classes, i.e., the cones associated with the two classes fall on either side of the hyperplane, (ii) the hyperplane completely does not separate the two classes, i.e.,  the cones fall on the same side of the hyperplane, (iii) the hyperplane cuts through $G_2$, (iv) the hyperplane cuts through $G_1$ via $\theta_1$, or (v) the hyperplane cuts through $G_1$ via $\theta_2$. Using this observation, we can now define the event 
%
\begin{equation}\label{theevent}
E( j,k_{1,1},k_{1,2},k_2 )
\end{equation} 
%
whereby from among the $m$ total hyperplanes, $j$ 
hyperplanes  separate the cones, $k_{1,1}$ hyperplanes  cut  $G_1$ in  $\theta_1$, $k_{1,2}$ hyperplanes cut  $G_1$  in  $\theta_2$, and  $k_2$ hyperplanes cut  $G_2$. 
See Table \ref{table::redness factors for 2d cones} for an easy reference of these quantities. Note that we must distinguish between hyperplanes that cut through $\theta_1$ and those that cut through $\theta_2$; $k_{1,1}$ hyperplanes cut $G_1$ and land within $\theta_1$ so that $x$ is \textit{not} on the same side of the hyperplane as $G_2$ whereas $k_{1,2}$ hyperplanes cut $G_1$ and land within $\theta_2$ so that $x$ \textit{is} on the same side of the hyperplane as $G_2$. These orientations will affect the computation of the membership index. Using the above definition of (\ref{theevent}), we use the law of total probability to get a handle on $\mathbb{P}[\widehat{b}_x = 1]$, the probability that the test point $x$ gets classified correctly, as follows,

{\small
\begin{align}
\mathbb{P}[\widehat{b}_x = 1] &= \mathbb{P}\left[\sum_{i=1}^m r(\ell,i,t_i^\star,1) > \sum_{i=1}^m r(\ell,i,t_i^\star,2)\right] \notag\\
&= \underset{j+k_{1,1}+k_{1,2}+k_2\leq m}{\sum_{j,k_{1,1}, k_{1,2},k_2 } }%_{j=0}^m \sum_{k_{1,1}=0}^m \sum_{k_{1,2}=0}^m \sum_{k_2=0}^m} 
\mathbb{P}\left[\sum_{i=1}^m r(\ell,i,t^\star_i,1) > \sum_{i=1}^m r(\ell,i,t^\star_i,2) \;| E( j,k_{1,1},k_{1,2},k_2 )
%\; j \mbox{ separate}, k_{1,1}\mbox{ cut } G_1 \mbox{ in } \theta_1, k_{1,2}\mbox{ cut } G_1 \mbox{ in } \theta_2, k_2\mbox{ cut } G_2
\right]\notag\\
&\qquad\qquad\qquad\qquad\times \mathbb{P}\left[% j \mbox{ separate}, k_{1,1}\mbox{ cut } G_1 \mbox{ in } \theta_1, k_{1,2}\mbox{ cut } G_1 \mbox{ in } \theta_2,  k_2\mbox{ cut } G_2
E( j,k_{1,1},k_{1,2},k_2 )
\right]. \label{classification with cutting continuous}
\end{align}}
 The latter probability in (\ref{classification with cutting continuous}) is similar to the probability density of a multinomial random variable:
\begin{align}
&\mathbb{P}\left[E( j,k_{1,1},k_{1,2},k_2 )% j \mbox{ separate}, k_{1,1}\mbox{ cut } G_1 \mbox{ in } \theta_1, k_{1,2}\mbox{ cut } G_1 \mbox{ in } \theta_2,  k_2\mbox{ cut } G_2
\right] \notag \\
&= \binom{m}{j,k_{1,1},k_{1,2},k_2,m-j-k_{1,1}-k_{1,2}-k_2} \left(\frac{A_{12}}{\pi}\right)^j \left(\frac{\theta_1}{\pi} \right)^{k_{1,1}} \left(\frac{\theta_2}{\pi} \right)^{k_{1,2}} \notag \\
&\quad\times \left(\frac{A_2}{\pi} \right)^{k_2} \left(\frac{\pi-A_1-A_2-A_{12}}{\pi}\right)^{m-j-k_{1,1}-k_{1,2}-k_2}, \label{multinomial probability}
\end{align}
where $\binom{n}{k_1,k_2,\dots,k_m} = \frac{n!}{k_1!k_2!\cdots k_m!}$. 




To evaluate the conditional probability in (\ref{classification with cutting continuous}), we must determine the value of $r(\ell,i,t_i^\star,g)$, for $g=1,2$, given the hyperplane cutting pattern event. Table \ref{table::redness factors for 2d cones} summarizes the possible cases. \edit{In the cases where the hyperplane cuts through either $G_1$ or $G_2$, we model the location of the hyperplane within the class by a random variable defined on the interval $[0,1]$, with no assumed distribution. We let $u$, $u'$, $u_h$, $u_h'$ $\in [0,1]$ (for an index $h$) denote independent copies of such random variables.}
%In the cases where the hyperplane cuts through either $G_1$ or $G_2$, we model the location of the hyperplane within the class by a uniform random variable. We will use the notation $z\sim U(a,b)$ to indicate that $z$ is a uniform random variable over the interval $[a,b]$, and let $u$, $u'$, $u_h$, $u_h'$ (for an index $h$) denote independent copies of a $U(0,1)$ uniform random variable; therefore $\theta u \sim U(0, \theta)$. 

\begin{table}[!htbp]
\centering
\begin{tabular}{| c | c |c | c |}
\hline
Hyperplane Case & Number in event \eqref{theevent} & Class $g$ & Value of $r(\ell,i,t_i^\star,g)$ (see \eqref{RF3 continuous})\\ 
\hline
\hline
\multirow{2}{*}{(i) separates} & \multirow{2}{*}{$j$} & 1 & $1$\\ 
 & & 2 & $0$\\ 
\hline
\hline
\multirow{2}{*}{(ii) does not separate} & \multirow{2}{*}{$m - j - k_2 - k_{1,1} - k_{1,2}$}  &  1 & $\frac{A_1|A_1-A_2|}{(A_1+A_2)^2}$ \\ 
&& 2 & $\frac{A_2|A_1-A_2|}{(A_1+A_2)^2}$\\ 
\hline
\hline
\multirow{2}{*}{(iii) cuts $G_2$} & \multirow{2}{*}{$k_2$} &  1 & $\frac{A_1|A_1-A_2u'|}{(A_1+A_2u')^2}$\\ 
&& 2 & $\frac{A_2 u'|A_1-A_2u'|}{(A_1+A_2u')^2}$\\ 
\hline
\hline
\multirow{2}{*}{(iv) cuts $G_1$, $\theta_1$}& \multirow{2}{*}{$k_{1,1}$} & 1 & $1$\\ 
&& 2 & $0$\\ 
\hline
\hline
\multirow{2}{*}{(v) cuts $G_1$, $\theta_2$} & \multirow{2}{*}{$k_{1,2}$} & 1 & $\frac{(\theta_1+\theta_2u)|\theta_1+\theta_2u-A_2|}{(\theta_1+\theta_2u+A_2)^2}$ \\ 
&& 2 & $\frac{A_2|\theta_1+\theta_2u-A_2|}{(\theta_1+\theta_2u+A_2)^2}$ \\ 
\hline
\end{tabular}
\caption{Summary of (\ref{RF3 continuous}) when up to one cone can be cut per hyperplane, where $ u, u'$ \edit{are independent random variables defined over the interval $[0,1]$}.}
%are independent $U(0,1)$ random variables.} 
\label{table::redness factors for 2d cones}
\end{table}

Using the computations given in Table \ref{table::redness factors for 2d cones} and assuming $j$ hyperplanes separate (i.e. condition (i) described above), $k_{1,1}$ hyperplanes cut $G_1$ in $\theta_1$ (condition (iv) above), $k_{1,2}$ hyperplanes cut $G_1$ in $\theta_2$ (condition (v) above),  $k_2$ hyperplanes cut $G_2$ (condition (iii) above), and $m-j-k_{1,1}-k_{1,2}-k_2$ hyperplanes do not separate (condition (ii) above), we compute the membership index parameters defined in \eqref{RF3 continuous} as: 
\begin{align}\label{compRF1}
\sum_{i=1}^m r(\ell,i,t_i^\star,1) &= j + (m-j-k_{1,1}-k_{1,2}-k_2)\frac{A_1|A_1-A_2|}{(A_1+A_2)^2} + k_{1,1} \notag \\
&\quad + \sum_{h=1}^{k_{1,2}}\frac{(\theta_1+\theta_2 u_h)|\theta_1+\theta_2 u_h-A_2|}{(\theta_1+\theta_2 u_h+A_2)^2} + \sum_{h=1}^{k_2}\frac{A_1|A_1-A_2 u_h'|}{(A_1+A_2 u_h')^2} \notag\\
&=  j + k_{1,1} 
 + \sum_{h=1}^{k_{1,2}}\frac{(\theta_1+\theta_2 u_h)|\theta_1+\theta_2 u_h-A_1|}{(\theta_1+\theta_2 u_h+A_1)^2} + \sum_{h=1}^{k_2}\frac{A_1|A_1-A_1 u_h'|}{(A_1+A_1 u_h')^2}
%&=  j + k_{1,1} 
 %+\sum_{h=1}^{k_{1,2}}\frac{(\theta_1+\theta_2 u_h)|\theta_1+\theta_2 u_h-A_1|}{(\theta_1+\theta_2 u_h+A_1)^2} +\sum_{h=1}^{k_2}\frac{A_1|A_1-A_1 u_h'|}{(A_1+A_1 u_h')^2}
\end{align}
and
\begin{align}\label{compRF2}
\sum_{i=1}^m r(\ell,i,t_i^\star,2) &=  (m-j-k_{1,1}-k_{1,2}-k_2)\frac{A_2|A_1-A_2|}{(A_1+A_2)^2}  \notag \\
&\quad + \sum_{h=1}^{k_{1,2}}\frac{A_2|\theta_1+\theta_2 u_h-A_2|}{(\theta_1+\theta_2 u_h+A_2)^2} + \sum_{h=1}^{k_2}\frac{A_2 u_h'|A_1-A_2 u_h'|}{(A_1+A_2 u_h')^2}\notag\\
&=  \sum_{h=1}^{k_{1,2}}\frac{A_1|\theta_1+\theta_2 u_h-A_1|}{(\theta_1+\theta_2 u_h+A_1)^2} + \sum_{h=1}^{k_2}\frac{A_1 u_h'|A_1-A_1 u_h'|}{(A_1+A_1 u_h')^2},
\end{align}
where in both cases we have simplified using the assumption $A_1 = A_2$.
Thus, the conditional probability in (\ref{classification with cutting continuous}), can be expressed as:
\begin{align}
&\mathbb{P}\left[j + k_{1,1} + \sum_{h=1}^{k_{1,2}}\frac{|\theta_1+\theta_2 u_h-A_1|(\theta_1+\theta_2 u_h-A_1)}{(\theta_1+\theta_2 u_h+A_1)^2} + \sum_{h=1}^{k_2}\frac{|A_1-A_1 u_h'|(A_1-A_1 u_h')}{(A_1+A_1 u_h')^2} > 0\right] \label{conditional probability continuous RF3},
\end{align}
where it is implied that this probably is conditioned on the hyperplane configuration as in \eqref{classification with cutting continuous}.
Once the probability (\ref{conditional probability continuous RF3}) is known, we can calculate the full classification probability (\ref{classification with cutting continuous}).

Since by assumption, $\theta_1 + \theta_2 = A_1$, we have $\theta_1+\theta_2 u-A_1 \leq  0 $ and $A_1 -A_1 u' \geq  0$. Thus,  (\ref{conditional probability continuous RF3}) simplifies to \edit{
\begin{align}
&\mathbb{P}\left[j + k_{1,1} - \sum_{h=1}^{k_{1,2}}\frac{(\theta_1+\theta_2 u_h-A_1)^2}{(\theta_1+\theta_2 u_h+A_1)^2} + \sum_{h=1}^{k_2}\frac{(A_1-A_1 u_h')^2}{(A_1+A_1 u_h')^2} > 0\right]%\notag\\
%&= \mathbb{P}\left[j + k_{1,1} - \sum_{h=1}^{k_{1,2}}\frac{(\theta_2 u_h)^2}{(\theta_1+\theta_2 u_h+A_1)^2} + \sum_{h=1}^{k_2}\frac{(A_1 u_h')^2}{(A_1+A_1 u_h')^2} > 0\right]. 
\ \geq \ \mathbb{P}[\gamma > \beta]\label{conditional probability continuous RF3 A1=A2}
\end{align}
where $$\quad\beta = k_{1,2}\left(\frac{\theta_2}{A_1+\theta_1}\right)^2\quad \text{and} \quad\gamma = j+k_{1,1}.$$ 
To obtain the inequality in \eqref{conditional probability continuous RF3 A1=A2}, we used the fact that  $$j + k_{1,1} - \sum_{h=1}^{k_{1,2}}\frac{(\theta_1-A_1)^2}{(\theta_1+\theta_2 u_h+A_1)^2} + \sum_{h=1}^{k_2}\frac{(A_1-A_1 u_h')^2}{(A_1+A_1 u_h')^2} \geq j + k_{1,1} - \sum_{h=1}^{k_{1,2}}\frac{(\theta_1-A_1)^2}{(\theta_1+A_1)^2} + 0=\gamma-\beta.$$
}
%
%Next, using that $\theta_2 u \geq 0$ and $A_1 u' \leq A_1$ for the random variables in the denominators, we can bound (\ref{conditional probability continuous RF3 A1=A2}) from below by 
%\begin{align}
%(\ref{conditional probability continuous RF3 A1=A2}) \geq \mathbb{P}\left[j + k_{1,1} - \sum_{h=1}^{k_{1,2}}\frac{(\theta_2 u_h)^2}{(\theta_1+A_1)^2} + \sum_{h=1}^{k_2}\frac{(A_1 u_h')^2}{(2A_1)^2} > 0\right]. \label{RF3 first bound}
%\end{align}
%
%
%%%%%%%%comment out begins here%%%%%%%%
%Letting $\theta>0$ to be chosen later (and abusing notation slightly to allow $u$, $u'$ to be new independent uniform random variables), we can rewrite the above probability (\ref{RF3 first bound}) as
%\begin{align}
%P:= &\mathbb{P}\left[j + k_{1,1} - \frac{1}{(\theta_1+A_2)^2}\sum_{h=1}^{k_{1,2}}(\theta_2u_h)^2 + \frac{1}{(2A_1)^2} \sum_{h=1}^{k_2} (A_1u_h')^2> 0\right] \notag \\
%&= \mathbb{P}\left[ \frac{1}{(\theta_1+A_1)^2}\sum_{h=1}^{k_{1,2}}(\theta_2 u_h)^2 - \frac{1}{(2A_1)^2} \sum_{h=1}^{k_2} (A_1u_h')^2<  j + k_{1,1}\right] \notag \\
%&= 1- \mathbb{P}\left[ \frac{1}{(\theta_1+A_1)^2}\sum_{h=1}^{k_{1,2}}(\theta_2 u_h)^2 - \frac{1}{(2A_1)^2} \sum_{h=1}^{k_2} (A_1 u_h')^2 \geq  j + k_{1,1}\right] \notag \\
%&= 1- \mathbb{P}\left[ e^{\theta \left(\frac{1}{(\theta_1+A_1)^2}\sum_{h=1}^{k_{1,2}}(\theta_2 u_h)^2 - \frac{1}{(2A_1)^2} \sum_{h=1}^{k_2} (A_1 u_h')^2\right)} \geq  e^{\theta(j + k_{1,1})}\right] \notag \\
%&\geq 1-e^{-\theta(j + k_{1,1})} \mathbb{E}\left[ e^{\theta \left(\frac{1}{(\theta_1+A_1)^2}\sum_{h=1}^{k_{1,2}}(\theta_2 u_h)^2 - \frac{1}{(2A_1)^2} \sum_{h=1}^{k_2} (A_1 u_h')^2\right)} \right] \label{Markov step}\end{align}
%where in the last equality, $\theta$ is a non-negative parameter to be chosen later, and (\ref{Markov step}) follows from Markov's inequality. Continuing and using the independence of each hyperplane, we now have 
%
%\begin{align}
%P &\geq 1-e^{-\theta(j + k_{1,1})} \mathbb{E}\left[ \prod_{h=1}^{k_{1,2}} e^{ \frac{\theta}{(\theta_1+A_1)^2}(\theta_2 u_h)^2}\right] \mathbb{E}\left[ \prod_{h=1}^{k_2} e^{- \frac{\theta}{(2A_1)^2} (A_1 u_h')^2)} \right] \notag \\
%&= 1-e^{-\theta(j + k_{1,1})}\prod_{h=1}^{k_{1,2}} \mathbb{E}\left[ e^{ \frac{\theta}{(\theta_1+A_1)^2}(\theta_2 u_h)^2}\right] \prod_{h=1}^{k_2} \mathbb{E}\left[ e^{- \frac{\theta}{(2A_1)^2} (A_1 u_h')^2)} \right] \notag \\
%&= 1-e^{-\theta(j + k_{1,1})}\left(\mathbb{E}\left[ e^{ \frac{\theta}{(\theta_1+A_1)^2}(\theta_2 u)^2}\right] \right)^{k_{1,2}} \left( \mathbb{E}\left[ e^{- \frac{\theta}{(2A_1)^2} (A_1 u')^2)} \right]\right)^{k_2}. \label{last equality}
%\end{align}
% Next, one readily computes the following (we include the computation in the appendix, Section \ref{app:erf}, for completeness):
%\begin{align}\label{helloerf}
%\mathbb{E}\left[ e^{ \frac{\theta}{(\theta_1+A_1)^2}(\theta_2 u)^2}\right] = \frac{\sqrt{\pi} \mbox{erfi}(\frac{\theta_2}{A_1+\theta_1}\sqrt{\theta})}{\frac{\theta_2}{A_1+\theta_1}\sqrt{\theta}}
%\end{align}
%and
%\begin{align}\label{helloerf2}
%\mathbb{E}\left[ e^{- \frac{\theta}{(2A_1)^2} (A_1 u')^2)} \right] = \frac{\sqrt{\pi}\mbox{erf}(\frac{1}{2}\sqrt{\theta})}{\frac{1}{2}\sqrt{\theta}},
%\end{align}
%where $\mbox{erf}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} \,\, dt$ is the error function and $\mbox{erfi}(x) = -i\mbox{erf}(ix)= \frac{2}{\sqrt{\pi}} \int_0^x e^{t^2} \,\, dt$ is the imaginary error function. 
%Therefore,
%\begin{align}
%%&\mathbb{P}\left[j + k_{1,1} - \sum_{h=1}^{k_{1,2}}\frac{(\theta_1+\theta_2 u-A_2)^2}{(\theta_1+A_2)^2} + \sum_{h=1}^{k_2}\frac{(A_1-A_2 u')^2}{(A_1+A_2)^2} > 0\right] \notag\\
%&P \geq 1-e^{-\theta(j + k_{1,1})}\left(\frac{\sqrt{\pi} \mbox{erfi}(\frac{\theta_2}{A_1+\theta_1}\sqrt{\theta})}{\frac{\theta_2}{A_1+\theta_1}\sqrt{\theta}} \right)^{k_{1,2}} \left( \frac{\sqrt{\pi}\mbox{erf}(\frac{1}{2}\sqrt{\theta})}{\frac{1}{2}\sqrt{\theta}}\right)^{k_2}. \label{RF3 bound erf}
%\end{align}
%%
%Now, we note the following trivial upper bounds on the $\mbox{erf}$ and $\mbox{erfi}$ functions.
%\begin{align}
%\mbox{erf}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2}\,\,dt \leq \frac{2}{\sqrt{\pi}} \int_0^x 1 \,\,dt = \frac{2x}{\sqrt{\pi}} \\
%\mbox{erfi}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{t^2} \,\,dt \leq \frac{2}{\sqrt{\pi}} e^{x^2} \int_0^x 1 \,\, dt = \frac{2x}{\sqrt{\pi}} e^{x^2}.
%\end{align}
%Applying the above bounds to \eqref{RF3 bound erf} 
%gives 
%\begin{align}
%\mathbb{P}[\widehat{b}_x = 1] &\geq %1-e^{-\theta(j + k_{1,1})}\left(\frac{\sqrt{\pi} \mbox{erfi}(\frac{A_2-\theta_1}{A_2+\theta_1}\sqrt{\theta})}{\frac{A_2-\theta_1}{A_2+\theta_1}\sqrt{\theta}} \right)^{k_{1,2}} \left( \frac{\sqrt{\pi}\mbox{erf}(\frac{A_1}{A_1+A_2}\sqrt{\theta})}{\frac{A_1}{A_1+A_2}\sqrt{\theta}}\right)^{k_2} \label{byebyeA2}\\
%%&= 
%1-e^{-\theta(j + k_{1,1})}\left(\frac{\sqrt{\pi} \mbox{erfi}(\frac{\theta_2}{A_1+\theta_1}\sqrt{\theta})}{\frac{\theta_2}{A_1+\theta_1}\sqrt{\theta}} \right)^{k_{1,2}} \left( \frac{\sqrt{\pi}\mbox{erf}(\frac{\sqrt{\theta}}{2})}{\frac{\sqrt{\theta}}{2}}\right)^{k_2} \notag\\
%&\geq 1-2^{k_{1,2}+k_2} e^{-\theta(j+k_{1,1})} e^{k_{1,2}\theta(\frac{\theta_2}{A_1+\theta_1})^2}\notag\\
%&= 1-2^{k_{1,2}+k_2} e^{\theta\left(k_{1,2}(\frac{\theta_2}{A_1+\theta_1})^2 -(j+k_{1,1})\right)}\notag\\
%&= 1-\alpha e^{\theta(\beta-\gamma)} =: f(\theta) \label{RF3 bound erf trivial}
%\end{align}
%where $$\alpha = 2^{k_{1,2}+k_2},\quad\beta = k_{1,2}\left(\frac{\theta_2}{A_1+\theta_1}\right)^2\quad \text{and} \quad\gamma = j+k_{1,1}.$$ Recall that we wish to choose $\theta>0$ such that $f(\theta)$ in \eqref{RF3 bound erf trivial} is maximized. If $\beta-\gamma <0$, then taking $\theta\rightarrow\infty$ maximizes $f(\theta)$ as $f(\theta)\rightarrow 1$. If $\beta-\gamma\geq 0$, then taking $\theta\rightarrow 0$ maximizes $f(\theta)$ as $f(\theta)\rightarrow 1-\alpha < 0$. But, since we are bounding a probability, we always have the trivial lower bound of zero. So, when $\beta-\gamma \geq 0$ we can use the simple bound $ \mathbb{P}[\widehat{b}_x = 1] \geq 0$.
%%%%%%%%comment out ends here%%%%%%%%
%
\edit{By conditioning on $\gamma>\beta$},  the probability of interest \eqref{classification with cutting continuous} reduces to (note the bounds on the summation indices):
\begin{align}
\mathbb{P}[\widehat{b}_x = 1] %&= \mathbb{P}\left[\sum_{i=1}^m r(\ell,i,t_i^\star,1) > \sum_{i=1}^m r(\ell,i,t_i^\star,2)\right] \notag\\
&= \underset{j+k_{1,1}+k_{1,2}+k_2\leq m}{\sum_{j,k_{1,1}, k_{1,2},k_2 } }%_{j=0}^m \sum_{k_{1,1}=0}^m \sum_{k_{1,2}=0}^m \sum_{k_2=0}^m} 
\mathbb{P}\left[\sum_{i=1}^m r(\ell,i,t^\star_i,1) > \sum_{i=1}^m r(\ell,i,t^\star_i,2) \;| E( j,k_{1,1},k_{1,2},k_2 )
%\; j \mbox{ separate}, k_{1,1}\mbox{ cut } G_1 \mbox{ in } \theta_1, k_{1,2}\mbox{ cut } G_1 \mbox{ in } \theta_2, k_2\mbox{ cut } G_2
\right]\notag\\
&\qquad\qquad\qquad\qquad\times \mathbb{P}\left[% j \mbox{ separate}, k_{1,1}\mbox{ cut } G_1 \mbox{ in } \theta_1, k_{1,2}\mbox{ cut } G_1 \mbox{ in } \theta_2,  k_2\mbox{ cut } G_2
E( j,k_{1,1},k_{1,2},k_2 )
\right]\\
%
&\geq  %\underset{j+k_{1,1}+k_{1,2}+k_2\leq m, \,\, \beta-\gamma<0}%{\sum_{j=0}^m \sum_{k_{1,1}=0}^m \sum_{k_{1,2}=0}^m \sum_{k_2=0}^m}
{\sum_{\substack{j,k_{1,1}, k_{1,2},k_2 \\ j+k_{1,1}+k_{1,2}+k_2\leq m, \\ \beta-\gamma<0} }}
 \binom{m}{j,k_{1,1},k_{1,2},k_2,m-j-k_{1,1}-k_{1,2}-k_2} \left(\frac{A_{12}}{\pi}\right)^j \left(\frac{\theta_1}{\pi} \right)^{k_{1,1}} \notag\\
&\quad\quad \times  \left(\frac{\theta_2}{\pi} \right)^{k_{1,2}} \left(\frac{A_2}{\pi} \right)^{k_2} \left(\frac{\pi-A_1-A_2-A_{12}}{\pi}\right)^{m-j-k_{1,1}-k_{1,2}-k_2}.\label{choose theta bound}
\end{align}
The condition $\beta-\gamma < 0$ is equivalent to $k_{1,2}(\frac{\theta_2}{A_1+\theta_1})^2 - ( j+k_{1,1}) < 0$, which implies $ k_{1,2}(\frac{\theta_2}{A_1+\theta_1})^2 < j+k_{1,1}$. Assuming $\theta_1=\theta_2$ simplifies this condition to depend \textit{only} on the hyperplane configuration (and not $A_1$, $\theta_1$, and $\theta_2$) since $\frac{\theta_2}{A_1+\theta_1} = \frac{\theta_2}{3\theta_2} = \frac{1}{3}$. Thus, the condition $\beta-\gamma <0$ reduces to the condition $k_{1,2} < 9(j+k_{1,1})$ 
and (\ref{choose theta bound}) then simplifies to
\begin{align}
&%\underset{j+k_{1,1}+k_{1,2}+k_2\leq m, \,\, k_{1,2} < 9(j+k_{1,1})}
{\sum_{\substack{j+k_{1,1}+k_{1,2}+k_2\leq m, \\ k_{1,2} < 9(j+k_{1,1})}} %\sum_{k_{1,1}=0}^m \sum_{k_{1,2}=0}^m \sum_{k_2=0}^m
}
 \binom{m}{j,k_{1,1},k_{1,2},k_2,m-j-k_{1,1}-k_{1,2}-k_2} \left(\frac{A_{12}}{\pi}\right)^j \left(\frac{\theta_1}{\pi} \right)^{k_{1,1}+k_{1,2}} \notag\\
&\quad\quad \times  \left(\frac{A_2}{\pi} \right)^{k_2} \left(\frac{\pi-2A_1-A_{12}}{\pi}\right)^{m-j-k_{1,1}-k_{1,2}-k_2} \\
&= \sum_{\substack{j+k_{1,1}+k_{1,2}+k_2 + k = m, \\ k_{1,2} < 9(j+k_{1,1})}}
%\underset{j+k_{1,1}+k_{1,2}+k_2 + k = m, \,\, k_{1,2} < 9(j+k_{1,1})}{\sum_{j=0}^m \sum_{k_{1,1}=0}^m \sum_{k_{1,2}=0}^m \sum_{k_2=0}^m \sum_{k=0}^m} 
\binom{m}{j,k_{1,1},k_{1,2},k_2,k} \left(\frac{A_{12}}{\pi}\right)^j \left(\frac{\theta_1}{\pi} \right)^{k_{1,1}+k_{1,2}}   \left(\frac{A_2}{\pi} \right)^{k_2} \left(\frac{\pi-2A_1-A_{12}}{\pi}\right)^k, \\
&= %\underset{j+k_{1,1}+k_{1,2}+k_2 + k = m, \,\, k_{1,2} < 9(j+k_{1,1})}{\sum_{j=0}^m \sum_{k_{1,1}=0}^m \sum_{k_{1,2}=0}^m \sum_{k_2=0}^m \sum_{k=0}^m} 
\sum_{\substack{j+k_{1,1}+k_{1,2}+k_2 + k = m, \\ k_{1,2} < 9(j+k_{1,1})}}\binom{m}{j,k_{1,1},k_{1,2},k_2,k} \left(\frac{A_{12}}{\pi}\right)^j \left(\frac{A_1}{2\pi} \right)^{k_{1,1}+k_{1,2}}   \left(\frac{A_1}{\pi} \right)^{k_2} \left(\frac{\pi-2A_1-A_{12}}{\pi}\right)^k, \label{RF3 bound multinomial}
\end{align}
where we have introduced $k$ to denote the number of hyperplanes that do not separate nor cut through either of the groups, and simplified using the assumptions that $\theta_1 = \frac{A_1}{2}$ and $A_1 = A_2$. 

Note that if we did not have the condition $k_{1,2} < 9(j+k_{1,1})$ in the sum (\ref{RF3 bound multinomial}) (that is, if we summed over all terms), the quantity would sum to 1 (this can easily be seen by the Multinomial Theorem). Finally, this means (\ref{RF3 bound multinomial}) is equivalent to (\ref{RF3 bound multinomial complement}), thereby completing the proof.
\end{proof}

\subsubsection{Proof of Corollary \ref{Corollary 1}}

\begin{proof}
We can  bound (\ref{RF3 bound multinomial complement}) from below by  bounding the excluded terms in the sum (i.e., those that satisfy $k_{1,2} \geq 9(j+k_{1,1})$) from above. One approach to this would be to count the number of terms satisfying $k_{1,2} \geq 9(j+k_{1,1})$ and bound them by their maximum.  Using basic combinatorics (see the appendix, Section \ref{app:comb}),
that the number of terms satisfying $k_{1,2} \geq 9(j+k_{1,1})$ is given by
\begin{align} \label{multinomial count}
W_1 = \frac{1}{12} \left(\left\lfloor \frac{m}{10} \right\rfloor + 1\right) \left(\left\lfloor \frac{m}{10} \right\rfloor + 2\right) \left(150 \left\lfloor \frac{m}{10} \right\rfloor^2 - 10(4m + 1)\left\lfloor \frac{m}{10} \right\rfloor +3(m^2 + 3m + 2)\right) \sim m^4.
\end{align}
Then, the quantity (\ref{RF3 bound multinomial complement}) can be bounded below by
\begin{align} 
&1 - W_1 \max\left( \binom{m}{j,k_{1,1},k_{1,2},k_2,k} \left(\frac{A_{12}}{\pi}\right)^j \left(\frac{A_1}{2\pi} \right)^{k_{1,1}+k_{1,2}}   \left(\frac{A_1}{\pi} \right)^{k_2} \left(\frac{\pi-2A_1-A_{12}}{\pi}\right)^k \right) =\notag\\
&1 - W_1 \max\left( \binom{m}{j,k_{1,1},k_{1,2},k_2,k} \left(\frac{1}{2} \right)^{k_{1,1}+k_{1,2}} \left(\frac{A_{12}}{\pi}\right)^j \left(\frac{A_1}{\pi} \right)^{k_{1,1}+k_{1,2}+k_2} \left(\frac{\pi-2A_1-A_{12}}{\pi}\right)^k \right), \label{RF3 bound multinomial estimate}
\end{align}
where the maximum is taken over all $j,k_{1,1},k_{1,2}, k_2, k = 0,\dots,m$ such that $k_{1,2} \geq 9(j+k_{1,1})$.
Ignoring the constraint  $k_{1,2} \geq 9(j+k_{1,1})$, we can upper bound the multinomial coefficient \edit{using the trivial upper bound of $5^m$:}%.
%That is, assuming $\frac{m}{5} \in \mathbb{Z}$ for simplicity and applying Stirling's approximation for the factorial $n! \sim \sqrt{2\pi n} (\frac{n}{e})^n$, we get 
\edit{
\begin{align}\label{multinomial bound}
\binom{m}{j,k_{1,1},k_{1,2},k_2,k}  &\leq 5^m. 
%\frac{m!}{[(\frac{m}{5})!]^5} \notag\\
%&\sim \frac{\sqrt{2\pi m} (\frac{m}{e})^m}{[\sqrt{2\pi \frac{m}{5}} %(\frac{m}{5e})^{m/5}]^5} \notag\\
%&= \frac{5^{m + 5/2}}{(2\pi m)^2}. 
\end{align}
}
Since we are assuming $A_{12}$ is larger than $A_1$ and $\pi-2A_1-A_{12}$ \edit{(from the assumption that $2A_{12}\geq \pi - 2A_1$)}, the strategy is to take $j$ to be as large as possible while satisfying $k_{1,2} \geq 9j$ and $j + k_{1,2} = m$. Since $k_{1,2} \geq 9j$, we have $j + 9j \leq m$ which implies $j \leq \frac{m}{10}$. So, we take $j= \frac{m}{10}$, $k_{1,2} = \frac{9m}{10}$, and $k_{1,1} = k_2 = k = 0$. 
Then
\begin{align}
 \left(\frac{1}{2} \right)^{k_{1,1}+k_{1,2}} \left(\frac{A_{12}}{\pi}\right)^j \left(\frac{A_1}{\pi} \right)^{k_{1,1}+k_{1,2}+k_2} &\left(\frac{\pi-2A_1-A_{12}}{\pi}\right)^k \\
 &\leq \left(\frac{1}{2} \right)^{9m/10} \left(\frac{A_{12}}{\pi} \right)^{m/10} \left(\frac{A_1}{\pi} \right)^{9m/10} \notag\\
 &= \left(\frac{1}{2^9} \frac{A_{12}}{\pi} \left(\frac{A_1}{\pi}\right)^9 \right)^{m/10}.\label{rest bound}
\end{align}
Combining (\ref{RF3 bound multinomial estimate}) with the bounds given in (\ref{multinomial bound}) and (\ref{rest bound}), we have
\begin{align}
&\geq 1 - W_1  \edit{5^m}%\frac{5^{m + 5/2}}{(2\pi m)^2}  
\left(\frac{1}{2^9} \frac{A_{12}}{\pi} \left(\frac{A_1}{\pi}\right)^9 \right)^{m/10} \notag\\
&\sim 1- m^4  \edit{5^m}%\frac{5^{m + 5/2}}{(2\pi m)^2}  
\left(\frac{1}{2^9} \frac{A_{12}}{\pi} \left(\frac{A_1}{\pi}\right)^9 \right)^{m/10} \notag\\
&= 1- m^2  %\frac{5^{5/2}}{(2\pi)^2}  
\left(5^{10} \frac{1}{2^9} \frac{A_{12}}{\pi} \left(\frac{A_1}{\pi}\right)^9 \right)^{m/10}.
\end{align}
For the above to tend to 1 as $m\rightarrow \infty$, we need $ \frac{5^{10}}{2^9}  \frac{A_{12}}{\pi} \left(\frac{A_1}{\pi}\right)^9 < 1$. This is equivalent to $A_{12} \left(\frac{A_1}{2}\right)^9 < \frac{\pi^{10}}{5^{10}}$, which implies $A_{12} \theta_1^9 < \left(\frac{\pi}{5}\right)^{10} = \frac{\pi}{5}\left(\frac{\pi}{5}\right)^{9}$. Note that if $\theta_1 = \frac{\pi}{5}$, then $A_1 = A_2 = 2\theta_1 = \frac{2\pi}{5}$. Then $A_{12}$ could be at most $\frac{\pi}{5}$. But, this can't be because we have assumed $A_{12} \geq A_1$. Thus, we must have $\theta_1 < \frac{\pi}{5}$. In fact, $\theta_1 = \frac{\pi}{6}$ is the largest possible, in which case $A_{12} = A_1 = A_2 = \frac{\pi}{3}$. If $ \theta_1 = \frac{\pi}{6}$, then $A_{12} \theta_1^9 < \frac{\pi}{5}\left(\frac{\pi}{5}\right)^{9}$ becomes $A_{12} < \frac{\pi}{5} \left(\frac{6}{5} \right)^9 \approx 3.24$. Therefore, since we are already assuming $A_{12} + 2A_1 \leq \pi$, this is essentially no further restriction on $A_{12}$, and the same would be true for all $\theta_1 \leq \frac{\pi}{6}$. This completes the proof.
\end{proof}

\subsubsection{Proof of Corollary \ref{Corollary 2}}

\begin{proof}
Consider (\ref{RF3 bound multinomial complement})  and set $j' = j + k_{1,1}$ and $r = k_2 + k$. Then we view (\ref{RF3 bound multinomial complement}) as a probability equivalent to 
\begin{align}\label{equivalent probability}
1 - \underset{j'+k_{1,2}+r = m, \,\, k_{1,2} \geq 9j'}{\sum_{j'=0}^{2m} \sum_{k_{1,2}=0}^m \sum_{r=0}^{2m}} \binom{m}{k_{1,2},j',r} \left(\frac{A_{12}+\frac{A_1}{2}}{\pi} \right)^{j'} \left(\frac{A_1}{2\pi} \right)^{k_{1,2}} \left(\frac{\pi-A_1-A_{12}}{\pi} \right)^{r}.
\end{align}
Note that multinomial coefficients are maximized when the parameters all attain the same value. Thus, the multinomial term above is maximized when $  k_{1,2}$, $j'$ and $r$ are all as close to one another as possible. Thus, given the additional constraint that $k_{1,2} \geq 9j'$, 
the multinomial term is maximized when $k_{1,2}=\frac{9m}{19}$, $j' = \frac{m}{19}$, and $r = \frac{9m}{19}$ (possibly with ceilings/floors as necessary if $m$ is not a multiple of 19), (see the appendix, Section \ref{app:facts}, for a quick explanation),
which means
\begin{align}
\binom{m}{k_{1,2},j',r} &\leq \frac{m!}{(\frac{9m}{19})!(\frac{m}{19})!(\frac{9m}{19})!} \label{facts} \\
&\sim \frac{\sqrt{2\pi m}(\frac{m}{e})^m}{2\pi \frac{9m}{19} (\frac{9m}{19e})^{18m/19} \sqrt{2\pi \frac{m}{19}}(\frac{m}{19e})^{m/19} }\label{Stirling step} \\
&= \frac{19\sqrt{19}}{18\pi m} \left( (\frac{19}{9})^{18/19} {19^{1/19}} \right)^m \notag \\
&\approx \frac{19\sqrt{19}}{18\pi m} 2.37^m, \label{trinomial bound}
\end{align}
where (\ref{Stirling step}) follows from Stirling's approximation for the factorial (and we use the notation $\sim$ to denote asymptotic equivalence, i.e. that two quantities have a ratio that tends to 1 as the parameter size grows). 

Now assume $A_{12} + \frac{3}{4}A_1 \leq \frac{\pi}{2}$, which implies $\pi-A_1-A_{12} \geq A_{12} + \frac{A_1}{2}$. Note also that $\pi-A_1 - A_{12} \geq A_1$ since it is assumed that $\pi-2A_1 - A_{12}\geq 0$. Therefore, we can lower bound (\ref{equivalent probability}) by
\begin{align}\label{corollary 2 bound}
1 - W_2\frac{19\sqrt{19}}{18\pi m} 2.37^m \left(\frac{\pi-A_1-A_{12}}{\pi} \right)^m,
\end{align}
where $W_2$ is the number of terms in the summation in (\ref{equivalent probability}), and is given by
\begin{align}
W_2= \frac{1}{6} \left(\left\lfloor \frac{m}{10} \right\rfloor + 1\right) \left(100 \left\lfloor \frac{m}{10} \right\rfloor^2 + (5 - 30m)\left\lfloor \frac{m}{10} \right\rfloor +3(m^2 + 3m + 2)\right) \sim m^3.
\end{align}
Thus, (\ref{corollary 2 bound}) goes to 1 as $m\rightarrow \infty$ when $2.37\left( \frac{\pi-A_1-A_{12}}{\pi}\right) <1 $, which holds if $A_1+A_{12} > 0.58\pi$.
\end{proof}

\section{Discussion and Conclusion}\label{sec::conclude}
In this work, we have presented a supervised classification algorithm that operates on binary, or one-bit, data. Along with encouraging numerical experiments, we have also included a theoretical analysis for a simple case. We believe our framework and analysis approach is relevant to analyzing similar, multi-\edit{level}-type algorithms. 
Future directions of this work include the use of dithers for more complicated data geometries, \edit{identifying settings where real-valued measurements may be worth the additional complexity, analyzing geometries with non-uniform densities of data,} as well as a generalized theory for high dimensional data belonging to many classes and utilizing multiple \edit{level}s within the algorithm.  \edit{In addition, we believe the framework will extend nicely into other applications such as hierarchical clustering and classification as well as detection problems. In particular, the membership function scores themselves can provide information about the classes and/or data points that can then be utilized for detection, structured classification, false negative rates, and so on.   We believe this framework will naturally extend to these types of settings and provide both simplistic algorithmic approaches as well as the ability for mathematical rigor. }


\acks{
DN acknowledges support from the Alfred P. Sloan Foundation, NSF CAREER DMS $\#1348721$ and NSF BIGDATA DMS $\#1740325$.  RS acknowledges support from the NSF under DMS-1517204. The authors would like to thank the reviewers for their suggestions, questions, and comments which significantly improved the manuscript.}

\newpage
\appendix

\section{Elementary Computations}
%%%comment out starts here
%\subsection{Derivation of \eqref{helloerf} and \eqref{helloerf2}}\label{app:erf}
%The expected values above are related to the moment generating function of squares of uniform random variables. Let $Y= U^2$ where $U\sim U(a,b)$. Then the pdf of Y is given by
%\begin{align}
%f_Y(y) = \begin{cases} 
%      \frac{1}{\sqrt{y}(b-a)} & a^2 \leq y \leq b^2 \\
%      0 & \mbox{otherwise}.
%   \end{cases}
%\end{align}
%Then,
%\begin{align}
%\mathbb{E}\left(e^{\frac{\theta}{c}U^2}\right) &= \mathbb{E}\left(e^{\frac{\theta}{c}Y}\right) \\
%&= \int_{-\infty}^{\infty} e^{\frac{\theta}{c}y} f_Y(y) \,\, dy \\
%&= \frac{1}{b-a} \int_{a^2}^{b^2} \frac{1}{\sqrt{y}}e^{\frac{\theta}{c}y} \,\, dy \\
%&= \frac{2}{b-a} \int_a^b e^{\frac{\theta}{c}x^2} \,\, dx \\
%&= \frac{\sqrt{\pi}(\mbox{erfi}(b\sqrt{\frac{\theta}{c}})-\mbox{erfi}(a\sqrt{\frac{\theta}{c}}))}{\sqrt{\frac{\theta}{c}}(b-a)}, \label{erfi mgf}
%\end{align}
%where we have used the substitution $x=\sqrt{y}$. 
%Similarly,
%\begin{align}
%\mathbb{E}\left(e^{-\frac{\theta}{c}U^2}\right) &= \frac{\sqrt{\pi}(\mbox{erf}(b\sqrt{\frac{\theta}{c}})-\mbox{erf}(a\sqrt{\frac{\theta}{c}}))}{\sqrt{\frac{\theta}{c}}(b-a)}, \label{erf mgf}
%\end{align}
%where $\mbox{erf}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} \,\, dt$ is the error function and $\mbox{erfi}(x) = -i\mbox{erf}(ix) = \frac{2}{\sqrt{\pi}}\int_0^x e^{t^2} \,\, dt$ is the imaginary error function. Then  \eqref{helloerf} and \eqref{helloerf2} hold by observing that $\mbox{erf}(0) = \mbox{erfi}(0) = 0$.

\subsection{Derivation of \eqref{multinomial count}}\label{app:comb}

Suppose we have $M$ objects that must be divided into 5 boxes (for us, the boxes are the 5 different types of hyperplanes). Let $n_i$ denote the number of objects put into box $i$. Recall that in general, $M$ objects can be divided into $k$ boxes $\binom{M+k-1}{k-1}$ ways.

How many arrangements satisfy $n_1 \geq 9(n_2 + n_3)$? To simplify, let $n$ denote the total number of objects in boxes 2 and 3 (that is, $n = n_2 + n_3$). Then, we want to know how many arrangements satisfy $n_1 \geq 9n$? 

If $n=0$, then $n_1 \geq 9n$ is satisfied no matter how many objects are in box 1. So, this reduces to the number of ways to arrange $M$ objects into 3 boxes, which is given by $\binom{M+2}{2}$.

Suppose $n=1$. For $n_1 \geq 9n$ to be true, we must at least reserve 9 objects in box 1. Then $M-10$ objects remain to be placed in 3 boxes, which can be done in $\binom{(M-10)+2}{2}$ ways. But, there are 2 ways for $n=1$, either $n_2=1$ or $n_3 = 1$, so we must multiply this by 2. Thus, $\binom{(M-10)+2}{2} \times 2$ arrangements satisfy  $n_1 \geq 9n$.

Continuing in this way, in general for a given $n$, there are $\binom{M-10n+2}{2} \times (n+1)$ arrangements that satisfy $n_1 \geq 9n$. There are $n+1$ ways to arrange the objects in boxes 2 and 3, and $\binom{M-10n+2}{2}$ ways to arrange the remaining objects after $9n$ have been reserved in box 1. 

Therefore, the total number of arrangements that satisfy $n_1 \geq 9n$ is given by
\begin{align}
\sum_{n=0}^{\lfloor \frac{M}{10} \rfloor} \binom{M-10n+2}{2} \times (n+1).
\end{align}
To see the upper limit of the sum above, note that we must have $M-10n+2 \geq 2$, which means $n \leq \frac{M}{10}$. Since $n$ must be an integer, we take $n \leq \lfloor \frac{M}{10} \rfloor$. After some heavy algebra (i.e. using software!), one can express this sum as:

\begin{align}% \label{multinomial count}
W &= \frac{1}{12} \left(\left\lfloor \frac{M}{10} \right\rfloor + 1\right) \left(\left\lfloor \frac{M}{10} \right\rfloor + 2\right) \left(150 \left\lfloor \frac{M}{10} \right\rfloor^2 - 10(4M + 1)\left\lfloor \frac{M}{10} \right\rfloor +3(M^2 + 3M + 2)\right) \\
&\sim M^4.
\end{align}

\subsection{Derivation of \eqref{facts}}\label{app:facts}
Suppose we want to maximize (over the choices of $a,b,c$) a trinomial $\frac{m!}{a!b!c!}$ subject to $a+b+c=m$ and $a>9b$. Since $m$ is fixed, this is equivalent to choosing $a,b,c$ so as to minimize $a!b!c!$ subject to these constraints. First, fix $c$ and consider optimizing $a$ and $b$ subject to $a+b = m-c =: k$ and $a>9b$ in order to minimize $a!b!$. For convenience, suppose $k$ is a multiple of $10$. We claim the optimal choice is to set $a=9b$ (i.e. $a = \frac{9}{10}k$ and $b=\frac{1}{10}k$). Write $a = 9b + x$ where $x$ must be some non-negative integer in order to satisfy the constraint. We then wish to compare $(9b)!b!$ to $(9b+x)!(b-x)!$, since the sum of $a$ and $b$ must be fixed. One readily observes that:
$$
(9b+x)!(b-x)! = \frac{(9b+x)(9b+x-1)\cdots(9b+1)}{b(b-1)\cdots(b-x+1)}\cdot (9b)!b! \geq \frac{9b\cdot 9b\cdots 9b}{b\cdot b\cdots b}\cdot (9b)!b! = 9^x\cdot (9b)!b!.
$$
Thus, we only increase the product $a!b!$ when $a>9b$, so the optimal choice is when $a=9b$. This holds for any choice of $c$.  A similar argument shows that optimizing $b$ and $c$ subject to $9b + b + c = m$ to minimize $(9b)!b!c!$ results in the choice that $c=9b$.  Therefore, one desires that $a=c=9b$ and $a+b+c=m$, which means $a=c=\frac{9}{19}m$ and $b=\frac{1}{19}m$.


\bibliographystyle{natbib}
\bibliography{../bib}

\end{document}