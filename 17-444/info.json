{
    "abstract": "With ever growing data volume and model size, an error-tolerant, communication efficient, yet versatile distributed algorithm has become vital for the success of many large-scale machine learning applications. In this work we propose \\mspg, an implementation of the flexible proximal gradient algorithm in model parallel systems equipped with the partially asynchronous communication protocol. The worker machines communicate asynchronously with a controlled staleness bound $s$ and operate at different frequencies. We characterize various convergence properties of \\mspg: 1) Under a general non-smooth and non-convex setting, we prove that every limit point of the sequence generated by \\mspg is a critical point of the objective function; 2) \\YI{Under an error bound condition of convex objective functions}, we prove that the optimality gap decays linearly for every $s$ steps; 3) \\YI{Under the Kurdyka-{\\L}ojasiewicz inequality and a sufficient decrease assumption}, we prove that the sequences generated by \\mspg converge to the same critical point, provided that a proximal Lipschitz condition is satisfied.",
    "authors": [
        "Yi Zhou",
        "Yingbin Liang",
        "Yaoliang Yu",
        "Wei Dai",
        "Eric P. Xing"
    ],
    "id": "17-444",
    "issue": 19,
    "pages": [
        1,
        32
    ],
    "title": "Distributed Proximal Gradient Algorithm for Partially Asynchronous Computer Clusters ",
    "volume": 19,
    "year": 2018
}