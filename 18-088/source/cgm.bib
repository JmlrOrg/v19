@article{sdpnalplus,
abstract = {In this paper, we present a majorized semismooth Newton-CG augmented Lagrangian method, called SDPNAL{\$}+{\$}, for semidefinite programming (SDP) with partial or full nonnegative constraints on the matrix variable. SDPNAL{\$}+{\$} is a much enhanced version of SDPNAL introduced by Zhao, Sun and Toh [SIAM Journal on Optimization, 20 (2010), pp.{\~{}}1737--1765] for solving generic SDPs. SDPNAL works very efficiently for nondegenerate SDPs but may encounter numerical difficulty for degenerate ones. Here we tackle this numerical difficulty by employing a majorized semismooth Newton-CG augmented Lagrangian method coupled with a convergent 3-block alternating direction method of multipliers introduced recently by Sun, Toh and Yang [arXiv preprint arXiv:1404.5378, (2014)]. Numerical results for various large scale SDPs with or without nonnegative constraints show that the proposed method is not only fast but also robust in obtaining accurate solutions. It outperforms, by a significant margin, two other competitive publicly available first order methods based codes: (1) an alternating direction method of multipliers based solver called SDPAD by Wen, Goldfarb and Yin [Mathematical Programming Computation, 2 (2010), pp.{\~{}}203--230] and (2) a two-easy-block-decomposition hybrid proximal extragradient method called 2EBD-HPE by Monteiro, Ortiz and Svaiter [Mathematical Programming Computation, (2013), pp.{\~{}}1--48]. In contrast to these two codes, we are able to solve all the 95 difficult SDP problems arising from the relaxations of quadratic assignment problems tested in SDPNAL to an accuracy of {\$}10{\^{}}{\{}-6{\}}{\$} efficiently, while SDPAD and 2EBD-HPE successfully solve 30 and 16 problems, respectively.},
archivePrefix = {arXiv},
arxivId = {1406.0942},
author = {Yang, L. and Sun, D. and Toh, K. C.},
eprint = {1406.0942},
journal = {Mathematical Programming Computation},
keywords = {Augmented Lagrangian,Degeneracy,Semidefinite programming,Semismooth Newton-CG method},
number = {3},
pages = {331--366},
title = {{SDPNAL+: A majorized semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints}},
volume = {7},
year = {2015}
}
@article{SCSsolver,
abstract = {We introduce a first order method for solving very large convex cone programs. The method uses an operator splitting method, the alternating directions method of multipliers, to solve the homogeneous self-dual embedding, an equivalent feasibility problem involving finding a nonzero point in the intersection of a subspace and a cone. This approach has several favorable properties. Compared to interior-point methods, first-order methods scale to very large problems, at the cost of requiring more time to reach very high accuracy. Compared to other first-order methods for cone programs, our approach finds both primal and dual solutions when available or a certificate of infeasibility or unboundedness otherwise, is parameter-free, and the per-iteration cost of the method is the same as applying a splitting method to the primal or dual alone. We discuss efficient implementation of the method in detail, including direct and indirect methods for computing projection onto the subspace, scaling the original problem data, and stopping criteria. We describe an open-source implementation, which handles the usual (symmetric) non-negative, second-order, and semidefinite cones as well as the (non-self-dual) exponential and power cones and their duals. We report numerical results that show speedups over interior-point cone solvers for large problems, and scaling to very large general cone programs.},
archivePrefix = {arXiv},
arxivId = {1312.3039},
author = {O'Donoghue, B. and Chu, E. and Parikh, N. and Boyd, S.},
eprint = {1312.3039},
journal = {Journal of Optimization Theory and Applications},
keywords = {Cone programming,First-order methods,Operator splitting,Optimization},
number = {3},
pages = {1042--1068},
title = {{Conic optimization via operator splitting and homogeneous self-dual embedding}},
volume = {169},
year = {2016}
}
@article{Jaggi2013,
abstract = {We provide stronger and more general primal-dual convergence results for Frank- Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimiza- tion, enabled by a simple framework of du- ality gap certificates. Our analysis also holds if the linear subproblems are only solved ap- proximately (as well as if the gradients are inexact), and is proven to be worst-case opti- mal in the sparsity of the obtained solutions. On the application side, this allows us to unify a large variety of existing sparse greedy methods, in particular for optimization over convex hulls of an atomic set, even if those sets can only be approximated, including sparse (or structured sparse) vectors or ma- trices, low-rank matrices, permutation matri- ces, or max-norm bounded matrices. We present a new general framework for con- vex optimization over matrix factorizations, where every Frank-Wolfe iteration will con- sist of a low-rank update, and discuss the broad application areas of this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.1942v1},
author = {Jaggi, M.},
eprint = {arXiv:1301.1942v1},
journal = {ICML},
keywords = {{\{}{\#}{\}}4April4Mendeley-Unclassified},
title = {{Revisiting Frank-Wolfe: Projection-free sparse convex optimization}},
year = {2013}
}
@article{Goldstein2009,
abstract = {The class of L1-regularized optimization problems has received much attention recently because of the introduction of “compressed sensing,” which allows images and signals to be reconstructed from small amounts of data. Despite this recent attention, many L1-regularized problems still remain difficult to solve, or require techniques that are very problem-specific. In this paper, we show that Bregman iteration can be used to solve a wide variety of constrained optimization problems. Using this technique, we propose a “split Bregman” method, which can solve a very broad class of L1-regularized problems. We apply this technique to the Rudin–Osher–Fatemi functional for image denoising and to a compressed sensing problem that arises in magnetic resonance imaging.},
archivePrefix = {arXiv},
arxivId = {arXiv:quant-ph/0611061v2},
author = {Goldstein, T. and Osher, S.},
eprint = {0611061v2},
journal = {SIAM Journal on Imaging Sciences},
number = {2},
pages = {323--343},
pmid = {23383727},
primaryClass = {arXiv:quant-ph},
title = {{The split Bregman method for L1-regularized problems}},
volume = {2},
year = {2009}
}
@inproceedings{Hazan2008sdp,
abstract = {We propose an algorithm for approximately maximizing a concave function over the bounded semi-definite cone, which produces sparse solutions. Sparsity for SDP corresponds to low rank matrices, and is a important property for both computational as well as learning theoretic reasons. As an application, building on Aaronson's recent work, we derive a linear time algorithm for Quantum State Tomography.},
author = {Hazan, E.},
booktitle = {LATIN},
title = {{Sparse approximate solutions to semidefinite programs}},
year = {2008}
}
@article{Clarkson2010fw,
abstract = {The problem of maximizing a concave function f ( x ) in a simplex S can be solved approximately by a simple greedy algorithm. For given k , the algorithm can find a point x ( k ) on a k -dimensional face of S , such that f ( x ( k )) $\backslash${\&}ge; f ( x *) - O (1/ k ). Here f ( x *) is the maximum value of f in S. This algorithm and analysis were known before, and related to problems of statistics and machine learning, such as boosting, regression, and density mixture estimation. In other work, coming from computational geometry, the existence of $\backslash${\&}epsilon;- coresets was shown for the minimum enclosing ball problem, by means of a simple greedy algorithm. Similar greedy algorithms, that are special cases of the Frank-Wolfe algorithm, were described for other enclosure problems. Here these results are tied together, stronger convergence results are reviewed, and several coreset bounds are generalized or strengthened.},
author = {Clarkson, K.},
journal = {ACM Transactions on Algorithms},
number = {4},
pages = {1--30},
title = {{Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm}},
volume = {6},
year = {2010}
}
@article{FrankWolfe,
author = {Frank, M. and Wolfe, P.},
file = {:Users/mtepper/Library/Application Support/Mendeley Desktop/Downloaded/Frank, Wolfe - 1956 - An algorithm for quadratic programming.pdf:pdf},
journal = {Naval Research Logistics Quarterly},
number = {1-2},
pages = {95--110},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{An algorithm for quadratic programming}},
volume = {3},
year = {1956}
}
@book{NumericalOptimization,
abstract = {Despite application of cryogen spray (CS) precooling, customary treatment of port wine stain (PWS) birthmarks with a single laser pulse does not result in complete lesion blanching for a majority of patients. One obvious reason is nonselective absorption by epidermal melanin, which limits the maximal safe radiant exposure. Another possible reason for treatment failure is screening of laser light within large PWS vessels, which prevents uniform heating of the entire vessel lumen. Our aim is to identify the parameters of sequential CS cooling and laser irradiation that will allow optimal photocoagulation of various PWS blood vessels with minimal risk of epidermal thermal damage.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Nocedal, J. and Wright, S. J.},
booktitle = {Springer},
eprint = {NIHMS150003},
number = {2},
pages = {164--75},
pmid = {21384397},
title = {{Numerical Optimization}},
volume = {43},
year = {1999}
}
