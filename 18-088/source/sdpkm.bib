@article{Zhang2012,
abstract = {Manifold learning is a hot research topic in the field of computer science. A crucial issue with current manifold learning methods is that they lack a natural quantitative measure to assess the quality of learned embeddings, which greatly limits their applications to real-world problems. In this paper, a new embedding quality assessment method for manifold learning, named as normalization independent embedding quality assessment (NIEQA) is proposed. Compared with current assessment methods which are limited to isometric embeddings, the NIEQA method has a much larger application range due to two features. First, it is based on a new measure which can effectively evaluate how well local neighborhood geometry is preserved under normalization, hence it can be applied to both isometric and normalized embeddings. Second, it can provide both local and global evaluations to output an overall assessment. Therefore, NIEQA can serve as a natural tool in model selection and evaluation tasks for manifold learning. Experimental results on benchmark data sets validate the effectiveness of the proposed method. {\textcopyright} 2012 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {1108.1636},
author = {Zhang, P. and Ren, Y. and Zhang, B.},
eprint = {1108.1636},
journal = {Neurocomputing},
keywords = {Data analysis,Manifold learning,Nonlinear dimensionality reduction},
pages = {251--266},
title = {{A new embedding quality assessment method for manifold learning}},
volume = {97},
year = {2012}
}
@article{Pehlevan2018similarity,
author = {Pehlevan, C. and Sengupta, A. and Chklovskii, D.},
journal = {Neural Computation},
number = {1},
pages = {84--124},
title = {{Why do similarity matching objectives lead to Hebbian/anti-Hebbian networks?}},
volume = {30},
year = {2018}
}
@article{Pehlevan2015subspace,
abstract = {Neural network models of early sensory processing typically reduce the dimensionality of streaming input data. Such networks learn the principal subspace, in the sense of principal component analysis, by adjusting synaptic weights according to activity-dependent learning rules. When derived from a principled cost function, these rules are nonlocal and hence biologically implausible. At the same time, biologically plausible local rules have been postulated rather than derived from a principled cost function. Here, to bridge this gap, we derive a biologically plausible network for subspace learning on streaming data by minimizing a principled cost function. In a departure from previous work, where cost was quantified by the representation, or reconstruction, error, we adopt a multidimensional scaling cost function for streaming data. The resulting algorithm relies only on biologically plausible Hebbian and anti-Hebbian local learning rules. In a stochastic setting, synaptic weights converge to a stationary state, which projects the input data onto the principal subspace. If the data are generated by a nonstationary distribution, the network can track the principal subspace. Thus, our result makes a step toward an algorithmic theory of neural computation.},
annote = {PMID: 25973548},
author = {Pehlevan, C. and Hu, T. and Chklovskii, D.},
journal = {Neural Computation},
number = {7},
pages = {1461--1495},
title = {{A Hebbian/Anti-Hebbian neural network for linear subspace learning: A derivation from multidimensional scaling of streaming data}},
volume = {27},
year = {2015}
}
@incollection{Bachoc2012,
abstract = {In the last years many results in the area of semidefinite programming were obtained for invariant (finite dimensional, or infinite dimensional) semidefinite programs - SDPs which have symmetry. This was done for a variety of problems and applications. The purpose of this handbook chapter is to give the reader the necessary background for dealing with semidefinite programs which have symmetry. Here the basic theory is given and it is illustrated in applications from coding theory, combinatorics, geometry, and polynomial optimization.},
archivePrefix = {arXiv},
arxivId = {1007.2905},
author = {Bachoc, C. and Gijswijt, D. C. and Schrijver, A. and Vallentin, F.},
booktitle = {Handbook on semidefinite, conic and polynomial optimization},
eprint = {1007.2905},
pages = {219--269},
publisher = {Springer},
title = {{Invariant semidefinite programs}},
year = {2012}
}
@inproceedings{Pehlevan2017olfaction,
abstract = {— A key step in insect olfaction is the transformation of a dense representation of odors in a small population of neurons -projection neurons (PNs) of the antennal lobe -into a sparse representation in a much larger population of neurons -Kenyon cells (KCs) of the mushroom body. What computational purpose does this transformation serve? We propose that the PN-KC network implements an online clustering algorithm which we derive from the k-means cost function. The vector of PN-KC synaptic weights converging onto a given KC represents the corresponding cluster centroid. KC activities represent attri-bution indices, i.e. the degree to which a given odor presentation is attributed to each cluster. Remarkably, such clustering view of the PN-KC circuit naturally accounts for several of its salient features. First, attribution indices are nonnegative thus rationalizing rectification in KCs. Second, the constraint on the total sum of attribution indices for each presentation is enforced by a Lagrange multiplier identified with the activity of a single inhibitory interneuron reciprocally connected with KCs. Third, the soft-clustering version of our algorithm reproduces observed sparsity and overcompleteness of the KC representation which may optimize supervised classification downstream.},
author = {Pehlevan, C. and Genkin, A. and Chklovskii, D.},
booktitle = {Asilomar},
title = {{A clustering neural network model of insect olfaction}},
year = {2017}
}
@article{Cristianini2002,
abstract = {Editor: Kernel based methods are increasingly being used for data modeling because of their conceptual simplicity and outstanding performance on many tasks. However, the kernel function is often chosen using trial-and-error heuristics. In this paper we address the problem of measuring the degree of agreement between a kernel and a learning task. A quantitative measure of agreement is important from both a theoretical and practical point of view. We propose a quantity to capture this notion, which we call Alignment. We study its theoretical properties, and derive a series of simple algorithms for adapting a kernel to the labels and vice versa. This produces a series of novel methods for clustering and transduction, kernel combination and kernel selection. The algorithms are tested on two publicly available datasets and are shown to exhibit good performance.},
author = {Cristianini, N. and Kandola, J. and Elisseeff, A. and Shawe-Taylor, J.},
journal = {NIPS},
keywords = {alignment,eigenvalues,eigenvectors,kernels,transduction},
title = {{On kernel-target alignment}},
year = {2002}
}
@article{Lanckriet2004,
author = {Lanckriet, G. and Cristianini, N. and Bartlett, P. and Ghaoui, L. and Jordan, M.},
journal = {Journal of Machine Learning Research},
number = {Jan},
pages = {27--72},
title = {{Learning the kernel matrix with semidefinite programming}},
volume = {5},
year = {2004}
}
@article{Cortes2012_centeredalignment,
author = {Cortes, C. and Mohri, M. and Rostamizadeh, A.},
journal = {Journal of Machine Learning Research},
number = {Mar},
pages = {795--828},
title = {{Algorithms for learning kernels based on centered alignment}},
volume = {13},
year = {2012}
}
@techreport{Javanmard2016,
abstract = {Statistical inference problems arising within signal processing, data mining, and machine learning naturally give rise to hard combinatorial optimization problems. These problems become intractable when the dimensionality of the data is large, as is often the case for modern datasets. A popular idea is to construct convex relaxations of these combinatorial problems, which can be solved efficiently for large-scale datasets. Semidefinite programming (SDP) relaxations are among the most powerful methods in this family and are surprisingly well suited for a broad range of problems where data take the form of matrices or graphs. It has been observed several times that when the statistical noise is small enough, SDP relaxations correctly detect the underlying combinatorial structures. In this paper we develop asymptotic predictions for several detection thresholds, as well as for the estimation error above these thresholds. We study some classical SDP relaxations for statistical problems motivated by graph synchronization and community detection in networks. We map these optimization problems to statistical mechanics models with vector spins and use nonrigorous techniques from statistical mechanics to characterize the corresponding phase transitions. Our results clarify the effectiveness of SDP relaxations in solving high-dimensional statistical problems.},
archivePrefix = {arXiv},
arxivId = {1511.08769},
author = {Javanmard, A. and Montanari, A. and Ricci-Tersenghi, F.},
eprint = {1511.08769},
institution = {arXiv:1511.08769},
pmid = {27001856},
title = {{Phase transitions in semidefinite relaxations}},
year = {2015}
}
@techreport{Amini2014,
abstract = {The stochastic block model (SBM) is a popular tool for community detection in networks, but fitting it by maximum likelihood (MLE) involves an infeasible optimization problem. We propose a new semi-definite programming (SDP) solution to the problem of fitting the SBM, derived as a relaxation of the MLE. Our relaxation is tighter than other recently proposed SDP relaxations, and thus previously established theoretical guarantees carry over, but empirically our approach gives substantially better results. We put all these SDPs into a unified framework and make a connection to the well-known problem of sparse PCA. Our relaxation focuses on the balanced case of a network with equally sized communities, and as we show, that makes it the ideal tool for fitting network histograms, a concept gaining popularity in the graphon estimation literature.},
archivePrefix = {arXiv},
arxivId = {1406.5647},
author = {Amini, A. and Levina, E.},
eprint = {1406.5647},
institution = {arXiv:1406.5647},
pages = {1--29},
title = {{On semidefinite relaxations for the block model}},
year = {2014}
}
@inproceedings{Yu2012regularizers,
abstract = {We demonstrate that almost all non- parametric dimensionality reduction meth- ods can be expressed by a simple procedure: regularized loss minimization plus singular value truncation. By distinguishing the role of the loss and regularizer in such a pro- cess, we recover a factored perspective that reveals some gaps in the current literature. Beyond identifying a useful new loss for man- ifold unfolding, a key contribution is to de- rive new convex regularizers that combine distance maximization with rank reduction. These regularizers can be applied to any loss.},
author = {Yu, Y. and Neufeld, J. and Kiros, R. and Zhang, X. and Schuurmans, D.},
booktitle = {ICML},
title = {{Regularizers versus losses for nonlinear dimensionality reduction}},
year = {2012}
}
@article{Cho2009,
author = {Cho, Y. and Saul, L.},
journal = {NIPS},
pages = {342--350},
title = {{Kernel methods for deep learning}},
year = {2009}
}
@techreport{So2013,
abstract = {The problem of finding completely positive matrices with equal cp-rank and rank is considered. We give some easy-to-check sufficient conditions on the entries of a doubly nonnegative matrix for it to be completely positive with equal cp-rank and rank. An algorithm is also presented to show its efficiency.},
archivePrefix = {arXiv},
arxivId = {1308.3193},
author = {So, W. and Xu, C.},
eprint = {1308.3193},
institution = {arXiv:1308.3193},
title = {{When does CP-rank equal rank?}},
year = {2013}
}
@article{ODonoghue2016,
author = {O'Donoghue, B. and Chu, E. and Parikh, N. and Boyd, S.},
journal = {Journal of Optimization Theory and Applications},
keywords = {ODonoghue2016},
number = {3},
pages = {1042--1068},
title = {{Conic optimization via operator splitting and homogeneous self-dual embedding}},
volume = {169},
year = {2016}
}
@inproceedings{Hadsell2006,
author = {Hadsell, R. and Chopra, S. and LeCun, Y.},
booktitle = {CVPR},
title = {{Dimensionality reduction by learning an invariant mapping}},
year = {2006}
}
@techreport{Mixon2016,
abstract = {We introduce a model-free relax-and-round algorithm for k-means clustering based on a semidefinite relaxation due to Peng and Wei. The algorithm interprets the SDP output as a denoised version of the original data and then rounds this output to a hard clustering. We provide a generic method for proving performance guarantees for this algorithm, and we analyze the algorithm in the context of subgaussian mixture models. We also study the fundamental limits of estimating Gaussian centers by k-means clustering in order to compare our approximation guarantee to the theoretically optimal k-means clustering solution.},
archivePrefix = {arXiv},
arxivId = {1602.06612},
author = {Mixon, D. and Villar, S. and Ward, R.},
eprint = {1602.06612},
institution = {arXiv:1602.06612},
title = {{Clustering subgaussian mixtures by semidefinite programming}},
year = {2016}
}
@techreport{Iguchi2015,
abstract = {Recently, Awasthi et al. introduced an SDP relaxation of the {\$}k{\$}-means problem in {\$}\backslashmathbb R{\^{}}m{\$}. In this work, we consider a random model for the data points in which {\$}k{\$} balls of unit radius are deterministically distributed throughout {\$}\backslashmathbb R{\^{}}m{\$}, and then in each ball, {\$}n{\$} points are drawn according to a common rotationally invariant probability distribution. For any fixed ball configuration and probability distribution, we prove that the SDP relaxation of the {\$}k{\$}-means problem exactly recovers these planted clusters with probability {\$}1-e{\^{}}{\{}-\backslashOmega(n){\}}{\$} provided the distance between any two of the ball centers is {\$}{\textgreater}2+\backslashepsilon{\$}, where {\$}\backslashepsilon{\$} is an explicit function of the configuration of the ball centers, and can be arbitrarily small when {\$}m{\$} is large.},
archivePrefix = {arXiv},
arxivId = {1505.04778},
author = {Iguchi, T. and Mixon, D. and Peterson, J. and Villar, S.},
eprint = {1505.04778},
institution = {arXiv:1505.04778},
title = {{On the tightness of an SDP relaxation of k-means}},
year = {2015}
}
@article{Roweis2000,
author = {Roweis, S. and Saul, L.},
journal = {Science},
number = {5500},
title = {{Nonlinear dimensionality reduction by locally linear embedding}},
volume = {290},
year = {2000}
}
@article{Tenenbaum2000,
author = {Tenenbaum, J. and Silva, V. and Langford, J.},
journal = {Science},
number = {5500},
title = {{A global geometric framework for nonlinear dimensionality reduction}},
volume = {290},
year = {2000}
}
@misc{Lichman:2013,
author = {Lichman, M},
institution = {University of California, Irvine, School of Information and Computer Sciences},
title = {{{\{}UCI{\}} machine learning repository}},
year = {2013}
}
@article{Byrd1995,
abstract = {An algorithm for solving large nonlinear optimization problems with simple bounds is described. It is based on the gradient projection method and uses a limited memory BFGS matrix to approximate the Hessian of the objective function. It is shown how to take advantage of the form of the limited memory approximation to implement the algorithm efficiently. The results of numerical tests on a set of large problems are reported.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Byrd, R. and Lu, P. and Nocedal, J. and Zhu, C.},
eprint = {arXiv:1011.1669v3},
journal = {SIAM Journal on Scientific Computing},
keywords = {49,65,bound constrained optimization,large-scale optimization,limited memory method,nonlinear optimization,quasi-Newton method},
number = {5},
pages = {1190--1208},
pmid = {15003161},
title = {{A limited memory algorithm for bound constrained optimization}},
volume = {16},
year = {1995}
}
@article{Maxfield1962,
author = {Maxfield, J. and Minc, H.},
journal = {Proceedings of the Edinburgh Mathematical Society (Series 2)},
number = {02},
pages = {125--129},
title = {{On the matrix equation X' X= A}},
volume = {13},
year = {1962}
}
@inproceedings{boumal2016non,
author = {Boumal, N. and Voroninski, V. and Bandeira, A.},
booktitle = {NIPS},
title = {{The non-convex Burer-Monteiro approach works on smooth semidefinite programs}},
year = {2016}
}
@inproceedings{Kulis2007,
author = {Kulis, B. and Surendran, A. and Platt, J.},
booktitle = {AISTATS},
title = {{Fast low-rank semidefinite programming for embedding and clustering}},
year = {2007}
}
@inproceedings{Hou2015,
abstract = {Clustering is one of the most fundamental tasks in data mining. To analyze complex real-world data emerging in many data-centric applications, the problem of non-exhaustive, overlapping clustering has been studied where the goal is to find overlapping clusters and also detect outliers simultaneously. We propose a novel convex semidefinite program (SDP) as a relaxation of the non-exhaustive, overlapping clustering problem. Although the SDP formulation enjoys attractive theoretical properties with respect to global optimization, it is computationally intractable for large problem sizes. As an alternative, we optimize a low-rank factorization of the solution. The resulting problem is nonconvex, but has a smaller number of solution variables. We construct an optimization solver using an augmented Lagrangian methodology that enables us to deal with problems with tens of thousands of data points. The new solver provides more accurate and reliable answers than other approaches. By exploiting the connection between graph clustering objective functions and a kernel k-means objective, our new low-rank solver can also compute overlapping communities of social networks with state-of-the-art accuracy.},
author = {Hou, Y. and Whang, J. and Gleich, D. and Dhillon, I.},
booktitle = {KDD},
keywords = {community detection,overlapping clustering,semidefi-},
title = {{Non-exhaustive, overlapping clustering via low-rank semidefinite programming categories and subject descriptors}},
year = {2015}
}
@article{Burer2003,
author = {Burer, S. and Monteiro, R.},
journal = {Mathematical Programming},
number = {2},
pages = {329--357},
title = {{A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization}},
volume = {95},
year = {2003}
}
@article{Burer2009,
author = {Burer, S. and Anstreicher, K. and D{\"{u}}r, M.},
journal = {Linear Algebra and its Applications},
number = {9},
pages = {1539--1552},
title = {{The difference between {\$}5 \backslashtimes 5{\$} doubly nonnegative and completely positive matrices}},
volume = {431},
year = {2009}
}
@article{Kaykobad1987,
author = {Kaykobad, M.},
journal = {Linear Algebra and its Applications},
pages = {27--33},
title = {{On nonnegative factorization of matrices}},
volume = {96},
year = {1987}
}
@inproceedings{Weiss2012,
abstract = {With the growing availability of very large image databases, there has been a surge of interest in methods based on “semantic hashing”, i.e. compact binary codes of data-points so that the Hamming distance between codewords correlates with similarity. In reviewing and comparing existing methods, we show that their relative performance can change drastically depending on the definition of ground-truth neighbors. Motivated by this finding, we propose a new formulation for learning binary codes which seeks to reconstruct the affinity between datapoints, rather than their distances. We show that this criterion is intractable to solve exactly, but a spectral relaxation gives an algorithm where the bits correspond to thresholded eigenvectors of the affinity matrix, and as the number of datapoints goes to infinity these eigenvectors converge to eigenfunctions of Laplace-Beltrami operators, similar to the recently proposed Spectral Hashing (SH) method. Unlike SH whose performance may degrade as the number of bits increases, the optimal code using our formulation is guaranteed to faithfully reproduce the affinities as the number of bits increases. We show that the number of eigenfunctions needed may increase exponentially with dimension, but introduce a “kernel trick” to allow us to compute with an exponentially large number of bits but using only memory and computation that grows linearly with dimension. Experiments shows that MDSH outperforms the state-of-the art, especially in the challenging regime of small distance thresholds.},
author = {Weiss, Y. and Fergus, R. and Torralba, A.},
booktitle = {ECCV},
title = {{Multidimensional spectral hashing}},
year = {2012}
}
@article{Weiss2008,
abstract = {Semantic hashing[1] seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper, we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresholded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds, we show how to efficiently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outperform the state-of-the art.},
author = {Weiss, Y. and Torralba, A. and Fergus, R.},
journal = {NIPS},
title = {{Spectral hashing}},
year = {2008}
}
@article{Vinh2010,
author = {Vinh, N. and Epps, J. and Bailey, J.},
journal = {The Journal of Machine Learning Research},
pages = {2837--2854},
title = {{Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance}},
volume = {11},
year = {2010}
}
@article{Belkin2003,
author = {Belkin, M. and Niyogi, P.},
journal = {Neural Computation},
number = {6},
pages = {1373--1396},
title = {{Laplacian eigenmaps for dimensionality reduction and data representation}},
volume = {15},
year = {2003}
}
@article{Lloyd1982,
author = {Lloyd, S.},
journal = {IEEE Transactions on Information Theory},
number = {2},
pages = {129--137},
title = {{Least squares quantization in PCM}},
volume = {28},
year = {1982}
}
@article{Jain50years,
abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering. ?? 2009 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Jain, A.},
eprint = {0402594v3},
journal = {Pattern Recognition Letters},
keywords = {Data clustering,Historical developments,King-Sun Fu prize,Perspectives on clustering,User's dilemma},
number = {8},
pages = {651--666},
primaryClass = {arXiv:cond-mat},
title = {{Data clustering: 50 years beyond K-means}},
volume = {31},
year = {2010}
}
@article{Weinberger2006,
author = {Weinberger, K. and Saul, L.},
journal = {AAAI},
title = {{An introduction to nonlinear dimensionality reduction by maximum variance unfolding}},
year = {2006}
}
@article{Peng2007_sdk-kmeans,
abstract = {One of the fundamental clustering problems is to assign n points into k clusters based on minimal sum‐of‐squared distances (MSSC), which is known to be NP‐hard. In this paper, by using matrix arguments, we first model MSSC as a so‐called 0‐1 semidefinite programming (SDP) problem. We show that our 0‐1 SDP model provides a unified framework for several clustering approaches such as normalized k‐cut and spectral clustering. Moreover, the 0‐1 SDP model allows us to solve the underlying problem approximately via the linear programming and SDP relaxations. Second, we consider the issue of how to extract a feasible solution of the original 0‐1 SDP model from the optimal solution of the relaxed SDP problem. By using principal component analysis, we develop a rounding procedure to construct a feasible partitioning from a solution of the relaxed problem. In our rounding procedure, we need to solve a K‐means clustering problem in {\$}\backslashRe{\^{}}{\{}k-1{\}}{\$}, which can be done in {\$}O(n{\^{}}{\{}k{\^{}}2-2k+2{\}}){\$} time. In case of biclustering, the...},
author = {Peng, J. and Wei, Y.},
file = {:Users/mtepper/Library/Application Support/Mendeley Desktop/Downloaded/Peng, Wei - 2007 - Approximating K‐means‐type Clustering via Semidefinite Programming.pdf:pdf},
journal = {SIAM Journal on Optimization},
keywords = {0‐1 SDP,68T10,90C22,K‐means clustering,approximation,computational complexity,principal component analysis,relaxation},
month = {jan},
number = {1},
pages = {186--205},
title = {{Approximating K-means-type clustering via semidefinite programming}},
volume = {18},
year = {2007}
}
@inproceedings{Awasthi2015,
author = {Awasthi, P. and Bandeira, A. and Charikar, M. and Krishnaswamy, R. and Villar, S. and Ward, R.},
booktitle = {ITCS},
keywords = {clustering,convex optimization,exact recovery,kmeans,kmedians},
title = {{Relax, no need to round: Integrality of clustering formulations}},
year = {2015}
}
@article{Fevotte2011,
abstract = {This letter describes algorithms for nonnegative matrix factorization (NMF) with the $\beta$-divergence ($\beta$-NMF). The $\beta$-divergence is a family of cost functions parameterized by a single shape parameter $\beta$ that takes the Euclidean distance, the Kullback-Leibler divergence, and the Itakura-Saito divergence as special cases ($\beta$ = 2, 1, 0 respectively). The proposed algorithms are based on a surrogate auxiliary function (a local majorization of the criterion function). We first describe a majorization-minimization algorithm that leads to multiplicative updates, which differ from standard heuristic multiplicative updates by a $\beta$-dependent power exponent. The monotonicity of the heuristic algorithm can, however, be proven for $\beta$ ∈ (0, 1) using the proposed auxiliary function. Then we introduce the concept of the majorization-equalization (ME) algorithm, which produces updates that move along constant level sets of the auxiliary function and lead to larger steps than MM. Simulations on synthetic and real data illustrate t...},
archivePrefix = {arXiv},
arxivId = {arXiv:1010.1763v3},
author = {F{\'{e}}votte, C. and Idier, J.},
eprint = {arXiv:1010.1763v3},
journal = {Neural Computation},
number = {9},
pages = {2421--2456},
pmid = {21671793},
title = {{Algorithms for nonnegative matrix factorization with the $\beta$-divergence}},
volume = {23},
year = {2011}
}
@article{tepper2016compressed,
author = {Tepper, M. and Sapiro, G.},
journal = {IEEE Transactions on Signal Processing},
number = {9},
pages = {2269--2283},
title = {{Compressed nonnegative matrix factorization is fast and accurate}},
volume = {64},
year = {2016}
}
@article{xu2012nmf,
abstract = {This paper introduces an algorithm for the nonnegative matrix factorization-and-completion problem, which aims to find nonnegative low-rank matrices X and Y so that the product {\{}XY{\}} approximates a nonnegative data matrix M whose elements are partially known (to a certain accuracy). This problem aggregates two existing problems: (i) nonnegative matrix factorization where all entries of M are given, and (ii) low-rank matrix completion where nonnegativity is not required. By taking the advantages of both nonnegativity and low-rankness, one can generally obtain superior results than those of just using one of the two properties. We propose to solve the non-convex constrained least-squares problem using an algorithm based on the classical alternating direction augmented Lagrangian method. Preliminary convergence properties of the algorithm and numerical simulation results are presented. Compared to a recent algorithm for nonnegative matrix factorization, the proposed algorithm produces factorizations of similar quality using only about half of the matrix entries. On tasks of recovering incomplete grayscale and hyperspectral images, the proposed algorithm yields overall better qualities than those produced by two recent matrix-completion algorithms that do not exploit nonnegativity.},
author = {Xu, Y. and Yin, W. and Wen, Z. and Zhang, Y.},
file = {:Users/mtepper/Library/Application Support/Mendeley Desktop/Downloaded/Xu et al. - 2012 - An alternating direction algorithm for matrix completion with nonnegative factors.pdf:pdf},
journal = {Front. Math. China},
keywords = {nmf},
mendeley-tags = {nmf},
number = {2},
pages = {365--384},
title = {{An alternating direction algorithm for matrix completion with nonnegative factors}},
volume = {7},
year = {2012}
}
@article{Tepper2014consensus,
author = {Tepper, M. and Sapiro, G.},
journal = {SIAM Journal on Imaging Sciences},
number = {4},
pages = {2488--2525},
title = {{A bi-clustering framework for consensus problems}},
volume = {7},
year = {2014}
}
@inproceedings{ng01,
abstract = {Despite many empirical successes of spectral clustering methods|algorithms that cluster points using eigenvectors of matrices derivedfrom the data|there are several unresolved issues. First,there are a wide variety of algorithms that use the eigenvectorsin slightly dierent ways. Second, many of these algorithms haveno proof that they will actually compute a reasonable clustering.},
author = {Ng, A. and Jordan, M. and Weiss, Y.},
booktitle = {NIPS},
keywords = {clustering},
title = {{On spectral clustering: Analysis and an algorithm}},
year = {2001}
}
