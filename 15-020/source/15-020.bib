

@TECHREPORT{Shewchuk94anintroduction,
    author = {Shewchuk, J.R.},
    title = {An introduction to the conjugate gradient method without the agonizing pain},
    institution = {},
    year = {1994}
}


@inproceedings{Zaharia2010,
 author = {Zaharia, M. and Chowdhury, M. and Franklin, M.J. and Shenker, S. and Stoica, I.},
 title = {Spark: Cluster Computing with Working Sets},
 booktitle = {USENIX Conference},
 year = {2010},
} 

@article{Dean2008,
 author = {Dean, J. and Ghemawat, S.},
 title = {MapReduce: Simplified Data Processing on Large Clusters},
 journal = {Commun. ACM},
 year = {2008},
 pages = {107--113},
} 

@inproceedings{weimer2015,
  title={REEF: Retainable Evaluator Execution Framework},
  author={Weimer, M. and Chen, Y. and Chun, B.G. and Condie, T. and Curino, C. and Douglas, C. and Lee, Y. and Majestro, T. and Malkhi, D. and Matusevych, S. and others},
  booktitle={ACM SIGMOD},
  pages={1343--1355},
  year={2015},
}



@article{sonnenburg2010,
 author = {Sonnenburg, S. T. and Franc, V.},
 title = {{COFFIN:} A computational framework for linear {SVM}s},
 journal = {ICML},
 year = {2010},
}

@article{alekh2013,
  author = {Agarwal, A. and Chapelle, O and Dudik, M. and Langford, J.},
  journal = {arXiv:1110.4198},
  title = {A Reliable Effective Terascale Linear System},
  year = {2013},
}

@article{hsieh14,
 author = {Hsieh, C.J. and Si, S. and Dhillon, I.S.},
 title = {A Divide-and-Conquer Solver for Kernel Support Vector Machines},
 journal = {ICML},
 year = {2014},
}

@article{discoicml,
 author = {Zhang, Y. and Xiao, L.},
 title = {Di{SCO}: {C}ommunication-Efficient Distributed Optimization of Self-Concordant Loss},
 journal = {ICML},
 year = {2015},
}

@article{byrd2012,
 author = {Byrd, R. H. and Chin, G. M. and Neveitt, W. and Nocedal, J.},
 title = {On the use of stochastic {H}essian information in optimization methods for machine learning},
 journal = {SIAM Journal of Optimization},
 year = {2012},
 pages = {977-995},
}


@article{yang2013a,
 author = {Yang, T.},
 title = {Trading computation for communication: distributed stochastic dual coordinate ascent},
 journal = {NIPS},
 year = {2013},
}

@article{yang2013b,
 author = {Yang, T. and Zhu, S. and Jin, R. and Lin, Y.},
 title = {Analysis of distributed stochastic dual coordinate ascent},
 journal = {arXiv:1312.1031},
 year = {2013},
}

@article{pechyony2011,
 author = {Pechyony, D. and Shen, L. and Jones, R.},
 title = {Solving large scale linear {SVM} with distributed block minimization},
 journal = {NIPS workshop on Big Learning},
 year = {2011},
}

@article{jaggi2014,
 author = {Jaggi, M. and Smith, V. and Tak{\'a}{\v c}, M. and Terhorst, J. and Krishnan, S. and Hofmann, T. and Jordan, M.I.},
 title = {Communication-efficient distributed dual coordinate ascent},
 journal = {arXiv:1409.1458},
 year = {2014},
}

@article{zhang2012,
 author = {Zhang, C. and Lee, H. and Shin, K.G.},
 title = {Efficient distributed linear classification algorithms via the alternating direction method of multipliers},
 journal = {CIKM},
 year = {2012},
}

@article{mahajan2013,
 author = {Mahajan, D. and Keerthi, S. S. and Sundararajan, S. and Bottou, L.},
 title = {A parallel {SGD} method with strong convergence},
 journal = {NIPS Workshop on Optimization in Machine Learning},
 year = {2013},
}

@article{dhruv2013,
 author = {Mahajan, D. and Keerthi, S. S. and Sundararajan, S. and Bottou, L.},
 title = {A functional approximation based distributed learning algorithm},
 journal = {arXiv:1310.8418},
 year = {2013},
}

@article{deng2012,
 author = {Deng, W. and Yin, W.},
 title = {On the global linear convergence of the generalized alternating direction method of multipliers},
 journal = {Rice University CAAM Technical Report},
 volume = {TR12-14},
 year = {2012},
}

@BOOK {bertsekas1997,
        AUTHOR={Bertsekas, D. P. and Tsitsiklis, J. N.},
        TITLE={Parallel and distributed computation: Numerical methods},
        PUBLISHER={Athena Scientific},
        ADDRESS={Cambridge, MA},
        YEAR={1997},
}

@article{sharir2014,
 author = {Sharir, O. and Srebro, N. and Zhang, T.},
 title = {Communication Efficient Distributed Optimization using an Approximate Newton-type Method},
 journal = {arXiv:1312.7853v4},
 year = {2014},
}


@article{mairal2013,
 author = {Mairal, J.},
 title = {Optimization with first order surrogate functions},
 journal = {ICML},
 year = {2013},
}

@BOOK {boyd2004,
        AUTHOR={Boyd, S. and Vandenberghe, L.},
        TITLE={Convex optimization},
        PUBLISHER={Cambridge University Press},
        ADDRESS={Cambridge, UK},
        YEAR={2004},
}

@BOOK {smola2008,
        AUTHOR={Smola, A. and Vishwanathan, S.V.N.},
        TITLE={Introduction to Machine Learning},
        PUBLISHER={Cambridge University Press},
        ADDRESS={Cambridge, UK},
        YEAR={2008},
}

@BOOK {dennis1996,
        AUTHOR={Dennis, J.E. and Schnabel, R.B.},
        TITLE={Numerical methods for unconstrained optimization and nonlinear equations},
        PUBLISHER={Cambridge University Press},
        ADDRESS={SIAM, Philadelphia},
        YEAR={1996},
}


@article{wolfe1969,
 author = {Wolfe, P.},
 title = {Convergence conditions for ascent methods},
 journal = {SIAM Review},
 volume = {11},
 year = {1969},
 pages = {226--235},
}

@article{wolfe1971,
 author = {Wolfe, P.},
 title = {Convergence conditions for ascent methods: {II}: {S}ome corrections},
 journal = {SIAM Review},
 volume = {13},
 year = {1971},
 pages = {185--188},
}

@article{chu2006,
 author = {Chu, C.T. and Kim, S.K. and Lin, Y.A. and Yu, Y.Y. and Bradski, G. and Ng, A.Y. and Olukotun, K.},
 title = {Map-Reduce for machine learning on multicore},
 journal = {NIPS},
 year = {2006},
 pages = {281--288},
}

@article{johnson2013,
 author = {Johnson, R. and Zhang, T.},
 title = {Accelerating stochastic gradient descent using predictive variance reduction},
 journal = {NIPS},
 year = {2013},

}

@INPROCEEDINGS{agarwal2011,
    author = {Agarwal, A. and Chapelle, O. and Dudik, M. and Langford, J.},
    title = {A reliable effective terascale linear learning system},
    booktitle = {arXiv:1140.4198},
    year = {2011},
}

@inproceedings{bottou2010,
  author = {Bottou, L.},
  title = {Large-Scale Machine Learning with Stochastic Gradient Descent},
  year = {2010},
  booktitle = {COMPSTAT'2010},
  pages = {177--187},
}

@inproceedings{mcdonald2010,
  title = {Distributed Training Strategies for the Structured Perceptron},
  author = {McDonald, R.T. and Hall, K. and Mann, G.},
  booktitle = {HLT-NAACL},
  pages = {456-464},
  year = 2010
}

@inproceedings{mann2009,
  title = {Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models},
  author = {Mann, G. and McDonald, R.T. and Mohri, M. and Silberman, N. and Walker, D.},
  booktitle = {NIPS},
  pages = {1231-1239},
  year = 2009
}

@inproceedings{zinkevich2010,
   author = {Zinkevich, M. and Weimer, M. and Smola, A. and Li, L.},
   Booktitle = {NIPS},
   Pages = {2595--2603},
   Title = {Parallelized Stochastic Gradient Descent},
   Year = {2010},
}

@inproceedings{hall2010,
title = {MapReduce/Bigtable for Distributed Optimization},
author  = {Hall, K.B. and Gilpin, S. and Mann, G.},
year  = 2010,
booktitle = {NIPS Workshop on Leaning on Cores, Clusters, and Clouds}
}

@inproceedings{hsieh2008,
  title = {A dual coordinate descent method for large-scale linear {SVM}},
  author = {Hsieh, C.J. and Chang, K.W. and Lin, C.J. and Keerthi, S.S. and Sundararajan, S.},
  booktitle = {ICML},
  pages = {408-415},
  year = {2008},
}

@INPROCEEDINGS{leroux2012,
    author = {Le Roux, N. and Schmidt, M. and Bach, F.},
    title = {A stochastic gradient method with an exponential convergence rate for strongly convex optimization with finite training sets},
    booktitle = {arXiv},
    year = {2012},
}

@article{chang2008,
 author = {Chang, K.W. and Hsieh, C.J. and Lin, C.J.},
 title = {Coordinate Descent Method for Large-scale L2-loss Linear {SVM}},
 journal = {JMLR},
 year = {2008},
 pages = {1369--1398},
}

@article{lin2008,
 author = {Lin, C.J. and Weng, R.C. and Keerthi, S.S.},
 title = { Trust region Newton method for large-scale logistic regression},
 journal = {JMLR},
 year = {2008},
 pages = {627--650},
}

@article{wang2013,
 author = {Wang, P.W. and Lin, C.J.},
 title = {Iteration complexity of feasible descent methods for convex optimization},
 journal = {Technical Report, National Taiwan University},
 year = {2013},

}

@article{boyd2011,
 author = {S. Boyd and N. Parikh and E. Chu and B. Peleato and J. Eckstein},
 title = {Distributed optimization and statistical learning via the alternating direction method of multipliers},
 journal = {Foundations and Trends in Machine Learning},
 year = {2011},
 pages = {1--122},
}

@article{richtarik2013,
  added-at = {2013-01-02T00:00:00.000+0100},
  author = {Richt{\'a}rik, P. and Tak{\'a}c, M.},
  journal = {CoRR},
  title = {Parallel Coordinate Descent Methods for Big Data Optimization},
  volume = {abs/1212.0873},
  year = {2012},
}

@article{patriksson1998ca,
 author = {Patriksson, M.},
 title = {Cost Approximation: A Unified Framework of Descent Algorithms for Nonlinear Programs},
 journal = {SIAM J. on Optimization},
 volume = {8},
 year = {1998},
 pages = {561--582},
}

@article{patriksson1998fp,
 author = {Patriksson, M.},
 title = {Decomposition Methods for Differentiable Optimization Problems over Cartesian Product Sets},
 journal = {Comput. Optim. Appl.},
 volume = {9},
 year = {1998},
 pages = {5--42},
}

@inproceedings{OurPaper,
   author = {Authors},
   Booktitle = {Supplementary Material},
   Pages = {1--11},
   Title = {A functional approximation based distributed learning algorithm},
   Year = {2014},
}

@article{liu89,
  added-at = {2006-10-28T22:24:27.000+0200},
  author = {Liu, D. C. and Nocedal, J.},
  biburl = {http://www.bibsonomy.org/bibtex/2ba8d815c4e0841a274d10b20fcb97d8b/thomas},
  description = {On The Limited Memory Bfgs Method For Large Scale Optimization - Liu, Nocedal (ResearchIndex)},
  interhash = {3c07caded0ed60d42c5dc813b5459b83},
  intrahash = {ba8d815c4e0841a274d10b20fcb97d8b},
  journal = {Math. Programming},
  keywords = {memory uni large BFGS conjugate scale diplomarbeit limited gradient optimization L-BFGS used},
  number = {3, (Ser. B)},
  pages = {503--528},
  timestamp = {2006-10-28T22:24:27.000+0200},
  title = {On the limited memory {B}{F}{G}{S} method for large scale optimization},
  volume = 45,
  year = 1989
}
