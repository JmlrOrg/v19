\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{LASSO,candesTao04}
\citation{Meier08,liu09b,yuan06,bach11}
\citation{bach10,Mairal10}
\citation{RudinTV92,vogel,fl}
\citation{atomic}
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.2}}
\newlabel{eq.1}{{1.1}{1}{}{equation.3}{}}
\citation{moreau62}
\citation{sraBook,parikh2014proximal}
\citation{fista,nest07,Combettes09,KimICML10,schmidt11}
\citation{schmidt11,salzo,nocops}
\newlabel{eq.oned}{{1.2}{2}{}{equation.5}{}}
\newlabel{eq.multid}{{1.3}{2}{}{equation.6}{}}
\newlabel{eq.twod}{{1.4}{2}{}{equation.7}{}}
\newlabel{eq.2}{{1.5}{2}{}{equation.8}{}}
\newlabel{eq.fbs}{{1.6}{2}{}{equation.9}{}}
\citation{daviesTautString}
\citation{Barlow}
\citation{fastTV}
\citation{More83}
\citation{RudinTV92}
\citation{TwIST}
\citation{esedogluAnisoTV}
\citation{steidl2009}
\citation{ZhuTVAlg08}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Contributions}{3}{subsection.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Related Work}{3}{subsection.13}}
\citation{DahlTV10}
\citation{fl}
\citation{kolar}
\citation{harLev10}
\citation{chaDar09}
\citation{jegBac13}
\citation{vert}
\citation{frHaHoTi07,liuYe10}
\citation{chaDar09}
\citation{Combettes09,BoydTV}
\citation{dpTV}
\citation{fastTV}
\citation{pontow09}
\citation{jegBac13}
\citation{splitbreg}
\citation{splitbreg}
\citation{ramdas}
\citation{fista}
\citation{sparsa}
\citation{SALSA}
\citation{TwIST}
\citation{KimICML10}
\citation{YangTV}
\citation{trendf}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Summary of the Paper}{4}{subsection.14}}
\citation{chaDar09}
\citation{jegBac13}
\citation{fastTV}
\citation{fastTV}
\citation{dpTV}
\citation{Kolmogorov16}
\citation{boyd.kim,trendf}
\citation{wytock}
\@writefile{toc}{\contentsline {section}{\numberline {2}TV-L1: Fast Prox-Operators for $\text  {Tv}_1^{\text  {1D}}$}{5}{section.15}}
\newlabel{sec:tvl1}{{2}{5}{}{section.15}{}}
\newlabel{eq:l1tv}{{2.1}{5}{}{equation.16}{}}
\citation{grasmairTV07,daviesTautString,Barlow}
\citation{SteidlTautString}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The Taut-String Method for $\text  {Tv}_1^{\text  {1D}}$}{6}{subsection.17}}
\newlabel{eq:31}{{2.2}{6}{}{equation.18}{}}
\newlabel{eq:32}{{2.3}{6}{}{equation.19}{}}
\newlabel{eq:33}{{2.4}{6}{}{equation.20}{}}
\newlabel{eq:tautString}{{2.5}{6}{}{equation.21}{}}
\citation{daviesTautString}
\citation{daviesTautString}
\citation{fastTV}
\citation{fastTV}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example of the taut string method. The cumulative sum $\bm  {r}$ of the input signal values $\bm  {y}$ is shown as the dashed line; the black dots mark the points $(i,r_i)$. The bottom and top of the $\lambda $-width tube are shown in red. The taut string solution $\bm  {s}$ is shown as a blue line. }}{7}{figure.23}}
\newlabel{fig:tautStringExample}{{1}{7}{\small Example of the taut string method. The cumulative sum $\vr $ of the input signal values $\vy $ is shown as the dashed line; the black dots mark the points $(i,r_i)$. The bottom and top of the $\lambda $-width tube are shown in red. The taut string solution $\vs $ is shown as a blue line}{figure.23}{}}
\citation{Knuth97}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Taut string algorithm for TV-L1-proximity}}{8}{algorithm.24}}
\newlabel{algTV1classicTautString}{{1}{8}{}{algorithm.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Efficient Implementation of Taut-Strings}{8}{subsubsection.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example of the evolution of the taut string method. The smallest concave majorant (blue) and largest convex minorant (green) are updated are every step. At step (1) the algorithm is initialized. Steps (2) to (4) successfully manage to update majorant and minorant without producing crossings between them. Note how while the concave majorant keeps adding segments without issue, the convex minorant must remove and merge existing segments with new ones to mantain a convex function from the origin to the new points. At step (5) the end of the tube is reached, but the minorant and majorant slopes overlap, and so it is necessary to break the segment at the left-most point where the majorant/minorant touched the tube. Since the left-most touching point is in the concave majorant it's leftmost segment is removed and placed in the solution, while the convex minorant is updated as a straight line from the detected breakpoint to the last explored point, resulting in (6). The algorithm would then continue adding segments, but since the majorant/minorant slopes are still crossing, the procedure of fixing segments to the solution is repeated through steps (6), (7) and (8). Finally at step (9) the slopes are no longer crossing and the method would continue adding tube segments, but since the end of the tube has already been reached the algorithm stops. }}{9}{figure.26}}
\newlabel{fig:classictautstringrun}{{2}{9}{\small Example of the evolution of the taut string method. The smallest concave majorant (blue) and largest convex minorant (green) are updated are every step. At step (1) the algorithm is initialized. Steps (2) to (4) successfully manage to update majorant and minorant without producing crossings between them. Note how while the concave majorant keeps adding segments without issue, the convex minorant must remove and merge existing segments with new ones to mantain a convex function from the origin to the new points. At step (5) the end of the tube is reached, but the minorant and majorant slopes overlap, and so it is necessary to break the segment at the left-most point where the majorant/minorant touched the tube. Since the left-most touching point is in the concave majorant it's leftmost segment is removed and placed in the solution, while the convex minorant is updated as a straight line from the detected breakpoint to the last explored point, resulting in (6). The algorithm would then continue adding segments, but since the majorant/minorant slopes are still crossing, the procedure of fixing segments to the solution is repeated through steps (6), (7) and (8). Finally at step (9) the slopes are no longer crossing and the method would continue adding tube segments, but since the end of the tube has already been reached the algorithm stops}{figure.26}{}}
\citation{fastTV}
\citation{fastTV}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Linearized Taut-String Method for $\text  {Tv}_1^{\text  {1D}}$}{10}{subsection.28}}
\newlabel{sec:taut}{{2.2}{10}{}{subsection.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example of the evolution of the linearized taut string method. The smallest affine majorant of the tube bottom (blue) and greatest affine minorant of the tube ceiling (green) are updated at every step. At step (1) the algorithm is initialized. Steps (2) to (4) successfully manage to update majorant/minorant without crossings. At step (5), however, the slopes cross, and so it is necessary to break the segment. Since the left-most tube touching point is the one in the majorant, the majorant is broken down at that point and its left-hand side is added to the solution, resulting in (6). The method is then restarted at the break point, with majorant/minorant being updated at step (7), though at step (8) once again a crossing is detected. Hence, at step (9) a breaking point is introduced again and the algorithm is restarted once more. Following this, step (10) manages to update majorant/minorant slopes up to the end of the tube, and so at step (11) the final segment is built using the (now equal) slopes. }}{11}{figure.30}}
\newlabel{fig:tautStringAlgEvolution}{{3}{11}{\small Example of the evolution of the linearized taut string method. The smallest affine majorant of the tube bottom (blue) and greatest affine minorant of the tube ceiling (green) are updated at every step. At step (1) the algorithm is initialized. Steps (2) to (4) successfully manage to update majorant/minorant without crossings. At step (5), however, the slopes cross, and so it is necessary to break the segment. Since the left-most tube touching point is the one in the majorant, the majorant is broken down at that point and its left-hand side is added to the solution, resulting in (6). The method is then restarted at the break point, with majorant/minorant being updated at step (7), though at step (8) once again a crossing is detected. Hence, at step (9) a breaking point is introduced again and the algorithm is restarted once more. Following this, step (10) manages to update majorant/minorant slopes up to the end of the tube, and so at step (11) the final segment is built using the (now equal) slopes}{figure.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Illustration of the geometric concepts involved in the linearized taut string method. The greatest linear minorant (of the tube ceiling) is depicted in green, while the smallest linear majorant (of the tube bottom) is shown in blue. The $\delta $ slopes and $h$ heights are presented updated up to the index shown as $\relax $\@@underline {\hbox {\setbox \tw@ \hbox {\begingroup \pdfcolorstack \main@pdfcolorstack push{0 g 0 G}\aftergroup \pdfcolorstack \main@pdfcolorstack pop\relax i\endgraf \endgroup }\dp \tw@ \z@ \box \tw@ }}\mathsurround \z@ $\relax $. }}{12}{figure.39}}
\newlabel{fig:tautStringAlgVariables}{{4}{12}{\small Illustration of the geometric concepts involved in the linearized taut string method. The greatest linear minorant (of the tube ceiling) is depicted in green, while the smallest linear majorant (of the tube bottom) is shown in blue. The $\delta $ slopes and $h$ heights are presented updated up to the index shown as $\underbar i$}{figure.39}{}}
\@writefile{toc}{\contentsline {paragraph}{Height variables.}{12}{algorithm.40}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Linearized taut string algorithm for TV-L1-proximity}}{13}{algorithm.40}}
\newlabel{algTV1tautString}{{2}{13}{}{algorithm.40}{}}
\citation{fastTV}
\citation{fastTV}
\citation{fastTV}
\citation{fastTV}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of the main features of reviewed taut-string algorithms.}}{14}{table.43}}
\newlabel{tab:tautstringComparison}{{1}{14}{Comparison of the main features of reviewed taut-string algorithms}{table.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Comparison of Taut-String Methods and a Hybrid Strategy}{14}{subsection.41}}
\newlabel{sec:tauthybrid}{{2.3}{14}{}{subsection.41}{}}
\citation{daviesTautString}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Taut-string Methods for Weighted $\text  {Tv}_1^{\text  {1D}}$}{15}{subsection.44}}
\newlabel{sec:tautweighted}{{2.4}{15}{}{subsection.44}{}}
\newlabel{eq.14}{{2.6}{15}{}{equation.45}{}}
\newlabel{eq.15}{{2.7}{15}{}{equation.46}{}}
\newlabel{eq:34}{{2.8}{15}{}{equation.47}{}}
\citation{Conn}
\citation{More83}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example of the weighted taut string method with $\bm  {w}=(1.35$, $3.03$, $0.73$, $0.06$, $0.71$, $0.20$, $0.12$, $1.49$, $1.41)$. The cumulative sum $\bm  {r}$ of the input signal values $\bm  {y}$ is shown as the dashed line, with the black dots marking the points $(i,\bm  {r}_i)$. The bottom and ceiling of the tube are shown in red, which vary in width at each step following the weights $\bm  {w}_i$. The weighted taut string solution $\bm  {s}$ is shown as a blue line. }}{16}{figure.49}}
\newlabel{fig:tautStringWeightedExample}{{5}{16}{\small Example of the weighted taut string method with $\vw =(1.35$, $3.03$, $0.73$, $0.06$, $0.71$, $0.20$, $0.12$, $1.49$, $1.41)$. The cumulative sum $\vr $ of the input signal values $\vy $ is shown as the dashed line, with the black dots marking the points $(i,\vr _i)$. The bottom and ceiling of the tube are shown in red, which vary in width at each step following the weights $\vw _i$. The weighted taut string solution $\vs $ is shown as a blue line}{figure.49}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Modified lines for weighted version of Algorithm\nobreakspace  {}\ref  {algTV1tautString}}}{16}{algorithm.50}}
\newlabel{algTV1tautStringWeighted}{{3}{16}{}{algorithm.50}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Other One-Dimensional TV Variants}{16}{section.51}}
\newlabel{sec:tvoneothers}{{3}{16}{}{section.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}TV-L2: Proximity for $\text  {Tv}_2^{\text  {1D}}$}{16}{subsection.52}}
\newlabel{sec:tvl2}{{3.1}{16}{}{subsection.52}{}}
\newlabel{eqTV2dual}{{3.1}{16}{}{equation.53}{}}
\citation{More83}
\newlabel{eqCSPTV2}{{3.2}{17}{}{equation.54}{}}
\newlabel{eq:25}{{3.3}{17}{}{equation.55}{}}
\newlabel{eq:2}{{3.4}{17}{}{equation.56}{}}
\newlabel{eq:1}{{3.5}{17}{}{equation.57}{}}
\newlabel{eq:MSNupdate}{{3.6}{17}{}{equation.58}{}}
\citation{nest07,fista}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces MSN based TV-L2 proximity}}{18}{algorithm.59}}
\newlabel{algMS}{{4}{18}{}{algorithm.59}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces GP algorithm for TV-L$_2$ proximity}}{18}{algorithm.61}}
\newlabel{alg:GP}{{5}{18}{}{algorithm.61}{}}
\newlabel{eq:gp}{{3.7}{18}{}{equation.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}TV-Lp: Proximity for $\text  {Tv}_p^{1D}$}{19}{subsection.62}}
\newlabel{sec:tvp}{{3.2}{19}{}{subsection.62}{}}
\newlabel{eqTVpdual}{{3.8}{19}{}{equation.63}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Efficient Projection onto the $\ell _q$-ball}{19}{subsubsection.64}}
\newlabel{eq:11}{{3.9}{19}{}{equation.65}{}}
\newlabel{eq:8}{{3.10}{19}{}{equation.66}{}}
\newlabel{eq:10}{{3.11}{19}{}{equation.67}{}}
\newlabel{eq:lpgrad}{{3.12}{19}{}{equation.68}{}}
\newlabel{eq:hessian}{{3.13}{19}{}{equation.69}{}}
\citation{JaggiFW}
\citation{Bertsekas}
\newlabel{eq:lpredhess}{{3.14}{20}{}{equation.70}{}}
\newlabel{eq:smlpredhess}{{3.15}{20}{}{equation.71}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Frank-Wolfe Algorithm for TV-$L_p$ Proximity}{20}{subsubsection.73}}
\newlabel{sec:FW}{{3.2.2}{20}{}{subsubsection.73}{}}
\newlabel{eq:5}{{3.17}{20}{}{equation.74}{}}
\newlabel{alg:FW}{{6}{20}{}{algorithm.75}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces   Frank-Wolfe (FW) }}{20}{algorithm.75}}
\citation{JaggiFW}
\citation{condatl1,liu09,kiwiel}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Prox Operator for TV-L$_\infty $}{21}{subsection.76}}
\newlabel{eq:20}{{3.18}{21}{}{equation.77}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Prox Operators for Multidimensional TV}{21}{section.78}}
\newlabel{sec:proxMulti}{{4}{21}{}{section.78}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Proximity Stacking}{21}{subsection.79}}
\newlabel{eq:sumobj}{{4.1}{21}{}{equation.80}{}}
\citation{Combettes09}
\citation{Barbero11}
\citation{jegBac13}
\citation{ParallelDykstra}
\citation{pierra84}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Design schema in proximal optimization for minimizing the function $f(\bm  {x}) + \DOTSB \sum@ \slimits@ _{i=1}^m r_i(\bm  {x})$. Proximal stacking makes the sum of regularizers appear as a single one to the proximal method, while retaining modularity in the design of each proximity step through the use of a combiner method. For non-smooth $f$ the same schema applies by just replacing the $f$ gradient operator by its corresponding proximity operator.  }}{22}{figure.83}}
\newlabel{fig:proximalModulesStacking}{{6}{22}{\small Design schema in proximal optimization for minimizing the function $f(\vx ) + \sum _{i=1}^m r_i(\vx )$. Proximal stacking makes the sum of regularizers appear as a single one to the proximal method, while retaining modularity in the design of each proximity step through the use of a combiner method. For non-smooth $f$ the same schema applies by just replacing the $f$ gradient operator by its corresponding proximity operator}{figure.83}{}}
\newlabel{eq:proxstacking}{{4.2}{22}{}{equation.81}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Proximal Dykstra (PD)}{22}{subsubsection.84}}
\newlabel{sec:PD}{{4.1.1}{22}{}{subsubsection.84}{}}
\citation{Combettes09}
\newlabel{alg:PD}{{7}{23}{}{algorithm.85}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces   Proximal Dykstra }}{23}{algorithm.85}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Parallel-Proximal Dykstra}}{23}{algorithm.86}}
\newlabel{alg:PPXA}{{8}{23}{}{algorithm.86}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Alternating Reflections -- Douglas-Rachford (DR)}{23}{subsubsection.87}}
\newlabel{sec:DR}{{4.1.2}{23}{}{subsubsection.87}{}}
\newlabel{eq:DRproblem}{{4.3}{23}{}{equation.88}{}}
\citation{jegBac13}
\citation{jegBac13}
\citation{Bach13}
\citation{Bauschke04}
\citation{Combettes09}
\newlabel{eq:DRsequence}{{4.4}{24}{}{equation.89}{}}
\newlabel{eq:tvdr}{{4.5}{24}{}{equation.90}{}}
\newlabel{eq:drprojection}{{4.6}{24}{}{equation.91}{}}
\citation{YangTV}
\newlabel{alg:DR}{{9}{25}{}{algorithm.92}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces   Alternating reflections -- Douglas Rachford (DR) }}{25}{algorithm.92}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Alternating-Direction Method of Multipliers (ADMM)}{25}{subsubsection.93}}
\newlabel{sec:ADMM}{{4.1.3}{25}{}{subsubsection.93}{}}
\citation{chambollePock}
\citation{chambollePock}
\citation{chambollePock}
\citation{fista}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Alternating Direction Method of Multipliers (ADMM)}}{26}{algorithm.94}}
\newlabel{alg:ADMM}{{10}{26}{}{algorithm.94}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Dual Proximity Methods}{26}{subsubsection.95}}
\newlabel{sec:dualproxmethods}{{4.1.4}{26}{}{subsubsection.95}{}}
\citation{condatGenProx}
\citation{chambollePock}
\citation{Kolmogorov16}
\citation{ChambolleErgodic14}
\citation{ChambolleBlockAccel}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Two-Dimensional TV}{29}{subsection.96}}
\newlabel{sec:twod}{{4.2}{29}{}{subsection.96}{}}
\newlabel{eq.twod.again}{{4.7}{29}{}{equation.97}{}}
\newlabel{eq:13}{{4.8}{29}{}{equation.98}{}}
\newlabel{eq:14}{{4.9}{29}{}{equation.99}{}}
\newlabel{eq:16}{{4.10}{29}{}{equation.100}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Higher-Dimensional TV}{29}{subsection.101}}
\newlabel{sec:multid}{{4.3}{29}{}{subsection.101}{}}
\newlabel{eq.multid.again}{{4.11}{29}{}{equation.102}{}}
\newlabel{eq:17}{{4.12}{29}{}{equation.103}{}}
\citation{LAPACK}
\newlabel{eq:18}{{4.13}{30}{}{equation.104}{}}
\newlabel{eq:19}{{4.14}{30}{}{equation.105}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments and Applications}{30}{section.106}}
\newlabel{sec:exps}{{5}{30}{}{section.106}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}$\text  {Tv}_1^{\text  {1D}}$ Experiments and Applications}{30}{subsection.107}}
\newlabel{sec:TVproxExp}{{5.1}{30}{}{subsection.107}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Running Time Results for Synthetic Data}{30}{subsubsection.108}}
\citation{fastTV}
\citation{slep}
\citation{liuYe10}
\citation{fastTV}
\citation{dpTV}
\citation{Kolmogorov16}
\newlabel{fig:tv1runlambda}{{7(b)}{31}{Subfigure 7(b)}{subfigure.112}{}}
\newlabel{sub@fig:tv1runlambda}{{(b)}{31}{Subfigure 7(b)\relax }{subfigure.112}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Running times (in secs) for proposed and state of the art solvers for $\text  {Tv}_1^{\text  {1D}}$-proximity with increasing a) input sizes, b) penalties. Both axes are on a log-scale. }}{31}{figure.114}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{31}{figure.114}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{31}{figure.114}}
\newlabel{fig:tv1run}{{7}{31}{Running times (in secs) for proposed and state of the art solvers for $\tvell _1^{\oned }$-proximity with increasing a) input sizes, b) penalties. Both axes are on a log-scale}{figure.114}{}}
\citation{fastTV}
\citation{INRIAholidays}
\citation{Imagenet}
\citation{INRIAholidays}
\citation{Imagenet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Worst Case Scenario}{32}{subsubsection.115}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Running Times on Natural Images}{32}{subsubsection.118}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Running times (in secs) for proposed and state of the art solvers for $\text  {Tv}_1^{\text  {1D}}$-proximity in the worst-case scenario for Condat's method, for increasing input sizes. Both axes are on a log-scale. }}{33}{figure.117}}
\newlabel{fig:tv1worst}{{8}{33}{Running times (in secs) for proposed and state of the art solvers for $\tvell _1^{\oned }$-proximity in the worst-case scenario for Condat's method, for increasing input sizes. Both axes are on a log-scale}{figure.117}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Detail of image data sets used for large-scale $\text  {Tv}_1^{\text  {1D}}$ experiments.  }}{33}{table.120}}
\newlabel{tab:imagedatasetsTVL1}{{2}{33}{Detail of image data sets used for large-scale $\tvell _1^{\oned }$ experiments}{table.120}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Running times (in secs) for the top performing proposed and state of the art solvers for $\text  {Tv}_1^{\text  {1D}}$-proximity over the whole INRIA Holidays data set, for increasing penalties. }}{34}{figure.122}}
\newlabel{fig:tv1inria}{{9}{34}{Running times (in secs) for the top performing proposed and state of the art solvers for $\tvell _1^{\oned }$-proximity over the whole INRIA Holidays data set, for increasing penalties}{figure.122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Running times (in secs) for the top performing proposed and state of the art solvers for $\text  {Tv}_1^{\text  {1D}}$-proximity over the whole Large Scale Visual Recognition Challenge 2010 validation data set, for increasing penalties. }}{35}{figure.124}}
\newlabel{fig:tv1imagenet}{{10}{35}{Running times (in secs) for the top performing proposed and state of the art solvers for $\tvell _1^{\oned }$-proximity over the whole Large Scale Visual Recognition Challenge 2010 validation data set, for increasing penalties}{figure.124}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Running Time Results for Weighted TV-L1}{35}{subsubsection.125}}
\citation{Kumar15}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Running times (in secs) for Projected Newton and Taut String solvers for weighted and uniform $\text  {Tv}_1^{\text  {1D}}$-proximity with increasing a) input sizes, b) penalties. Both axes are on a log-scale. }}{36}{figure.129}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{36}{figure.129}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{36}{figure.129}}
\newlabel{fig:tv1wrun}{{11}{36}{Running times (in secs) for Projected Newton and Taut String solvers for weighted and uniform $\tvell _1^{\oned }$-proximity with increasing a) input sizes, b) penalties. Both axes are on a log-scale}{figure.129}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Experiments for other 1D-TV Variants}{36}{subsection.132}}
\newlabel{sec:expothertv1d}{{5.2}{36}{}{subsection.132}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Running Time Results for TV-L2}{36}{subsubsection.133}}
\newlabel{sec:exptvl2}{{5.2.1}{36}{}{subsubsection.133}{}}
\citation{slep}
\newlabel{fig:tv2runN}{{12(a)}{37}{Subfigure 12(a)}{subfigure.134}{}}
\newlabel{sub@fig:tv2runN}{{(a)}{37}{Subfigure 12(a)\relax }{subfigure.134}{}}
\newlabel{fig:tv2runlambda}{{12(b)}{37}{Subfigure 12(b)}{subfigure.135}{}}
\newlabel{sub@fig:tv2runlambda}{{(b)}{37}{Subfigure 12(b)\relax }{subfigure.135}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Running times (in secs) for MSN, GP and a hybrid MSN+GP approach for $\text  {Tv}_2^{\text  {1D}}$-proximity with increasing a) input sizes, b) penalties. Both axes are on a log-scale.  }}{37}{figure.137}}
\newlabel{fig:tv2run}{{12}{37}{Running times (in secs) for MSN, GP and a hybrid MSN+GP approach for $\tvell _2^{\oned }$-proximity with increasing a) input sizes, b) penalties. Both axes are on a log-scale}{figure.137}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{37}{figure.137}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{37}{figure.137}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Running Time Results for TV-Lp}{37}{subsubsection.138}}
\newlabel{sec:exptvlp}{{5.2.2}{37}{}{subsubsection.138}{}}
\citation{fl}
\citation{cgh,cghsvm,frHaHoTi07}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Running times (in secs) for GP with PN projection, GP with SLEP's {\it  epp} projection, FW and a hybrid GP+FW algorithm, for $\text  {Tv}_p^{\text  {1D}}$-proximity with increasing input sizes and three different choices of $p$. Both axes are on a log-scale.  }}{38}{figure.143}}
\newlabel{fig:tvpfixedrun}{{13}{38}{Running times (in secs) for GP with PN projection, GP with SLEP's {\it epp} projection, FW and a hybrid GP+FW algorithm, for $\tvell _p^{\oned }$-proximity with increasing input sizes and three different choices of $p$. Both axes are on a log-scale}{figure.143}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{38}{figure.143}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{38}{figure.143}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{38}{figure.143}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Running Time Results for TV-L$\infty $}{38}{subsubsection.152}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Application: Proximal Optimization for Fused-Lasso}{38}{subsubsection.159}}
\newlabel{sec:FLasso}{{5.2.4}{38}{}{subsubsection.159}{}}
\newlabel{eq:24}{{5.1}{38}{}{equation.160}{}}
\citation{land}
\citation{kolar}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Attained duality gaps (a-c) and running times (d-f, in secs) for GP with PN projection, GP with SLEP's {\it  epp} projection, FW and a hybrid GP+FW algorithm, for $\text  {Tv}_p^{\text  {1D}}$-proximity with increasing penalties and three different choices of $p$. Both axes are on a log-scale.  }}{39}{figure.151}}
\newlabel{fig:tvplambdafixedrun}{{14}{39}{Attained duality gaps (a-c) and running times (d-f, in secs) for GP with PN projection, GP with SLEP's {\it epp} projection, FW and a hybrid GP+FW algorithm, for $\tvell _p^{\oned }$-proximity with increasing penalties and three different choices of $p$. Both axes are on a log-scale}{figure.151}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{39}{figure.151}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{39}{figure.151}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{39}{figure.151}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{39}{figure.151}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {}}}{39}{figure.151}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {}}}{39}{figure.151}}
\citation{liuYe10}
\citation{fista}
\newlabel{fig:tvinfrunN}{{15(a)}{40}{Subfigure 15(a)}{subfigure.153}{}}
\newlabel{sub@fig:tvinfrunN}{{(a)}{40}{Subfigure 15(a)\relax }{subfigure.153}{}}
\newlabel{fig:tvinfrunlambda}{{15(b)}{40}{Subfigure 15(b)}{subfigure.154}{}}
\newlabel{sub@fig:tvinfrunlambda}{{(b)}{40}{Subfigure 15(b)\relax }{subfigure.154}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Running times (in secs) for GP for $\text  {Tv}_{\infty }^{\text  {1D}}$-proximity with increasing a) input sizes, b) penalties. Both axes are on a log-scale.  }}{40}{figure.156}}
\newlabel{fig:tvinfrun}{{15}{40}{Running times (in secs) for GP for $\tvell _{\infty }^{\oned }$-proximity with increasing a) input sizes, b) penalties. Both axes are on a log-scale}{figure.156}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{40}{figure.156}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{40}{figure.156}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces   Fused-Lasso models addressed by proximal splitting. }}{40}{figure.158}}
\newlabel{fig:FLmodels}{{16}{40}{Fused-Lasso models addressed by proximal splitting}{figure.158}{}}
\citation{yaoLiang,rinaldo,fl}
\citation{slep}
\citation{Stransky06}
\citation{Golub99}
\citation{Alon99}
\citation{RogersOvarian05}
\citation{HuaFS09}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.5}Fused-Lasso Experiments: Simulation}{41}{subsubsection.161}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.6}Fused-Lasso Experiments: Microarray Classification}{41}{subsubsection.164}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Relative distance to optimum vs time of the Fused Lasso optimizers under comparison, for the different layouts of synthetic matrices.}}{42}{figure.163}}
\newlabel{fig:FL-obj}{{17}{42}{Relative distance to optimum vs time of the Fused Lasso optimizers under comparison, for the different layouts of synthetic matrices}{figure.163}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Classification accuracies for the presented Fused--Lasso models on microarray data. For the Variable Fusion models an $\ell _2$ version of TV was employed.  }}{42}{table.166}}
\newlabel{tab:FLreal}{{3}{42}{Classification accuracies for the presented Fused--Lasso models on microarray data. For the Variable Fusion models an $\ell _2$ version of TV was employed}{table.166}{}}
\citation{RudinTV92}
\citation{TwIST}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}2D-TV: Experiments and Applications}{43}{subsection.167}}
\newlabel{sec:2d.expt}{{5.3}{43}{}{subsection.167}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Image Denoising through Anisotropic Filtering}{43}{subsubsection.168}}
\newlabel{sec:aniso2D}{{5.3.1}{43}{}{subsubsection.168}{}}
\newlabel{eq.rof}{{5.2}{43}{}{equation.169}{}}
\citation{BioucasTV06}
\citation{Choksi2DBarcode,LiL1TV}
\citation{splitbreg}
\citation{goldfarb2009parametric}
\citation{chambollePock}
\citation{chambollePock}
\citation{ChambolleErgodic14}
\newlabel{eq:26}{{5.3}{44}{}{equation.170}{}}
\citation{SALSA}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Types of noise and parameters for each test image. A $\varnothing $ indicates that such noise was not applied for the image. {\it  Gaussian} and {\it  Speckle} correspond to gaussian additive and multiplicative (respectively) noises with zero mean and the indicated variance. {\it  Salt \& Pepper} noise turns into black or white the indicated fraction of image pixels. {\it  Poisson} regenerates each pixel by drawing a random value from a Poisson distribution with mean equal to the original pixel value, thus producing a more realistic noise.  }}{45}{table.172}}
\newlabel{tab:noises}{{4}{45}{\small Types of noise and parameters for each test image. A $\varnothing $ indicates that such noise was not applied for the image. {\it Gaussian} and {\it Speckle} correspond to gaussian additive and multiplicative (respectively) noises with zero mean and the indicated variance. {\it Salt \& Pepper} noise turns into black or white the indicated fraction of image pixels. {\it Poisson} regenerates each pixel by drawing a random value from a Poisson distribution with mean equal to the original pixel value, thus producing a more realistic noise}{table.172}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Relative distance to optimum vs time of the denoising 2D-TV algorithms under comparison, for the different images considered in the experiments.}}{46}{figure.174}}
\newlabel{fig:2DTV-obj}{{18}{46}{Relative distance to optimum vs time of the denoising 2D-TV algorithms under comparison, for the different images considered in the experiments}{figure.174}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Increased Signal to Noise Ratio (ISNR) vs time of the denoising 2D-TV algorithms under comparison, for the different images considered in the experiments.}}{47}{figure.176}}
\newlabel{fig:2DTV-ISNR}{{19}{47}{Increased Signal to Noise Ratio (ISNR) vs time of the denoising 2D-TV algorithms under comparison, for the different images considered in the experiments}{figure.176}{}}
\citation{DuanTV}
\citation{WangTV14}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Parallelization Experiments}{48}{subsubsection.179}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Summary of key points of the compared $\text  {Tv}_{1,1}^{\text  {2D}}$ proximity (denoising) methods.  }}{49}{table.178}}
\newlabel{tab:2DmethodsComparison}{{5}{49}{\small Summary of key points of the compared $\tvell _{1,1}^{\twod }$ proximity (denoising) methods}{table.178}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Anisotropic Image Deconvolution}{49}{subsubsection.182}}
\citation{fista}
\citation{SALSA,TwIST}
\citation{Krishnan09}
\citation{HAIS07,NeurocomputingRejilla}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Multicore speedups on different images}}{50}{figure.181}}
\newlabel{fig.multicore}{{20}{50}{Multicore speedups on different images}{figure.181}{}}
\newlabel{eq:27}{{5.4}{50}{}{equation.183}{}}
\citation{frHaHoTi07}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Convolution kernels used for each test image. {\it  Average} substitutes each pixel with the average of its surrounding $n \times m$ neighbors. {\it  Disk} performs the same operation within a disk-shaped neighborhood of the shown radius. {\it  Gaussian} uses a $n \times n$ neighborhood and assigns different weights to each neighbor following the value of a gaussian distribution of the indicated deviation centered at the current pixel. {\it  Motion} emulates the distortions produced when taking a picture in motion, defining a neighborhood following a vector of the indicated length and angle.  }}{51}{table.185}}
\newlabel{tab:convolutions}{{6}{51}{\small Convolution kernels used for each test image. {\it Average} substitutes each pixel with the average of its surrounding $n \times m$ neighbors. {\it Disk} performs the same operation within a disk-shaped neighborhood of the shown radius. {\it Gaussian} uses a $n \times n$ neighborhood and assigns different weights to each neighbor following the value of a gaussian distribution of the indicated deviation centered at the current pixel. {\it Motion} emulates the distortions produced when taking a picture in motion, defining a neighborhood following a vector of the indicated length and angle}{table.185}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Relative distance to optimum vs time of the deconvolution 2D-TV algorithms under comparison, for the different images considered in the experiments.}}{52}{figure.187}}
\newlabel{fig:2DTVdeconv-obj}{{21}{52}{Relative distance to optimum vs time of the deconvolution 2D-TV algorithms under comparison, for the different images considered in the experiments}{figure.187}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Increased Signal to Noise Ratio (ISNR) vs time of the deconvolution 2D-TV algorithms under comparison, for the different images considered in the experiments.}}{53}{figure.189}}
\newlabel{fig:2DTVdeconv-ISNR}{{22}{53}{Increased Signal to Noise Ratio (ISNR) vs time of the deconvolution 2D-TV algorithms under comparison, for the different images considered in the experiments}{figure.189}{}}
\citation{frHaHoTi07}
\citation{frHaHoTi07}
\citation{goldfarb2009parametric}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}2D Fused-Lasso Signal Approximator}{54}{subsubsection.190}}
\newlabel{sec:TV2FLSA}{{5.3.4}{54}{}{subsubsection.190}{}}
\newlabel{eq:29}{{5.5}{54}{}{equation.191}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Application of Higher-Dimensional TV}{54}{subsection.192}}
\newlabel{sec:appl.multi}{{5.4}{54}{}{subsection.192}{}}
\newlabel{eq:30}{{5.6}{54}{}{equation.193}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Size details of video sequences used in the video denoising experiments.  }}{55}{table.195}}
\newlabel{tab:videoSequences}{{7}{55}{\small Size details of video sequences used in the video denoising experiments}{table.195}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Relative distance to optimum vs time of the denoising 3D-TV algorithms under comparison, for the different video sequences considered in the experiments.}}{55}{figure.197}}
\newlabel{fig:3DTV-obj}{{23}{55}{Relative distance to optimum vs time of the denoising 3D-TV algorithms under comparison, for the different video sequences considered in the experiments}{figure.197}{}}
\citation{dpTV}
\citation{bauCom}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Increased Signal to Noise Ratio (ISNR) vs time of the denoising 3D-TV algorithms under comparison, for the different video sequences considered in the experiments.}}{56}{figure.199}}
\newlabel{fig:3DTV-ISNR}{{24}{56}{Increased Signal to Noise Ratio (ISNR) vs time of the denoising 3D-TV algorithms under comparison, for the different video sequences considered in the experiments}{figure.199}{}}
\citation{moreau62}
\citation{bauCom}
\citation{bauCom}
\@writefile{toc}{\contentsline {section}{\numberline {A}Mathematical Background}{57}{section.201}}
\newlabel{app:mathbackground}{{A}{57}{Acknowledgments}{section.201}{}}
\newlabel{eq.19}{{A.1}{57}{Acknowledgments}{equation.202}{}}
\newlabel{eq.3}{{A.2}{57}{Acknowledgments}{equation.203}{}}
\newlabel{eq:7}{{A.3}{57}{Acknowledgments}{equation.204}{}}
\newlabel{eq.5}{{A.4}{57}{Acknowledgments}{equation.206}{}}
\newlabel{eq.6}{{A.5}{57}{Prox operator}{equation.208}{}}
\newlabel{prop.decomp}{{A.3}{57}{Moreau decomposition}{theorem.209}{}}
\newlabel{eq.10}{{A.6}{57}{Moreau decomposition}{equation.210}{}}
\citation{Bach13}
\newlabel{prop.pd}{{A.4}{58}{Acknowledgments}{theorem.211}{}}
\newlabel{eq.12}{{A.7}{58}{Acknowledgments}{equation.212}{}}
\newlabel{eq.13}{{A.8}{58}{Acknowledgments}{equation.213}{}}
\citation{Bach13}
\citation{Bach13}
\citation{Bach13}
\newlabel{pro:decompsubmodular}{{A.11}{59}{Acknowledgments}{theorem.220}{}}
\newlabel{pro:projpolytopes}{{A.12}{60}{Acknowledgments}{theorem.221}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}proxTV Toolbox}{61}{section.222}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proof on the Equality of Taut-String Problems}{61}{section.227}}
\newlabel{app:tautStringProof}{{C}{61}{Acknowledgments}{section.227}{}}
\newlabel{the:tautStringEq}{{C.1}{61}{Equality of taut-string problems}{theorem.228}{}}
\newlabel{eq:tautStringEq1}{{C.1}{61}{Equality of taut-string problems}{equation.229}{}}
\newlabel{eq:tautStringEq2}{{C.2}{61}{Equality of taut-string problems}{equation.230}{}}
\newlabel{eq:tautStringEq1KKT1}{{C.3}{62}{Acknowledgments}{equation.231}{}}
\newlabel{eq:tautStringEq1KKT2}{{C.4}{62}{Acknowledgments}{equation.232}{}}
\newlabel{eq:tautStringEq1KKT3}{{C.5}{62}{Acknowledgments}{equation.233}{}}
\newlabel{eq:tautStringEq1KKT4}{{C.6}{62}{Acknowledgments}{equation.234}{}}
\newlabel{eq:tautStringEq1KKT5}{{C.7}{62}{Acknowledgments}{equation.235}{}}
\newlabel{eq:tautStringEq2KKT1}{{C.8}{62}{Acknowledgments}{equation.236}{}}
\newlabel{eq:tautStringEq2KKT2}{{C.9}{62}{Acknowledgments}{equation.237}{}}
\newlabel{eq:tautStringEq2KKT3}{{C.10}{62}{Acknowledgments}{equation.238}{}}
\newlabel{eq:tautStringEq2KKT4}{{C.11}{62}{Acknowledgments}{equation.239}{}}
\newlabel{eq:tautStringEq2KKT5}{{C.12}{62}{Acknowledgments}{equation.240}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Proof on the Equivalence of Linearized Taut-String Method}{63}{section.241}}
\newlabel{app:tautStringAlgEq}{{D}{63}{Acknowledgments}{section.241}{}}
\citation{alaiz2013group,wytock}
\citation{boyd.kim,trendf}
\citation{TRON}
\citation{L-BFGS-B}
\citation{ProjNewton}
\citation{Bertsekas}
\@writefile{toc}{\contentsline {section}{\numberline {E}Projected-Newton for Weighted $\text  {Tv}_1^{\text  {1D}}$}{65}{section.243}}
\newlabel{app:projNewton}{{E}{65}{Acknowledgments}{section.243}{}}
\newlabel{eq.16}{{E.1}{65}{Acknowledgments}{equation.244}{}}
\newlabel{eq:6}{{E.2}{65}{Acknowledgments}{equation.245}{}}
\citation{LAPACK}
\citation{ProjNewton}
\citation{NumOptNocedal}
\citation{BM3Dvideos}
\@writefile{loa}{\contentsline {algorithm}{\numberline {11}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Stepsize selection for Projected Newton}}{67}{algorithm.249}}
\newlabel{algStep}{{11}{67}{Acknowledgments}{algorithm.249}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {12}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip PN algorithm for TV-L1-proximity}}{67}{algorithm.250}}
\newlabel{algTV1}{{12}{67}{Acknowledgments}{algorithm.250}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Testing Images and Videos, and Experimental Results}{67}{section.251}}
\newlabel{app:images}{{F}{67}{Acknowledgments}{section.251}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  Test images used in the experiments together with their sizes in pixels. Images displayed have been scaled down to fit in page.  }}{69}{figure.257}}
\newlabel{fig:imagesExp}{{25}{69}{Test images used in the experiments together with their sizes in pixels. Images displayed have been scaled down to fit in page}{figure.257}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces  Noisy versions of images used in the experiments.  }}{70}{figure.259}}
\newlabel{fig:imagesNoised}{{26}{70}{Noisy versions of images used in the experiments}{figure.259}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces  Denoising results for the test images.  }}{71}{figure.261}}
\newlabel{fig:imagesDenoised}{{27}{71}{Denoising results for the test images}{figure.261}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces  Noisy and convoluted versions of images used in the experiments.  }}{72}{figure.263}}
\newlabel{fig:imagesBlurred}{{28}{72}{Noisy and convoluted versions of images used in the experiments}{figure.263}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces  Deconvolution results for the test images.  }}{73}{figure.265}}
\newlabel{fig:imagesDeblurred}{{29}{73}{Deconvolution results for the test images}{figure.265}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces   A selection of frames from the {\it  salesman} video sequence. }}{74}{figure.267}}
\newlabel{fig:videoClean}{{30}{74}{A selection of frames from the {\it salesman} video sequence}{figure.267}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces   Noisy frames from the {\it  salesman} video sequence. }}{75}{figure.269}}
\newlabel{fig:videoNoisy}{{31}{75}{Noisy frames from the {\it salesman} video sequence}{figure.269}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces   Denoised frames from the {\it  salesman} video sequence. }}{76}{figure.271}}
\newlabel{fig:videoAniso}{{32}{76}{Denoised frames from the {\it salesman} video sequence}{figure.271}{}}
\bibstyle{abbrvnat}
\bibcite{SALSA}{{1}{2010}{{Afonso et~al.}}{{Afonso, Bioucas-Dias, and Figueiredo}}}
\bibcite{alaiz2013group}{{2}{2013}{{Ala{\i }z et~al.}}{{Ala{\i }z, Barbero, and Dorronsoro}}}
\bibcite{LAPACK}{{3}{1999}{{Anderson et~al.}}{{}}}
\bibcite{bach10}{{4}{2010}{{Bach}}{{}}}
\bibcite{Bach13}{{5}{2013}{{Bach}}{{}}}
\bibcite{bach11}{{6}{2011}{{Bach et~al.}}{{Bach, Jenatton, Mairal, and Obozinski}}}
\bibcite{HAIS07}{{7}{2008}{{Barbero et~al.}}{{Barbero, L{\'o}pez, and Dorronsoro}}}
\bibcite{Barbero11}{{8}{2011}{{Barbero and Sra}}{{}}}
\bibcite{NeurocomputingRejilla}{{9}{2009}{{Barbero et~al.}}{{Barbero, L{\'o}pez, and Dorronsoro}}}
\bibcite{Barlow}{{10}{1972}{{Barlow}}{{}}}
\bibcite{bauCom}{{11}{2011}{{Bauschke and Combettes}}{{}}}
\bibcite{Bauschke04}{{12}{2004}{{Bauschke}}{{}}}
\bibcite{fista}{{13}{2009}{{Beck and Teboulle}}{{}}}
\bibcite{ProjNewton}{{14}{1982}{{Bertsekas}}{{}}}
\bibcite{Bertsekas}{{15}{1999}{{Bertsekas}}{{}}}
\bibcite{TwIST}{{16}{2007}{{Bioucas-Dias and Figueiredo}}{{}}}
\bibcite{BioucasTV06}{{17}{2006}{{Bioucas-Dias et~al.}}{{Bioucas-Dias, Figueiredo, and Oliveira}}}
\bibcite{BM3Dvideos}{{18}{2013}{{BM3D}}{{}}}
\bibcite{L-BFGS-B}{{19}{1994}{{Byrd et~al.}}{{Byrd, Lu, Nocedal, and Zhu}}}
\bibcite{candesTao04}{{20}{2004}{{Cand\IeC {\`e}s and Tao}}{{}}}
\bibcite{chaDar09}{{21}{2009}{{Chambolle and Darbon}}{{}}}
\bibcite{chambollePock}{{22}{2011}{{Chambolle and Pock}}{{}}}
\bibcite{ChambolleErgodic14}{{23}{2014}{{Chambolle and Pock}}{{}}}
\bibcite{ChambolleBlockAccel}{{24}{2015}{{Chambolle and Pock}}{{}}}
\bibcite{atomic}{{25}{2012}{{Chandrasekaran et~al.}}{{Chandrasekaran, Recht, Parrilo, and Willsky}}}
\bibcite{Choksi2DBarcode}{{26}{2010}{{Choksi et~al.}}{{Choksi, van Gennip, and Oberman}}}
\bibcite{ParallelDykstra}{{27}{2009}{{Combettes}}{{}}}
\bibcite{Combettes09}{{28}{2009}{{Combettes and Pesquet}}{{}}}
\bibcite{fastTV}{{29}{2012}{{Condat}}{{}}}
\bibcite{condatGenProx}{{30}{2014}{{Condat}}{{}}}
\bibcite{condatl1}{{31}{2016}{{Condat}}{{}}}
\bibcite{Conn}{{32}{2000}{{Conn et~al.}}{{Conn, Gould, and Toint}}}
\bibcite{DahlTV10}{{33}{2010}{{Dahl et~al.}}{{Dahl, Hansen, Jensen, and Jensen}}}
\bibcite{daviesTautString}{{34}{2001}{{Davies and Kovac}}{{}}}
\bibcite{DuanTV}{{35}{2012}{{Duan and Tai}}{{}}}
\bibcite{esedogluAnisoTV}{{36}{2004}{{Esedoglu and Osher}}{{}}}
\bibcite{frHaHoTi07}{{37}{2007}{{{Friedman} et~al.}}{{{Friedman}, {Hastie}, {H{\"o}fling}, and {Tibshirani}}}}
\bibcite{goldfarb2009parametric}{{38}{2009}{{Goldfarb and Yin}}{{}}}
\bibcite{splitbreg}{{39}{2009}{{Goldstein T.}}{{}}}
\bibcite{Golub99}{{40}{1999}{{Golub et~al.}}{{}}}
\bibcite{grasmairTV07}{{41}{2007}{{Grasmair}}{{}}}
\bibcite{harLev10}{{42}{2010}{{Harchaoui and L{\'e}vy-Leduc}}{{}}}
\bibcite{HuaFS09}{{43}{2009}{{Hua et~al.}}{{Hua, Tembe, and Dougherty}}}
\bibcite{Ito1999}{{44}{1999}{{Ito and Kunisch}}{{}}}
\bibcite{JaggiFW}{{45}{2013}{{Jaggi}}{{}}}
\bibcite{jegBac13}{{46}{2013}{{Jegelka et~al.}}{{Jegelka, Bach, and Sra}}}
\bibcite{INRIAholidays}{{47}{2008}{{Jegou et~al}}{{}}}
\bibcite{dpTV}{{48}{2013}{{Johnson}}{{}}}
\bibcite{KimICML10}{{49}{2010}{{Kim et~al.}}{{Kim, Sra, and Dhillon}}}
\bibcite{boyd.kim}{{50}{2009}{{Kim et~al.}}{{Kim, Koh, Boyd, and Gorinevsky}}}
\bibcite{kiwiel}{{51}{2008}{{Kiwiel}}{{}}}
\bibcite{Knuth97}{{52}{1997}{{Knuth}}{{}}}
\bibcite{kolar}{{53}{2010}{{Kolar et~al.}}{{Kolar, Song, Ahmed, and Xing}}}
\bibcite{Kolmogorov16}{{54}{2015}{{Kolmogorov et~al}}{{}}}
\bibcite{Krishnan09}{{55}{2009}{{Krishnan and Fergus}}{{}}}
\bibcite{Kumar15}{{56}{2015}{{Kumar et~al}}{{Kumar, KS and Barbero, Alvaro and Jegelka, Stefanie and Sra, Suvrit and Bach, Francis}}}
\bibcite{land}{{57}{1997}{{Land and Friedman}}{{}}}
\bibcite{LiL1TV}{{58}{1996}{{Li and Santosa}}{{}}}
\bibcite{TRON}{{59}{1999}{{Lin and Mor{\'e}}}{{}}}
\bibcite{liu09b}{{60}{2009}{{Liu and Zhang}}{{}}}
\bibcite{liu09}{{61}{2009}{{Liu and Ye}}{{}}}
\bibcite{slep}{{62}{2009}{{Liu et~al.}}{{Liu, Ji, and Ye}}}
\bibcite{liuYe10}{{63}{2010}{{Liu et~al.}}{{Liu, Yuan, and Ye}}}
\bibcite{Mairal10}{{64}{2010}{{Mairal et~al.}}{{Mairal, Jenatton, Obozinski, and Bach}}}
\bibcite{martinet70}{{65}{1970}{{Martinet}}{{}}}
\bibcite{Meier08}{{66}{2008}{{Meier et~al.}}{{Meier, {van de Geer}, and B{\"u}hlmann}}}
\bibcite{More83}{{67}{1983}{{Mor{\'e} and Sorensen}}{{}}}
\bibcite{moreau62}{{68}{1962}{{Moreau}}{{}}}
\bibcite{nest07}{{69}{2007}{{Nesterov}}{{}}}
\bibcite{NumOptNocedal}{{70}{2000}{{Nocedal and Wright}}{{}}}
\bibcite{parikh2014proximal}{{71}{2014}{{Parikh et~al.}}{{Parikh, Boyd, et~al.}}}
\bibcite{pierra84}{{72}{1984}{{Pierra}}{{}}}
\bibcite{pontow09}{{73}{2009}{{Pontow and Scherzer}}{{}}}
\bibcite{ramdas}{{74}{2014}{{Ramdas and Tibshirani}}{{}}}
\bibcite{cghsvm}{{75}{2008}{{Rapaport and Vert}}{{}}}
\bibcite{rinaldo}{{76}{2009}{{Rinaldo}}{{}}}
\bibcite{rock76}{{77}{1976}{{Rockafellar}}{{}}}
\bibcite{RogersOvarian05}{{78}{2005}{{Rogers et~al.}}{{Rogers, Girolami, Campbell, and Breitling}}}
\bibcite{RudinTV92}{{79}{1992}{{Rudin et~al.}}{{Rudin, Osher, and Fatemi}}}
\bibcite{Imagenet}{{80}{2015}{{Russakovsky et~al}}{{}}}
\bibcite{salzo}{{81}{2012}{{Salzo and Villa}}{{}}}
\bibcite{schmidt11}{{82}{2011}{{Schmidt et~al.}}{{Schmidt, Roux, and Bach}}}
\bibcite{nocops}{{83}{2012}{{Sra}}{{}}}
\bibcite{sraBook}{{84}{2011}{{Sra et~al.}}{{Sra, Nowozin, and Wright}}}
\bibcite{SteidlTautString}{{85}{2005}{{Steidl et~al.}}{{Steidl, Didas, and Neumann}}}
\bibcite{steidl2009}{{86}{2009}{{Steidl and Teuber }}{{}}}
\bibcite{Stransky06}{{87}{2006}{{Stransky et~al.}}{{}}}
\bibcite{LASSO}{{88}{1996}{{Tibshirani}}{{}}}
\bibcite{cgh}{{89}{2008}{{Tibshirani and Wang}}{{}}}
\bibcite{fl}{{90}{2005}{{Tibshirani et~al.}}{{Tibshirani, Saunders, Rosset, Zhu, and Knight}}}
\bibcite{trendf}{{91}{2014}{{Tibshirani}}{{}}}
\bibcite{Alon99}{{92}{1999}{{{U. Alon} et~al.}}{{}}}
\bibcite{vert}{{93}{2010}{{Vert and Bleakley}}{{}}}
\bibcite{vogel}{{94}{1996}{{Vogel and Oman}}{{}}}
\bibcite{BoydTV}{{95}{2012}{{Wahlberg et~al.}}{{Wahlberg, Boyd, Annergren, and Wang}}}
\bibcite{WangTV14}{{96}{2014}{{Jie Wang et~al.}}{{}}}
\bibcite{sparsa}{{97}{2009}{{Wright et~al.}}{{Wright, Nowak, and Figueiredo}}}
\bibcite{wytock}{{98}{2014}{{Wytock et~al.}}{{Wytock, Sra, and Kolter}}}
\bibcite{YangTV}{{99}{2013}{{Yang et~al.}}{{Yang, Wang, Fan, Zhang, Wonka, and Ye}}}
\bibcite{yaoLiang}{{100}{2013}{{Yu}}{{}}}
\bibcite{yuan06}{{101}{2006}{{Yuan and Lin}}{{}}}
\bibcite{ZhuTVAlg08}{{102}{2008}{{Zhu and Chan}}{{}}}
\newlabel{LastPage}{{}{82}{}{page.82}{}}
\xdef\lastpage@lastpage{82}
\xdef\lastpage@lastpageHy{82}
