\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{togneri2011overview}
\citation{stamatatos2014overview}
\citation{duygulu2002object,deng2010does,oquab2014learning}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:recog_tasks}{{1}{1}{}{section.1}{}}
\citation{oquab2014learning,griffin2007caltech}
\citation{griffin2007caltech}
\citation{togneri2011overview,stamatatos2014overview}
\newlabel{eq:ga_k}{{1}{2}{}{equation.2}{}}
\citation{Kay2008a}
\citation{pan2010survey}
\citation{pan2010survey}
\citation{oquab2014learning}
\citation{donahue2014decaf}
\citation{sharif2014cnn}
\citation{crammer2001algorithmic,lee2004multicategory,weston1999support}
\citation{gupta2014training}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related Work}{3}{subsection.3}}
\citation{Shannon1948}
\citation{pan2016ultrahigh,abramovich2015feature,davis2011bayesian}
\citation{Kay2008a}
\@writefile{toc}{\contentsline {section}{\numberline {2}Randomized Classification}{4}{section.4}}
\newlabel{sec:rc_motivation}{{2}{4}{}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Average generalization accuracy:} A diagram of the random quantities underlying the average generalization accuracy for $k$ labels ($\text  {AGA}_k$). At the training stage (left), a set of $k$ labels $\mathcal  {S}$ is sampled from the prior $\pi $, and score functions are trained from examples for these classes. At the test stage (right), one true class $Y^*$ is sampled uniformly from $\mathcal  {S}$, as well as a test example $X^*$. $\text  {AGA}_k$ measures the expected accuracy over these random variables.}}{6}{figure.6}}
\newlabel{fig:average_risk}{{1}{6}{\textbf {Average generalization accuracy:} A diagram of the random quantities underlying the average generalization accuracy for $k$ labels ($\text {AGA}_k$). At the training stage (left), a set of $k$ labels $\mathcal {S}$ is sampled from the prior $\pi $, and score functions are trained from examples for these classes. At the test stage (right), one true class $Y^*$ is sampled uniformly from $\mathcal {S}$, as well as a test example $X^*$. $\text {AGA}_k$ measures the expected accuracy over these random variables}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Marginal Classifier}{6}{subsection.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Classification rule:} Top: Score functions for three classes in a one-dimensional example space. Bottom: The classification rule chooses between $y^{(1)},y^{(2)}$ or $y^{(3)}$ by choosing the maximal score function. }}{7}{figure.10}}
\newlabel{fig:classification_rule}{{2}{7}{\textbf {Classification rule:} Top: Score functions for three classes in a one-dimensional example space. Bottom: The classification rule chooses between $y^{(1)},y^{(2)}$ or $y^{(3)}$ by choosing the maximal score function}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Examples of Marginal Classifiers}{7}{subsection.11}}
\citation{pereira2018toward}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Estimation of Average Accuracy}{8}{subsection.13}}
\newlabel{sec:estimation_average_accuracy}{{2.3}{8}{}{subsection.13}{}}
\newlabel{eq:test_risk}{{3}{9}{}{equation.14}{}}
\newlabel{eq:avtestrisk}{{4}{9}{}{equation.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Toy Example: Bivariate Normal}{9}{subsection.16}}
\newlabel{sec:toyExA}{{2.4}{9}{}{subsection.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Toy example:} \emph  {Left:} The joint distribution of $(X, Y)$ is bivariate normal with correlation $\rho = 0.7$. \emph  {Right:} A typical classification problem instance from the bivariate normal model with $k = 3$ classes. \emph  {(Top):} the conditional density of $X$ given label $Y$, for $Y = \{y^{(1)}, y^{(2)}, y^{(3)}\}$. \emph  {(Bottom):} the Bayes classification regions for the three classes.}}{10}{figure.17}}
\newlabel{fig:toy1}{{3}{10}{\textbf {Toy example:} \emph {Left:} The joint distribution of $(X, Y)$ is bivariate normal with correlation $\rho = 0.7$. \emph {Right:} A typical classification problem instance from the bivariate normal model with $k = 3$ classes. \emph {(Top):} the conditional density of $X$ given label $Y$, for $Y = \{y^{(1)}, y^{(2)}, y^{(3)}\}$. \emph {(Bottom):} the Bayes classification regions for the three classes}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Generalization accuracy for toy example:} The distribution of the generalization accuracy for $k = 2,3,\mathinner {\ldotp \ldotp \ldotp }, 10$ for the bivariate normal model with $\rho = 0.7$. Circles indicate the average generalization accuracy $\text  {AGA}_k$; the red curve is the theoretically computed average accuracy.}}{10}{figure.18}}
\newlabel{fig:toy2}{{4}{10}{\textbf {Generalization accuracy for toy example:} The distribution of the generalization accuracy for $k = 2,3,\hdots , 10$ for the bivariate normal model with $\rho = 0.7$. Circles indicate the average generalization accuracy $\text {AGA}_k$; the red curve is the theoretically computed average accuracy}{figure.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Extrapolation}{11}{section.19}}
\newlabel{sec:extrapolation}{{3}{11}{}{section.19}{}}
\newlabel{theorem:avrisk_identity}{{3}{11}{}{theorem.21}{}}
\newlabel{eq:avrisk_identity}{{5}{11}{}{equation.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Analysis of Average Accuracy}{11}{subsection.23}}
\newlabel{eq:U_function}{{6}{12}{}{equation.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Incorrect-Label Favorability}{12}{subsubsection.25}}
\newlabel{lemma:U_function}{{4}{12}{}{theorem.27}{}}
\newlabel{eq:Uniform}{{8}{12}{}{equation.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Correct-Label Favorability}{13}{subsubsection.29}}
\newlabel{eq:umax_beta}{{10}{14}{}{equation.31}{}}
\newlabel{eq:lala}{{15}{14}{}{equation.36}{}}
\newlabel{eq:Kbar}{{16}{14}{}{equation.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Favorability and Average Accuracy for the Toy Example}{14}{subsection.38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Estimation}{15}{subsection.41}}
\newlabel{sec:extrapolation_estimation}{{3.3}{15}{}{subsection.41}{}}
\newlabel{eq:linearKu}{{17}{15}{}{equation.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Favorability for toy example:} \emph  {Left:} The level curves of the function $U_{x^*}(M_y)$ in the bivariate normal model with $\rho = 0.7$. \emph  {Right:} The function ${D}(u)$ gives the cumulative distribution function of the random variable $U_{X^*}(M_Y)$.}}{16}{figure.39}}
\newlabel{fig:toy3}{{5}{16}{\textbf {Favorability for toy example:} \emph {Left:} The level curves of the function $U_{x^*}(M_y)$ in the bivariate normal model with $\rho = 0.7$. \emph {Right:} The function ${D}(u)$ gives the cumulative distribution function of the random variable $U_{X^*}(M_Y)$}{figure.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Average accuracy with different $\rho $'s:} \emph  {Left:} The average accuracy $\text  {AGA}_k$. \emph  {Right:} ${D}(u)$ function for the bivariate normal model with $\rho \in \{0.3, 0.5, 0.7, 0.9\}$. }}{16}{figure.40}}
\newlabel{fig:toy4}{{6}{16}{\textbf {Average accuracy with different $\rho $'s:} \emph {Left:} The average accuracy $\text {AGA}_k$. \emph {Right:} ${D}(u)$ function for the bivariate normal model with $\rho \in \{0.3, 0.5, 0.7, 0.9\}$}{figure.40}{}}
\newlabel{eq:avrisk_linear}{{20}{17}{}{equation.45}{}}
\newlabel{eq:avrisk_hat}{{22}{17}{}{equation.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Model Selection}{17}{subsection.48}}
\newlabel{sec:modelselection}{{3.4}{17}{}{subsection.48}{}}
\citation{Kay2008a}
\citation{Kay2008a}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}The Kay KDE-based estimator}{18}{subsection.57}}
\newlabel{sec:KDEcomparison}{{3.5}{18}{}{subsection.57}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Simulation Study}{19}{section.61}}
\newlabel{sec:simulation_study}{{4}{19}{}{section.61}{}}
\citation{cao1994comparative}
\citation{Kay2008a}
\citation{Scott1992}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Simulation Results}{20}{subsection.62}}
\citation{LFWTech}
\citation{amos2016openface}
\citation{amos2016openface}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Maximum RMSE (se) across all signal-to-noise-levels in predicting $\text  {TA}_{k_2}$ from $k_1$ classes in multivariate Gaussian simulation. Standard errors were computed by nesting the maximum operation within the bootstrap, to properly account for the variance of a maximum of estimated means.}}{21}{table.63}}
\newlabel{tab:sim_max_error}{{1}{21}{Maximum RMSE (se) across all signal-to-noise-levels in predicting $\text {TA}_{k_2}$ from $k_1$ classes in multivariate Gaussian simulation. Standard errors were computed by nesting the maximum operation within the bootstrap, to properly account for the variance of a maximum of estimated means}{table.63}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Evaluation}{21}{section.66}}
\newlabel{sec:extrapolation_example}{{5}{21}{}{section.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Simulation results (RMSE):} Simulation study consisting of multivariate Gaussian $Y$ with nearest neighbor classifier. Prediction RMSE vs true $k_2$-class accuracy for ClassExReg with radial basis (\textsf  {ClassExReg}), KDE-based methods with biased cross-validation (\textsf  {KDE\_BCV}) and unbiased cross-validation (\textsf  {KDE\_UCV}).}}{22}{figure.64}}
\newlabel{fig:sim_study}{{7}{22}{\textbf {Simulation results (RMSE):} Simulation study consisting of multivariate Gaussian $Y$ with nearest neighbor classifier. Prediction RMSE vs true $k_2$-class accuracy for ClassExReg with radial basis (\textsf {ClassExReg}), KDE-based methods with biased cross-validation (\textsf {KDE\_BCV}) and unbiased cross-validation (\textsf {KDE\_UCV})}{figure.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Simulation results (biases):} Simulation study consisting of multivariate Gaussian $Y$ with nearest neighbor classifier. Bias (mean predicted minus true accuracy) vs true $k_2$-class accuracy for ClassExReg with radial basis (\textsf  {ClassExReg}), KDE-based methods with biased cross-validation (\textsf  {KDE\_BCV}) and unbiased cross-validation (\textsf  {KDE\_UCV}).}}{23}{figure.65}}
\newlabel{fig:sim_study_bias}{{8}{23}{\textbf {Simulation results (biases):} Simulation study consisting of multivariate Gaussian $Y$ with nearest neighbor classifier. Bias (mean predicted minus true accuracy) vs true $k_2$-class accuracy for ClassExReg with radial basis (\textsf {ClassExReg}), KDE-based methods with biased cross-validation (\textsf {KDE\_BCV}) and unbiased cross-validation (\textsf {KDE\_UCV})}{figure.65}{}}
\citation{achanta2015telugu}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Face recognition setup (top):} Examples of labels and features from the \emph  {Labeled Faces in the Wild} data set. \textbf  {Telugu OCR (bottom)}: exemplars from six of the glyph classes, along with intermediate features and final transformations from the deep convolutional network.}}{24}{figure.68}}
\newlabel{fig:face_rec}{{9}{24}{\textbf {Face recognition setup (top):} Examples of labels and features from the \emph {Labeled Faces in the Wild} data set. \textbf {Telugu OCR (bottom)}: exemplars from six of the glyph classes, along with intermediate features and final transformations from the deep convolutional network}{figure.68}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Face-recognition extrapolation RMSEs}: RMSE (se) on predicting $\text  {TA}_{1672}$ from $k_1$ classes}}{25}{table.75}}
\newlabel{tab:lfw_accuracy}{{2}{25}{\textbf {Face-recognition extrapolation RMSEs}: RMSE (se) on predicting $\text {TA}_{1672}$ from $k_1$ classes}{table.75}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Results}{25}{subsection.73}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Predicted accuracy curves for face-recognition example}: The plots show predicted accuracies. Each red curve represents the predicted accuracies using a single subsample of size $k_1$. The black curve shows the average test accuracy obtained from the full data set. }}{26}{figure.74}}
\newlabel{fig:lfw_extrapolation2}{{10}{26}{\textbf {Predicted accuracy curves for face-recognition example}: The plots show predicted accuracies. Each red curve represents the predicted accuracies using a single subsample of size $k_1$. The black curve shows the average test accuracy obtained from the full data set}{figure.74}{}}
\citation{wong2010optional}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Telugu OCR extrapolated accuracies}: Extrapolating from $k_1$ to $k_2$ classes in Telugu OCR for three different classifiers: logistic regression, support vector machine, and deep convolutional network}}{27}{table.76}}
\newlabel{tab:tel_accuracy}{{3}{27}{\textbf {Telugu OCR extrapolated accuracies}: Extrapolating from $k_1$ to $k_2$ classes in Telugu OCR for three different classifiers: logistic regression, support vector machine, and deep convolutional network}{table.76}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{27}{section.77}}
\newlabel{sec:discussion}{{6}{27}{}{section.77}{}}
\bibdata{example}
\bibcite{abramovich2015feature}{{1}{2015}{{Abramovich and Pensky}}{{}}}
\bibcite{achanta2015telugu}{{2}{2015}{{Achanta and Hastie}}{{}}}
\bibcite{amos2016openface}{{3}{2016}{{Amos et~al.}}{{Amos, Ludwiczuk, and Satyanarayanan}}}
\bibcite{cao1994comparative}{{4}{1994}{{Cao et~al.}}{{Cao, Cuevas, and Manteiga}}}
\bibcite{crammer2001algorithmic}{{5}{2001}{{Crammer and Singer}}{{}}}
\bibcite{davis2011bayesian}{{6}{2011}{{Davis et~al.}}{{Davis, Pensky, and Crampton}}}
\bibcite{deng2010does}{{7}{2010}{{Deng et~al.}}{{Deng, Berg, Li, and Fei-Fei}}}
\bibcite{donahue2014decaf}{{8}{2014}{{Donahue et~al.}}{{Donahue, Jia, Vinyals, Hoffman, Zhang, Tzeng, and Darrell}}}
\bibcite{duygulu2002object}{{9}{2002}{{Duygulu et~al.}}{{Duygulu, Barnard, de~Freitas, and Forsyth}}}
\bibcite{griffin2007caltech}{{10}{2007}{{Griffin et~al.}}{{Griffin, Holub, and Perona}}}
\bibcite{gupta2014training}{{11}{2014}{{Gupta et~al.}}{{Gupta, Bengio, and Weston}}}
\bibcite{LFWTech}{{12}{2007}{{Huang et~al.}}{{Huang, Ramesh, Berg, and Learned-Miller}}}
\bibcite{Kay2008a}{{13}{2008}{{Kay et~al.}}{{Kay, Naselaris, Prenger, and Gallant}}}
\bibcite{lee2004multicategory}{{14}{2004}{{Lee et~al.}}{{Lee, Lin, and Wahba}}}
\bibcite{oquab2014learning}{{15}{2014}{{Oquab et~al.}}{{Oquab, Bottou, Laptev, and Sivic}}}
\bibcite{pan2016ultrahigh}{{16}{2016}{{Pan et~al.}}{{Pan, Wang, and Li}}}
\bibcite{pan2010survey}{{17}{2010}{{Pan and Yang}}{{}}}
\bibcite{pereira2018toward}{{18}{2018}{{Pereira et~al.}}{{Pereira, Lou, Pritchett, Ritter, Gershman, Kanwisher, Botvinick, and Fedorenko}}}
\bibcite{Scott1992}{{19}{1992}{{Scott}}{{}}}
\bibcite{Shannon1948}{{20}{1948}{{Shannon}}{{}}}
\bibcite{sharif2014cnn}{{21}{2014}{{Sharif~Razavian et~al.}}{{Sharif~Razavian, Azizpour, Sullivan, and Carlsson}}}
\bibcite{stamatatos2014overview}{{22}{2014}{{Stamatatos et~al.}}{{Stamatatos, Daelemans, Verhoeven, Juola, L{\'o}pez-L{\'o}pez, Potthast, and Stein}}}
\bibcite{togneri2011overview}{{23}{2011}{{Togneri and Pullella}}{{}}}
\bibcite{weston1999support}{{24}{1999}{{Weston and Watkins}}{{}}}
\bibcite{wong2010optional}{{25}{2010}{{Wong and Ma}}{{}}}
\newlabel{LastPage}{{}{30}{}{page.30}{}}
\xdef\lastpage@lastpage{30}
\xdef\lastpage@lastpageHy{30}
