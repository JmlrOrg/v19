\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{eSVM,ROSS}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Lanckriet2003,Honorio14}
\citation{Lanckriet2003}
\citation{Vapnik00,Zhang02,Bartlett03,Bousquet04,Kakade08}
\citation{OsadchyKF12,OsadchyKR16}
\citation{Klivans09,DanielyLS14}
\citation{Honorio14}
\citation{fromNtoN+1}
\citation{fromNtoN+1}
\@writefile{toc}{\contentsline {section}{\numberline {2}K-Hyperplane Hinge-Minimax Classifier}{3}{section.3}}
\newlabel{eq_non_linear_classifier}{{1}{3}{}{equation.5}{}}
\citation{marshall1960}
\citation{marshall1960}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Mixed Risk for K-Hyperplane Model}{4}{subsection.6}}
\newlabel{KHHM}{{2.1}{4}{}{subsection.6}{}}
\newlabel{def_hinge_risk}{{2}{4}{}{equation.8}{}}
\newlabel{eq_risk}{{4}{4}{}{equation.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}The Expected Risk of the Negative Class}{4}{subsubsection.14}}
\newlabel{subsection_k_minimax_risk}{{2.1.1}{4}{}{subsubsection.14}{}}
\citation{marshall1960}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A 2D illustrative example of Theorem \ref  {th_risk}. $z^*$ is the closest point to the mean of the negative distribution.}}{5}{figure.17}}
\newlabel{ex_il}{{2.1.1}{5}{}{figure.17}{}}
\newlabel{th_risk}{{1}{5}{}{theorem.15}{}}
\citation{OsadchyKR16}
\citation{OsadchyKR16}
\citation{OsadchyKR16}
\newlabel{eq_dual_func}{{5}{6}{}{equation.18}{}}
\newlabel{req_lambda_opt}{{6}{6}{}{equation.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}K-Hyperplane Hinge-Minimax (KHHM) Training}{6}{subsection.20}}
\newlabel{KHHM_Alg}{{2.2}{6}{}{subsection.20}{}}
\newlabel{eq_emp_risk}{{7}{6}{}{equation.21}{}}
\newlabel{linear_opt}{{8}{6}{}{equation.22}{}}
\citation{OsadchyKR16}
\citation{OsadchyKR16}
\citation{OsadchyKR16}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces KHHM Training}}{7}{algorithm.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Generalization Bound for KHHM Model}{8}{subsection.30}}
\newlabel{sec:khhm_bond}{{2.3}{8}{}{subsection.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Uniform generalization bound for the empirical background risk}{8}{subsubsection.31}}
\citation{PST}
\newlabel{eq:Delta}{{9}{9}{}{equation.32}{}}
\newlabel{eq_D1}{{10}{9}{}{equation.33}{}}
\newlabel{eq_D2}{{11}{9}{}{equation.34}{}}
\newlabel{lemma:d1}{{3}{9}{}{theorem.35}{}}
\newlabel{eq:U_bound}{{12}{9}{}{equation.36}{}}
\newlabel{e*}{{13}{9}{}{equation.37}{}}
\newlabel{e**}{{14}{9}{}{equation.38}{}}
\newlabel{e***}{{15}{9}{}{equation.39}{}}
\citation{Candes11}
\citation{Candes11}
\newlabel{lemma:d2}{{4}{10}{}{theorem.40}{}}
\citation{Candes11}
\citation{Candes11}
\newlabel{lemma:d3}{{5}{11}{}{theorem.43}{}}
\newlabel{theorem:minimax1}{{6}{12}{}{theorem.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Uniform generalization bound for the empirical risk of the hinge-loss}{12}{subsubsection.45}}
\citation{ledoux1991probability}
\citation{Bartlett03}
\newlabel{th_10}{{8}{13}{}{theorem.48}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Latent Hinge-Minimax Classifier}{13}{section.49}}
\newlabel{sec:LHM}{{3}{13}{}{section.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Schematic comparison of KHHM (left) and LHM (right) classifiers on a non-convex positive class. The LHM classifier iteratively discovers a partition of the positive set into convex components and builds KHHM model for each convex component.}}{14}{figure.51}}
\newlabel{fig:khhm_vs_lhm}{{2}{14}{Schematic comparison of KHHM (left) and LHM (right) classifiers on a non-convex positive class. The LHM classifier iteratively discovers a partition of the positive set into convex components and builds KHHM model for each convex component}{figure.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Expected Latent Mixed Risk}{14}{subsection.52}}
\newlabel{eq:lhm_risk}{{17}{14}{}{equation.53}{}}
\newlabel{eq:minimax_expected}{{18}{14}{}{equation.54}{}}
\newlabel{eq_expected_hinge_inLHM}{{19}{14}{}{equation.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Empirical Latent Mixed Risk}{14}{subsection.56}}
\newlabel{eq:emp_risk}{{20}{14}{}{equation.57}{}}
\newlabel{eq:component_loss}{{23}{15}{}{equation.60}{}}
\newlabel{sample:loss}{{24}{15}{}{equation.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}LHM Training Algorithm}{15}{subsection.62}}
\newlabel{sec:LHM:training}{{3.3}{15}{}{subsection.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The orange ellipses represent the negative distribution $Z(\mathaccentV {hat}05E{\mu },\mathaccentV {hat}05E{\Sigma })$, the red triangles corresponds to $Q^i_x$ and the blue ones to $Q^i$. \textbf  {Left}: $w^i_1, w^i_2$ are moved to pass through $x$, causing the probability $Q^i$ to increase. \textbf  {Right}:$w^i_*$ is moved to pass through $x$, causing the probability $Q^i$ to decrease.}}{16}{figure.65}}
\newlabel{fig:wx_demo}{{3}{16}{The orange ellipses represent the negative distribution $Z(\hat {\mu },\hat {\Sigma })$, the red triangles corresponds to $Q^i_x$ and the blue ones to $Q^i$. \textbf {Left}: $w^i_1, w^i_2$ are moved to pass through $x$, causing the probability $Q^i$ to increase. \textbf {Right}:$w^i_*$ is moved to pass through $x$, causing the probability $Q^i$ to decrease}{figure.65}{}}
\newlabel{eq:argmin}{{25}{16}{}{equation.63}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces LHM Training. $T$ is the threshold on the empirical risk change.}}{17}{algorithm.66}}
\newlabel{alg:lhm}{{2}{17}{}{algorithm.66}{}}
\newlabel{eq:9}{{29}{17}{}{equation.76}{}}
\newlabel{eq:10}{{30}{17}{}{equation.77}{}}
\newlabel{eq:11}{{31}{17}{}{equation.78}{}}
\newlabel{eq:12}{{32}{17}{}{equation.79}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Generalization Bound for LHM Model with Fixed Assignment}{17}{subsection.81}}
\newlabel{eq:21}{{34}{18}{}{equation.83}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Mapping LHM Classifier to a Neural Network}{18}{section.85}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An example of NN equivalent to LHM for two components and three hyperplanes in each.}}{19}{figure.88}}
\newlabel{fig:LHM_NN}{{4}{19}{An example of NN equivalent to LHM for two components and three hyperplanes in each}{figure.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Binary NN}{19}{subsection.86}}
\newlabel{sec:binary_nn}{{4.1}{19}{}{subsection.86}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Multi-Class NN}{19}{subsection.89}}
\newlabel{sec:mc_nn}{{4.2}{19}{}{subsection.89}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{20}{section.90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}K-Hyperplane Hinge-Minimax Classifier}{20}{subsection.91}}
\newlabel{sec:exp:KHHM}{{5.1}{20}{}{subsection.91}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Synthetic Data Example}{20}{subsubsection.96}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces AUC for different size partitions of positive and negative classes}}{20}{table.98}}
\newlabel{TAB:AUC}{{1}{20}{AUC for different size partitions of positive and negative classes}{table.98}{}}
\citation{UCIrep}
\citation{SUN}
\citation{SUN}
\citation{RUSboost}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Illustration of KHHM classifier construction on a toy example.The first 5 figures show the greedy initial step. The last figure shows the final classifier after 25 iterations. The contour lines show the covariance matrix of the negative distribution inside the intersection of hyperplane, which is used to find the optimal hyperplane, depicted in black.}}{21}{figure.93}}
\newlabel{fig_toy_ex}{{5}{21}{Illustration of KHHM classifier construction on a toy example.The first 5 figures show the greedy initial step. The last figure shows the final classifier after 25 iterations. The contour lines show the covariance matrix of the negative distribution inside the intersection of hyperplane, which is used to find the optimal hyperplane, depicted in black}{figure.93}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Letter Recognition}{21}{subsubsection.99}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Letter experiments. $K$ corresponds to the number of hyperplanes used in the hinge-minimax classifier. The times are in sec. AdaBoost uses 100 decision trees.}}{22}{table.101}}
\newlabel{letters}{{2}{22}{Letter experiments. $K$ corresponds to the number of hyperplanes used in the hinge-minimax classifier. The times are in sec. AdaBoost uses 100 decision trees}{table.101}{}}
\newlabel{SUN300}{{5.1.3}{22}{}{subsubsection.102}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Scene classification with 300 dim. features. The classification time of RBF kernel SVM is very high, since it chooses about 15,000 SVs from 19850 training examples. The RUSBoost uses 100 decision trees.}}{22}{table.105}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Large Scale Scene Recognition}{22}{subsubsection.102}}
\newlabel{sub:SUN}{{5.1.3}{22}{}{subsubsection.102}{}}
\citation{Everingham10}
\citation{Deva}
\citation{LDA}
\citation{eSVM}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces ROCs of the first three categories of the SUN data set, represented by a spatial pyramid of BOWs, obtained from dense HOG. The solid lines correspond to the hinge-minimax classifier, dotted lines correspond to the histogram intersection kernel SVM.}}{23}{figure.107}}
\newlabel{SUN_fullD}{{6}{23}{ROCs of the first three categories of the SUN data set, represented by a spatial pyramid of BOWs, obtained from dense HOG. The solid lines correspond to the hinge-minimax classifier, dotted lines correspond to the histogram intersection kernel SVM}{figure.107}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A qualitative comparison of the latent hinge minimax classifier (on the left) to the union of LDA classifiers (on the right).}}{23}{figure.110}}
\newlabel{LDAvsLHM_toy}{{7}{23}{A qualitative comparison of the latent hinge minimax classifier (on the left) to the union of LDA classifiers (on the right)}{figure.110}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Latent Hinge Minimax Classifier}{23}{subsection.108}}
\newlabel{sec:exp:LHM}{{5.2}{23}{}{subsection.108}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Synthetic Data}{23}{subsubsection.113}}
\newlabel{sec:synthetic_ex}{{5.2.1}{23}{}{subsubsection.113}{}}
\citation{Everingham10}
\citation{HOG}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces First four iterations of the LHM training on toy example and the corresponding loss convergence. }}{24}{figure.112}}
\newlabel{fig:2d_example2}{{8}{24}{First four iterations of the LHM training on toy example and the corresponding loss convergence}{figure.112}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Comparison of the LHM classifier to the equivalent NN for a varying number of hidden components (from 2 to 5) on PASCAL VOC 2007. The points above the diagonal line show the advantage of LHM classifier.}}{24}{figure.116}}
\newlabel{fig:VOC}{{9}{24}{Comparison of the LHM classifier to the equivalent NN for a varying number of hidden components (from 2 to 5) on PASCAL VOC 2007. The points above the diagonal line show the advantage of LHM classifier}{figure.116}{}}
\citation{Deva,OsadchyKF12}
\citation{imagenet_cvpr09}
\citation{vedaldi15matconvnet}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The table reports the accuracy at the EER point averaged over 20 classes and different hidden partitions (except for KHHM) on PASCAL VOC-2007 classification task using 80-dimensional HOG features.}}{25}{table.119}}
\newlabel{tab:VOC}{{4}{25}{The table reports the accuracy at the EER point averaged over 20 classes and different hidden partitions (except for KHHM) on PASCAL VOC-2007 classification task using 80-dimensional HOG features}{table.119}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Ensembles of Hyperplanes}{25}{subsubsection.117}}
\newlabel{sec:voc_ex}{{5.2.2}{25}{}{subsubsection.117}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Hinge-Minimax Training in Deep Architecture}{25}{subsection.122}}
\newlabel{sec:cifar_ex}{{5.3}{25}{}{subsection.122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Binary imbalanced classification: left -- the ``best-case'' transfer learning setting, right -- the ``worst-case'' transfer learning setting.}}{26}{figure.121}}
\newlabel{fig:binary}{{10}{26}{Binary imbalanced classification: left -- the ``best-case'' transfer learning setting, right -- the ``worst-case'' transfer learning setting}{figure.121}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Binary Imbalanced Setting}{26}{subsubsection.125}}
\citation{vedaldi15matconvnet}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Multi-class classification: left -- the ``best-case'' transfer learning setting, right -- ``worst-case'' transfer learning setting.}}{27}{figure.124}}
\newlabel{fig:mc}{{11}{27}{Multi-class classification: left -- the ``best-case'' transfer learning setting, right -- ``worst-case'' transfer learning setting}{figure.124}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Multi-Class Setting}{27}{subsubsection.126}}
\citation{Kontorovich18}
\bibdata{hinge_minimax}
\bibcite{Bartlett03}{{1}{2003}{{Bartlett and Mendelson}}{{}}}
\bibcite{PST}{{2}{1970}{{Bellman}}{{}}}
\bibcite{Bousquet04}{{3}{2004}{{Bousquet et~al.}}{{Bousquet, Boucheron, and Lugosi}}}
\bibcite{Candes11}{{4}{2011}{{Candes and Plan}}{{}}}
\bibcite{HOG}{{5}{2005}{{Dalal and Triggs}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Training Efficiency}{28}{section.127}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions and Future Work}{28}{section.128}}
\bibcite{DanielyLS14}{{6}{2014}{{Daniely et~al.}}{{Daniely, Linial, and Shalev{-}Shwartz}}}
\bibcite{imagenet_cvpr09}{{7}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{Everingham10}{{8}{2010}{{Everingham et~al.}}{{Everingham, Van~Gool, Williams, Winn, and Zisserman}}}
\bibcite{ROSS}{{9}{2014}{{Girshick et~al.}}{{Girshick, Donahue, Darrell, and Malik}}}
\bibcite{Deva}{{10}{2012}{{Hariharan et~al.}}{{Hariharan, Malik, and Ramanan}}}
\bibcite{LDA}{{11}{2001}{{Hastie et~al.}}{{Hastie, Tibshirani, and Friedman}}}
\bibcite{Honorio14}{{12}{2014}{{Honorio and Jaakkola}}{{}}}
\bibcite{Kakade08}{{13}{2008}{{Kakade et~al.}}{{Kakade, Sridharan, and Tewari}}}
\bibcite{Klivans09}{{14}{2009}{{Klivans and Sherstov}}{{}}}
\bibcite{Kontorovich18}{{15}{2018}{{Kontorovich}}{{}}}
\bibcite{fromNtoN+1}{{16}{2013}{{Kuzborskij et~al.}}{{Kuzborskij, Orabona, and Caputo}}}
\bibcite{Lanckriet2003}{{17}{2003}{{Lanckriet et~al.}}{{Lanckriet, Ghaoui, Bhattacharyya, and Jordan}}}
\bibcite{ledoux1991probability}{{18}{1991}{{Ledoux and Talagrand}}{{}}}
\bibcite{eSVM}{{19}{2011}{{Malisiewicz et~al.}}{{Malisiewicz, Gupta, and Efros}}}
\bibcite{marshall1960}{{20}{1960}{{Marshall and Olkin}}{{}}}
\bibcite{UCIrep}{{21}{1994}{{Murphy and Aha}}{{}}}
\bibcite{OsadchyKF12}{{22}{2012}{{Osadchy et~al.}}{{Osadchy, Keren, and Fadida-Specktor}}}
\bibcite{OsadchyKR16}{{23}{2016}{{Osadchy et~al.}}{{Osadchy, Keren, and Raviv}}}
\bibcite{RUSboost}{{24}{2008}{{Seiffert et~al.}}{{Seiffert, Khoshgoftaar, Hulse, and Napolitano}}}
\bibcite{Vapnik00}{{25}{2000}{{Vapnik}}{{}}}
\bibcite{vedaldi15matconvnet}{{26}{2015}{{Vedaldi and Lenc}}{{}}}
\bibcite{SUN}{{27}{2010}{{Xiao et~al.}}{{Xiao, Hays, Ehinger, Oliva, and Torralba}}}
\bibcite{Zhang02}{{28}{2002}{{Zhang}}{{}}}
\newlabel{LastPage}{{}{30}{}{page.30}{}}
\xdef\lastpage@lastpage{30}
\xdef\lastpage@lastpageHy{30}
