\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{Candes09a}
\citation{Wright09a}
\citation{Zhong15a}
\citation{Koren09a}
\citation{Hsieh12a}
\citation{Wright09a}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\citation{Koren09a}
\citation{Nowell07a}
\citation{Jain13b}
\citation{Chiang15a}
\citation{Chiang16a}
\citation{Candes11a}
\citation{Candes11a}
\@writefile{toc}{\contentsline {section}{\numberline {2}Exploiting Side Information for Learning Low-Rank Matrices}{5}{section.3}}
\newlabel{sec:main}{{2}{5}{}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Learning from Missing and Corrupted Observations}{5}{subsection.4}}
\newlabel{subsec:intro}{{2.1}{5}{}{subsection.4}{}}
\newlabel{eq:mc_rpca}{{1}{5}{}{equation.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Idealized Case: Perfect Side Information}{5}{subsection.6}}
\newlabel{subsec:perfect}{{2.2}{5}{}{subsection.6}{}}
\citation{Jain13b,Xu13a,Zhong15a}
\newlabel{eq:perfect_feature}{{2}{6}{Perfect side information}{equation.8}{}}
\newlabel{eq:imc_rpca}{{3}{6}{}{equation.9}{}}
\citation{Fazel01a}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}The Proposed Model: Exploiting Noisy Side Information}{7}{subsection.10}}
\newlabel{subsec:propose}{{2.3}{7}{}{subsection.10}{}}
\newlabel{eq:general_form}{{4}{7}{}{equation.11}{}}
\newlabel{eq:PCPNF_part}{{5}{7}{}{equation.12}{}}
\citation{Jain13b,Xu13a}
\citation{Candes11a}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Connections to models for matrix completion}{8}{subsubsection.13}}
\newlabel{eq:IMCNF}{{6}{8}{}{equation.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Connections to models for robust PCA}{8}{subsubsection.15}}
\newlabel{eq:PCPNF}{{7}{8}{}{equation.16}{}}
\citation{Candes11a}
\citation{Jain13a}
\citation{Liu13a}
\citation{Candes11a}
\citation{tseng2001convergence}
\citation{tseng2001convergence}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Settings of several low-rank matrix learning models in the form of our proposed problem\nobreakspace  {}\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:general_form}\unskip \@@italiccorr )}}.\relax }}{9}{table.caption.18}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:general_form}{{1}{9}{Settings of several low-rank matrix learning models in the form of our proposed problem~\eqref {eq:general_form}.\relax }{table.caption.18}{}}
\newlabel{eq:PCPF}{{8}{9}{}{equation.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Optimization}{9}{subsection.20}}
\newlabel{subsec:optimization}{{2.4}{9}{}{subsection.20}{}}
\citation{Cai10a}
\citation{Hsieh14a}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Alternative Minimization for Problem\nobreakspace  {}\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:general_form}\unskip \@@italiccorr )}} with Squared Loss\relax }}{10}{algocf.24}}
\newlabel{alg:general_form}{{1}{10}{}{algocf.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Theoretical Analysis on the Effect of Side Information}{10}{section.25}}
\newlabel{sec:theory}{{3}{10}{}{section.25}{}}
\citation{Recht11a,Shamir14a}
\citation{Candes09a,Candes12a}
\citation{Bartlett02a}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Generalization Bound of the Proposed Model}{11}{subsection.26}}
\newlabel{subsec:generalization}{{3.1}{11}{}{subsection.26}{}}
\newlabel{eq:general_form_hard}{{9}{11}{}{equation.27}{}}
\newlabel{lemma:expect_l_risk_2}{{2}{11}{Bound on Expected $\ell $-risk,~\citealp {Bartlett02a}}{theorem.29}{}}
\newlabel{lemma:rad_bound1}{{3}{12}{}{theorem.30}{}}
\newlabel{eq:feature_quality}{{10}{12}{}{equation.31}{}}
\newlabel{lemma:trM_bound}{{4}{12}{}{theorem.32}{}}
\citation{Xu13a,Zhong15a}
\newlabel{thm:gen_bound}{{5}{13}{}{theorem.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Sample Complexity for Matrix Completion}{13}{subsection.34}}
\newlabel{subsec:theory.mc}{{3.2}{13}{}{subsection.34}{}}
\newlabel{thm:sample_comp_mc}{{6}{13}{}{theorem.35}{}}
\citation{Srebro05a,SN12a}
\citation{Candes12a}
\citation{Shamir14a}
\citation{Candes12a}
\newlabel{thm:example0}{{7}{14}{}{theorem.36}{}}
\citation{Candes09a,Candes12a}
\citation{VC11a,Candes11a}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Sample Complexity given Partial and Corrupted Observations}{15}{subsection.37}}
\newlabel{subsec:theory.gen}{{3.3}{15}{}{subsection.37}{}}
\newlabel{thm:sample_comp_gen}{{8}{15}{}{theorem.38}{}}
\@writefile{toc}{\contentsline {paragraph}{Remark.}{15}{theorem.38}}
\citation{Chiang16a}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Results}{16}{section.39}}
\newlabel{sec:exp}{{4}{16}{}{section.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Synthetic Experiments}{16}{subsection.47}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Experiments on Matrix Completion Setting}{16}{subsubsection.48}}
\newlabel{subsec:exp.mc}{{4.1.1}{16}{}{subsubsection.48}{}}
\newlabel{fig:s1}{{1a}{17}{Subfigure 1a}{subfigure.41}{}}
\newlabel{sub@fig:s1}{{(a)}{a}{Subfigure 1a\relax }{subfigure.41}{}}
\newlabel{fig:s2}{{1b}{17}{Subfigure 1b}{subfigure.42}{}}
\newlabel{sub@fig:s2}{{(b)}{b}{Subfigure 1b\relax }{subfigure.42}{}}
\newlabel{fig:s3}{{1c}{17}{Subfigure 1c}{subfigure.43}{}}
\newlabel{sub@fig:s3}{{(c)}{c}{Subfigure 1c\relax }{subfigure.43}{}}
\newlabel{fig:f1}{{1d}{17}{Subfigure 1d}{subfigure.44}{}}
\newlabel{sub@fig:f1}{{(d)}{d}{Subfigure 1d\relax }{subfigure.44}{}}
\newlabel{fig:f2}{{1e}{17}{Subfigure 1e}{subfigure.45}{}}
\newlabel{sub@fig:f2}{{(e)}{e}{Subfigure 1e\relax }{subfigure.45}{}}
\newlabel{fig:f3}{{1f}{17}{Subfigure 1f}{subfigure.46}{}}
\newlabel{sub@fig:f3}{{(f)}{f}{Subfigure 1f\relax }{subfigure.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Performance of various methods for matrix completion under certain fixed sparsity of observations $\rho _{obs}$ (upper figures) and fixed feature quality $\rho _f$ (lower figures). We observe that all feature-based methods perform better than standard matrix completion (MC) given perfect features ($\rho _f = 0$). However, IMCNF is less sensitive to feature noise as $\rho _f$ increases, indicating that it better exploits information from noisy features.\relax }}{17}{figure.caption.40}}
\newlabel{fig:mc_synthetic}{{1}{17}{Performance of various methods for matrix completion under certain fixed sparsity of observations $\rho _{obs}$ (upper figures) and fixed feature quality $\rho _f$ (lower figures). We observe that all feature-based methods perform better than standard matrix completion (MC) given perfect features ($\rho _f = 0$). However, IMCNF is less sensitive to feature noise as $\rho _f$ increases, indicating that it better exploits information from noisy features.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\rho _{obs} = 0.1$}}}{17}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\rho _{obs} = 0.25$}}}{17}{subfigure.1.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\rho _{obs} = 0.4$}}}{17}{subfigure.1.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\rho _f = 0.1$}}}{17}{subfigure.1.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {$\rho _f = 0.5$}}}{17}{subfigure.1.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {$\rho _f = 0.9$}}}{17}{subfigure.1.6}}
\citation{Jain13b}
\citation{Chen12a}
\citation{Candes11a}
\newlabel{eq:relative_error}{{11}{18}{}{equation.49}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Experiments on Robust PCA Setting}{18}{subsubsection.50}}
\newlabel{fig:rhos_01}{{2a}{19}{Subfigure 2a}{subfigure.52}{}}
\newlabel{sub@fig:rhos_01}{{(a)}{a}{Subfigure 2a\relax }{subfigure.52}{}}
\newlabel{fig:rhos_02}{{2b}{19}{Subfigure 2b}{subfigure.53}{}}
\newlabel{sub@fig:rhos_02}{{(b)}{b}{Subfigure 2b\relax }{subfigure.53}{}}
\newlabel{fig:rhos_03}{{2c}{19}{Subfigure 2c}{subfigure.54}{}}
\newlabel{sub@fig:rhos_03}{{(c)}{c}{Subfigure 2c\relax }{subfigure.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Performance of various methods for robust PCA given different feature noise level $\rho _f$ and sparsity of corruption $\rho _s$. These results show that PCPNF can make use of noisy yet informative features for better recovery. \relax }}{19}{figure.caption.51}}
\newlabel{fig:PCPNF_synthetic}{{2}{19}{Performance of various methods for robust PCA given different feature noise level $\rho _f$ and sparsity of corruption $\rho _s$. These results show that PCPNF can make use of noisy yet informative features for better recovery. \relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\rho _s = 0.1$}}}{19}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\rho _s = 0.2$}}}{19}{subfigure.2.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\rho _s = 0.3$}}}{19}{subfigure.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Experiments on Learning with Missing and Corrupted Observations}{19}{subsubsection.55}}
\citation{Massa06a}
\citation{Leskovec10a}
\citation{Hsieh12a,Chiang14a}
\newlabel{fig:MC_10}{{3a}{20}{Subfigure 3a}{subfigure.57}{}}
\newlabel{sub@fig:MC_10}{{(a)}{a}{Subfigure 3a\relax }{subfigure.57}{}}
\newlabel{fig:MC_07}{{3b}{20}{Subfigure 3b}{subfigure.58}{}}
\newlabel{sub@fig:MC_07}{{(b)}{b}{Subfigure 3b\relax }{subfigure.58}{}}
\newlabel{fig:MC_05}{{3c}{20}{Subfigure 3c}{subfigure.59}{}}
\newlabel{sub@fig:MC_05}{{(c)}{c}{Subfigure 3c\relax }{subfigure.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance of PCP-part and PCPF-part with perfect features for recovering $L_0$ from missing and corrupted observations (controlled by $\rho _{obs}$ and $\rho _s$ respectively). Both methods achieve recovery in white region and fail in black region, yet there is a gray region where only PCPF-part achieves recovery. This shows that by leveraging perfect features, PCPF-part can recover a much larger class of $L_0$ given both missing and corrupted observations are present. \relax }}{20}{figure.caption.56}}
\newlabel{fig:PCPF_part_synthetic}{{3}{20}{Performance of PCP-part and PCPF-part with perfect features for recovering $\realL $ from missing and corrupted observations (controlled by $\rho _{obs}$ and $\rho _s$ respectively). Both methods achieve recovery in white region and fail in black region, yet there is a gray region where only PCPF-part achieves recovery. This shows that by leveraging perfect features, PCPF-part can recover a much larger class of $\realL $ given both missing and corrupted observations are present. \relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\rho _{obs} = 1.0$}}}{20}{subfigure.3.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\rho _{obs} = 0.7$}}}{20}{subfigure.3.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\rho _{obs} = 0.5$}}}{20}{subfigure.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Real-world Applications}{20}{subsection.60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Relationship Prediction in Signed Networks}{20}{subsubsection.61}}
\newlabel{subsec:exp.sign_prediction}{{4.2.1}{20}{}{subsubsection.61}{}}
\citation{Chiang14a}
\citation{Hsieh12a}
\citation{Jain13a}
\citation{Chiang14a}
\citation{Chiang14a}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Relationship prediction on Epinions network. We see that given noisy user features, IMC performs worse even than methods without features (MF-ALS and HOCs), while IMCNF outperforms others by successfully exploiting noisy features.\relax }}{21}{table.caption.62}}
\newlabel{tab:sign_prediction}{{2}{21}{Relationship prediction on Epinions network. We see that given noisy user features, IMC performs worse even than methods without features (MF-ALS and HOCs), while IMCNF outperforms others by successfully exploiting noisy features.\relax }{table.caption.62}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Semi-supervised Clustering}{21}{subsubsection.63}}
\newlabel{subsec:exp.semi_clustering}{{4.2.2}{21}{}{subsubsection.63}{}}
\citation{Davis07a}
\citation{Li09a}
\citation{Yi13a}
\citation{Chiang14a}
\citation{Yi13a}
\citation{MT91a}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Statistics of semi-supervised clustering data sets.\relax }}{23}{table.caption.65}}
\newlabel{tab:semi_clustering}{{3}{23}{Statistics of semi-supervised clustering data sets.\relax }{table.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance of various semi-supervised clustering methods on real-world data sets. For the Mushrooms data set where features are perfect, both MCCC and IMCNF can output the ground-truth clustering with $0$ error rate. For Segment and Covtype where features are more noisy, IMCNF model outperforms MCCC as its error decreases given more constraints.\relax }}{23}{figure.caption.66}}
\newlabel{fig:semi_clustering}{{4}{23}{Performance of various semi-supervised clustering methods on real-world data sets. For the Mushrooms data set where features are perfect, both MCCC and IMCNF can output the ground-truth clustering with $0$ error rate. For Segment and Covtype where features are more noisy, IMCNF model outperforms MCCC as its error decreases given more constraints.\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Noisy Image Classification}{23}{subsubsection.67}}
\newlabel{sec:exp.noisy_classification}{{4.2.3}{23}{}{subsubsection.67}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Digit classification accuracy of PCP and PCPF with Eigendigit features. The column Clean shows the accuracy on $L_0$ and the column Noisy shows the accuracy on $R$. Denoised images from both PCP and PCPF achieve much higher accuracy than noisy images, and PCPF further outperforms PCP by incorporating Eigendigit features.\relax }}{24}{table.caption.69}}
\newlabel{tab:digit_classification}{{4}{24}{Digit classification accuracy of PCP and PCPF with Eigendigit features. The column Clean shows the accuracy on $\realL $ and the column Noisy shows the accuracy on $R$. Denoised images from both PCP and PCPF achieve much higher accuracy than noisy images, and PCPF further outperforms PCP by incorporating Eigendigit features.\relax }{table.caption.69}{}}
\@writefile{toc}{\contentsline {paragraph}{Exploiting Eigendigit Features.}{24}{subsubsection.67}}
\@writefile{toc}{\contentsline {paragraph}{Exploiting both Eigendigit and Label-relevant Features}{24}{figure.caption.70}}
\newlabel{fig:digit_rhos_01}{{5a}{25}{Subfigure 5a}{subfigure.71}{}}
\newlabel{sub@fig:digit_rhos_01}{{(a)}{a}{Subfigure 5a\relax }{subfigure.71}{}}
\newlabel{fig:digit_rhos_02}{{5b}{25}{Subfigure 5b}{subfigure.72}{}}
\newlabel{sub@fig:digit_rhos_02}{{(b)}{b}{Subfigure 5b\relax }{subfigure.72}{}}
\newlabel{fig:digit_rhos_03}{{5c}{25}{Subfigure 5c}{subfigure.73}{}}
\newlabel{sub@fig:digit_rhos_03}{{(c)}{c}{Subfigure 5c\relax }{subfigure.73}{}}
\newlabel{fig:svm_digit_rhos_01}{{5d}{25}{Subfigure 5d}{subfigure.74}{}}
\newlabel{sub@fig:svm_digit_rhos_01}{{(d)}{d}{Subfigure 5d\relax }{subfigure.74}{}}
\newlabel{fig:svm_digit_rhos_02}{{5e}{25}{Subfigure 5e}{subfigure.75}{}}
\newlabel{sub@fig:svm_digit_rhos_02}{{(e)}{e}{Subfigure 5e\relax }{subfigure.75}{}}
\newlabel{fig:svm_digit_rhos_03}{{5f}{25}{Subfigure 5f}{subfigure.76}{}}
\newlabel{sub@fig:svm_digit_rhos_03}{{(f)}{f}{Subfigure 5f\relax }{subfigure.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Digit classification accuracy of various methods with both Eigendigit and label-relevant features. For each $\rho _s$, we construct the label-relevant features $Y$ with different quality by varying $\rho _f$. The results show that PCPNF-w/Y is able to better exploit noisy label-relevant features $Y$.\relax }}{25}{figure.caption.70}}
\newlabel{fig:digit_exp_pcpgf}{{5}{25}{Digit classification accuracy of various methods with both Eigendigit and label-relevant features. For each $\rho _s$, we construct the label-relevant features $Y$ with different quality by varying $\rho _f$. The results show that PCPNF-w/Y is able to better exploit noisy label-relevant features $Y$.\relax }{figure.caption.70}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Linear SVM, $\rho _s = 0.1$}}}{25}{subfigure.5.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Linear SVM, $\rho _s = 0.2$}}}{25}{subfigure.5.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Linear SVM, $\rho _s = 0.3$}}}{25}{subfigure.5.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Kernel SVM, $\rho _s = 0.1$}}}{25}{subfigure.5.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {Kernel SVM, $\rho _s = 0.2$}}}{25}{subfigure.5.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Kernel SVM, $\rho _s = 0.3$}}}{25}{subfigure.5.6}}
\citation{Harold33a}
\citation{Candes09a}
\citation{Zhong15a}
\citation{Wright09a}
\citation{Koren09a}
\citation{Hsieh12a,Chiang14a}
\citation{Chen14a}
\citation{Candes09a}
\citation{Candes12a}
\citation{SN12a}
\citation{Shamir14a}
\citation{RK10a,EC10b}
\citation{Menon11a,Chen12a,Natarajan14a,Shin15a}
\citation{Menon11a,Shin15a}
\citation{Natarajan14a}
\citation{Jain13a}
\citation{Xu13a,Zhong15a}
\citation{Wright09a}
\citation{VC11a,Candes11a}
\citation{Candes11a}
\citation{Xu10a}
\citation{Candes11a,Chen13a}
\citation{Ha15a}
\@writefile{toc}{\contentsline {section}{\numberline {5}Related Work}{26}{section.77}}
\newlabel{sec:related}{{5}{26}{}{section.77}{}}
\citation{Chiang16a}
\citation{Chiang16a}
\citation{Liu10a,Liu13a}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{27}{section.78}}
\newlabel{sec:conclusion}{{6}{27}{}{section.78}{}}
\citation{Hsieh15a}
\citation{SK08a}
\citation{RM03a}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proofs}{28}{section.79}}
\newlabel{lemma:rad_bound_nuclear}{{9}{28}{Preliminary Lemmas}{theorem.81}{}}
\newlabel{lemma:rad_bound_sparse}{{10}{28}{Preliminary Lemmas}{theorem.82}{}}
\citation{Shamir14a}
\citation{Shamir14a}
\citation{Shamir14a}
\newlabel{eq:first_rad_bound}{{12}{29}{Proof of Lemma~\ref {lemma:rad_bound1}}{equation.84}{}}
\newlabel{eq:separate_by_hit}{{13}{29}{Proof of Lemma~\ref {lemma:rad_bound1}}{equation.85}{}}
\citation{Shamir14a}
\newlabel{eq:lower_hit}{{14}{30}{Proof of Lemma~\ref {lemma:rad_bound1}}{equation.86}{}}
\newlabel{eq:second_rad_bound}{{15}{30}{Proof of Lemma~\ref {lemma:rad_bound1}}{equation.87}{}}
\newlabel{eq:trM_bound_tmp}{{16}{30}{Proof of Lemma~\ref {lemma:trM_bound}}{equation.89}{}}
\newlabel{eq:example0_feature}{{17}{31}{Proof of Theorem~\ref {thm:example0}}{equation.93}{}}
\bibdata{17-112}
\bibcite{Bartlett02a}{{1}{2003}{{Bartlett and Mendelson}}{{}}}
\bibcite{Cai10a}{{2}{2010}{{Cai et~al.}}{{Cai, Cand\`{e}s, and Shen}}}
\bibcite{EC10b}{{3}{2010}{{Cand\`{e}s and Plan}}{{}}}
\bibcite{Candes12a}{{4}{2012}{{Cand\`{e}s and Recht}}{{}}}
\bibcite{Candes09a}{{5}{2009}{{Cand\`{e}s and Tao}}{{}}}
\bibcite{Candes11a}{{6}{2011}{{Cand\`{e}s et~al.}}{{Cand\`{e}s, Li, Ma, and Wright}}}
\bibcite{VC11a}{{7}{2011}{{Chandrasekaran et~al.}}{{Chandrasekaran, Sanghavi, Parrilo, and Willsky}}}
\bibcite{Chen12a}{{8}{2012}{{Chen et~al.}}{{Chen, Zhang, Lu, Chen, Zheng, and Yu}}}
\bibcite{Chen13a}{{9}{2013}{{Chen et~al.}}{{Chen, Jalali, Sanghavi, and Caramanis}}}
\bibcite{Chen14a}{{10}{2014}{{Chen et~al.}}{{Chen, Jalali, Sanghavi, and Xu}}}
\bibcite{Chiang14a}{{11}{2014}{{Chiang et~al.}}{{Chiang, Hsieh, Natarajan, Dhillon, and Tewari}}}
\bibcite{Chiang15a}{{12}{2015}{{Chiang et~al.}}{{Chiang, Hsieh, and Dhillon}}}
\bibcite{Chiang16a}{{13}{2016}{{Chiang et~al.}}{{Chiang, Hsieh, and Dhillon}}}
\bibcite{Davis07a}{{14}{2007}{{Davis et~al.}}{{Davis, Kulis, Jain, Sra, and Dhillon}}}
\bibcite{Fazel01a}{{15}{2001}{{Fazel et~al.}}{{Fazel, Hindi, and Boyd}}}
\bibcite{Ha15a}{{16}{2015}{{Ha and Barber}}{{}}}
\bibcite{Harold33a}{{17}{1933}{{Hotelling}}{{}}}
\bibcite{Hsieh14a}{{18}{2014}{{Hsieh and Olsan}}{{}}}
\bibcite{Hsieh12a}{{19}{2012}{{Hsieh et~al.}}{{Hsieh, Chiang, and Dhillon}}}
\bibcite{Hsieh15a}{{20}{2015}{{Hsieh et~al.}}{{Hsieh, Natarajan, and Dhillon}}}
\bibcite{Jain13b}{{21}{2013}{{Jain and Dhillon}}{{}}}
\bibcite{Jain13a}{{22}{2013}{{Jain et~al.}}{{Jain, Netrapalli, and Sanghavi}}}
\bibcite{SK08a}{{23}{2008}{{Kakade et~al.}}{{Kakade, Sridharan, and Tewari}}}
\bibcite{RK10a}{{24}{2010}{{Keshavan et~al.}}{{Keshavan, Montanari, and Oh}}}
\bibcite{Koren09a}{{25}{2009}{{Koren et~al.}}{{Koren, Bell, and Volinsky}}}
\bibcite{Leskovec10a}{{26}{2010}{{Leskovec et~al.}}{{Leskovec, Huttenlocher, and Kleinberg}}}
\bibcite{Li09a}{{27}{2009}{{Li and Liu}}{{}}}
\bibcite{Nowell07a}{{28}{2007}{{Liben-Nowell and Kleinberg}}{{}}}
\bibcite{Liu10a}{{29}{2010}{{Liu et~al.}}{{Liu, Lin, and Yu}}}
\bibcite{Liu13a}{{30}{2013}{{Liu et~al.}}{{Liu, Lin, Yan, Sun, Yu, and Ma}}}
\bibcite{Massa06a}{{31}{2006}{{Massa and Avesani}}{{}}}
\bibcite{RM03a}{{32}{2003}{{Meir and Zhang}}{{}}}
\bibcite{Menon11a}{{33}{2011}{{Menon and Elkan}}{{}}}
\bibcite{Natarajan14a}{{34}{2014}{{Natarajan and Dhillon}}{{}}}
\bibcite{SN12a}{{35}{2012}{{Negahban and Wainwright}}{{}}}
\bibcite{Recht11a}{{36}{2011}{{Recht}}{{}}}
\bibcite{Shamir14a}{{37}{2014}{{Shamir and Shalev-Shwartz}}{{}}}
\bibcite{Shin15a}{{38}{2015}{{Shin et~al.}}{{Shin, Cetintas, Lee, and Dhillon}}}
\bibcite{Srebro05a}{{39}{2005}{{Srebro and Shraibman}}{{}}}
\bibcite{tseng2001convergence}{{40}{2001}{{Tseng}}{{}}}
\bibcite{MT91a}{{41}{1991}{{Turk and Pentland}}{{}}}
\bibcite{Wright09a}{{42}{2009}{{Wright et~al.}}{{Wright, Ganesh, Rao, Peng, and Ma}}}
\bibcite{Xu10a}{{43}{2010}{{Xu et~al.}}{{Xu, Caramanis, and Sanghavi}}}
\bibcite{Xu13a}{{44}{2013}{{Xu et~al.}}{{Xu, Jin, and Zhou}}}
\bibcite{Yi13a}{{45}{2013}{{Yi et~al.}}{{Yi, Zhang, Jin, Qian, and Jain}}}
\bibcite{Zhong15a}{{46}{2015}{{Zhong et~al.}}{{Zhong, Jain, and Dhillon}}}
\newlabel{LastPage}{{}{35}{}{page.35}{}}
\xdef\lastpage@lastpage{35}
\xdef\lastpage@lastpageHy{35}
