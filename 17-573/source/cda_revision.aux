\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{ngai2011application}
\citation{wakefield2007disease}
\citation{wang2010click}
\citation{minsker2014robust,maclaurin2014firefly,srivastava2015wasp,conrad2015accelerating}
\citation{albert1993bayesian}
\citation{polson2013bayesian}
\citation{johndrow2016inefficiency}
\citation{liu1999parameter,meng1999seeking,papaspiliopoulos2007general}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\citation{tran2016adaptive}
\@writefile{toc}{\contentsline {section}{\numberline {2}Calibrated Data Augmentation}{3}{section.2}}
\newlabel{sec:cda}{{2}{3}{}{section.2}{}}
\newlabel{eq:da}{{1}{3}{}{equation.2.1}{}}
\citation{papaspiliopoulos2007general}
\citation{rubin2004multiple}
\citation{liu1994fraction}
\citation{liu1994fraction}
\newlabel{eq:missinginfo}{{2}{4}{}{equation.2.2}{}}
\newlabel{eq:Q}{{3}{4}{}{equation.2.3}{}}
\citation{roberts1994simple}
\citation{tanner1987calculation,albert1993bayesian}
\citation{johndrow2016inefficiency}
\newlabel{rem:accrat}{{1}{5}{}{theorem.1}{}}
\newlabel{eq:MH-accrat}{{4}{5}{}{equation.2.4}{}}
\newlabel{rem:ergodic}{{2}{5}{Ergodicity}{theorem.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Initial Example: Probit with Intercept Only}{5}{subsection.2.1}}
\newlabel{eq:prop-marginal-probit-intercept}{{5}{6}{Initial Example: Probit with Intercept Only}{equation.2.5}{}}
\newlabel{eq:cda-probit-intercept}{{6}{6}{Initial Example: Probit with Intercept Only}{equation.2.6}{}}
\citation{tanner1987calculation,albert1993bayesian}
\citation{liu1999parameter}
\citation{meng1999seeking}
\newlabel{probit_demo_intercept_proposal}{{1(a)}{7}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@probit_demo_intercept_proposal}{{(a)}{7}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{probit_demo_intercept_density}{{1(b)}{7}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{sub@probit_demo_intercept_density}{{(b)}{7}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\newlabel{probit_demo_intercept}{{2.1}{7}{Initial Example: Probit with Intercept Only}{subfigure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Autocorrelation functions (ACFs) and kernel-smoothed density estimates for different CDA samplers in intercept-only probit model.}}{7}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ACF for CDA without MH adjustment.}}}{7}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Posterior density estimates without MH adjustment.}}}{7}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ACF for CDA with MH adjustment}}}{7}{figure.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Specific Algorithms}{7}{section.3}}
\newlabel{sec:algos}{{3}{7}{Initial Example: Probit with Intercept Only}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Probit Regression}{7}{subsection.3.1}}
\newlabel{probit_reg_model}{{3.1}{7}{Probit Regression}{subsection.3.1}{}}
\newlabel{eq:cda-probit}{{7}{7}{Probit Regression}{equation.3.7}{}}
\citation{albert1993bayesian}
\citation{liu1999parameter}
\citation{albert1993bayesian}
\citation{liu1999parameter}
\citation{albert1993bayesian}
\citation{liu1999parameter}
\newlabel{eq:prop-marginal-probit}{{8}{8}{Probit Regression}{equation.3.8}{}}
\newlabel{eq:varlb-probit}{{3.1}{8}{Probit Regression}{equation.3.8}{}}
\citation{polson2013bayesian}
\newlabel{probit_reg_trace}{{2(a)}{9}{Subfigure 2(a)}{subfigure.2.1}{}}
\newlabel{sub@probit_reg_trace}{{(a)}{9}{Subfigure 2(a)\relax }{subfigure.2.1}{}}
\newlabel{probit_reg_acf}{{2(b)}{9}{Subfigure 2(b)}{subfigure.2.2}{}}
\newlabel{sub@probit_reg_acf}{{(b)}{9}{Subfigure 2(b)\relax }{subfigure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Panel (a) demonstrates in traceplot and panel (b) in autocorrelation the substantial improvement in CDA by correcting the variance mis-match in probit regression with rare event data, compared with the original \citep  {albert1993bayesian} and parameter-expanded methods \citep  {liu1999parameter}.}}{9}{figure.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Traceplot for the original DA, parameter expanded DA and CDA algorithms.}}}{9}{figure.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ACF for original DA, parameter expanded DA and CDA algorithms.}}}{9}{figure.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Logistic Regression}{9}{subsection.3.2}}
\newlabel{logit_regression}{{9}{9}{Logistic Regression}{equation.3.9}{}}
\newlabel{eq:prop-marginal-logit}{{10}{9}{Logistic Regression}{equation.3.10}{}}
\citation{polson2013bayesian}
\citation{polson2013bayesian}
\citation{efron1978assessing}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Panel (a) demonstrates in traceplot and panel (b) in autocorrelation the substantial improvement of CDA in logistic regression with rare event data, compared with the original DA \citep  {polson2013bayesian}.}}{10}{figure.3}}
\newlabel{logit_random_mixing}{{3}{10}{Panel (a) demonstrates in traceplot and panel (b) in autocorrelation the substantial improvement of CDA in logistic regression with rare event data, compared with the original DA \citep {polson2013bayesian}}{figure.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Traceplots for DA and CDA.}}}{10}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ACF for DA and CDA.}}}{10}{figure.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}{Automatic Tuning of Calibration Parameters}}{10}{section.4}}
\newlabel{sec:tuning}{{4}{10}{Logistic Regression}{section.4}{}}
\newlabel{variance_dist}{{11}{11}{Logistic Regression}{equation.4.11}{}}
\newlabel{acceptance_dist}{{12}{11}{Logistic Regression}{equation.4.12}{}}
\citation{johndrow2016inefficiency}
\citation{johndrow2016inefficiency}
\citation{johndrow2016inefficiency}
\citation{johndrow2016inefficiency}
\newlabel{tuning_objective_function}{{13}{12}{Logistic Regression}{equation.4.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Geometric convergence rates for CDA-MH and CDA-Gibbs}{12}{section.5}}
\citation{johndrow2016inefficiency}
\citation{korattikara2014austerity,quiroz2016exact,bardenet2017markov}
\citation{johndrow2015approximations,rudolf2018perturbation}
\citation{johndrow2015approximations}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Effective sample size (with 95\% pointwise confidence interval) per $1,000$ steps with different sample size $n$ from $10$ to $10^{14}$, using logistic regression model with intercept. }}{14}{figure.4}}
\newlabel{massive_n_sims}{{4}{14}{Effective sample size (with 95\% pointwise confidence interval) per $1,000$ steps with different sample size $n$ from $10$ to $10^{14}$, using logistic regression model with intercept}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Simulation Study}{14}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Comparison with Downsampling Algorithm}{14}{subsection.6.1}}
\newlabel{sec:down_sampling}{{6.1}{14}{Comparison with Downsampling Algorithm}{subsection.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparing the performance of CDA and DA, coupled with sub-sampling approximation to reduce the number of sampled latent variables. }}{15}{figure.5}}
\newlabel{simMassiveNSubsampling}{{5}{15}{Comparing the performance of CDA and DA, coupled with sub-sampling approximation to reduce the number of sampled latent variables}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Comparison with Independence Metropolis-Hastings}{15}{subsection.6.2}}
\citation{mengersen1996rates}
\newlabel{eq:density_ratio_indep_proposal}{{14}{16}{Comparison with Independence Metropolis-Hastings}{equation.6.14}{}}
\citation{marcus2011informatics}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparing the acceptance ratios using the multivariate $t$-distribution and CDA proposals in logistic regression, with variance fixed at the inverse Fisher information. CDA has a much higher acceptance ratio than the multivariate $t$ proposal.}}{17}{figure.6}}
\newlabel{acceptance_rate_tail}{{6}{17}{Comparing the acceptance ratios using the multivariate $t$-distribution and CDA proposals in logistic regression, with variance fixed at the inverse Fisher information. CDA has a much higher acceptance ratio than the multivariate $t$ proposal}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Data Applications}{17}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}{Bernoulli Latent Factor Model with Group Intercepts for Network Modeling}}{17}{subsection.7.1}}
\citation{hoff2009simulation}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces ACFs show the mixing performance of $\beta _0$ and $\beta _1$ in modeling average sparsity in network connectivity of a brain. }}{18}{figure.7}}
\newlabel{fig:network_model}{{7}{18}{ACFs show the mixing performance of $\beta _0$ and $\beta _1$ in modeling average sparsity in network connectivity of a brain}{figure.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ACFs of the parameters $\beta _0$ and $\beta _1$ using DA.}}}{18}{figure.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ACFs of the parameters $\beta _0$ and $\beta _1$ using CDA.}}}{18}{figure.7}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Parameter estimates and computing speed of DA and CDA in Bernoulli latent factor modeling of a brain network.}}{19}{table.1}}
\newlabel{table:network}{{1}{19}{Parameter estimates and computing speed of DA and CDA in Bernoulli latent factor modeling of a brain network}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Poisson Log-Normal Model for Web Traffic Prediction}{19}{subsection.7.2}}
\citation{liu1994collapsed}
\citation{zhou2012lognormal}
\newlabel{eq:pos_approx}{{15}{20}{Poisson Log-Normal Model for Web Traffic Prediction}{equation.7.15}{}}
\citation{papaspiliopoulos2007general}
\citation{liu1999parameter}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces CDA significantly improves the mixing of the parameters in the Poisson log-normal. }}{22}{figure.8}}
\newlabel{data_poisson}{{8}{22}{CDA significantly improves the mixing of the parameters in the Poisson log-normal}{figure.8}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Autocorrelation of the parameters from DA.}}}{22}{figure.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Autocorrelation of the parameters from CDA.}}}{22}{figure.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Autocorrelation of the parameters from HMC.}}}{22}{figure.8}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Parameter estimates, prediction error and computing speed of the DA, CDA and HMC in Poisson regression model.}}{22}{table.2}}
\newlabel{table:Poisson}{{2}{22}{Parameter estimates, prediction error and computing speed of the DA, CDA and HMC in Poisson regression model}{table.2}{}}
\citation{tran2016adaptive}
\@writefile{toc}{\contentsline {section}{\numberline {8}Discussion}{23}{section.8}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proof of Remark \ref  {rem:accrat}}{23}{section.1}}
\citation{roberts1994simple}
\citation{roberts1994simple}
\citation{liu1999parameter}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proof of Remark \ref  {rem:ergodic}}{24}{section.2}}
\newlabel{gaussian_example}{{C}{24}{Poisson Log-Normal Model for Web Traffic Prediction}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Toy example: Hierarchical Normal }{24}{section.3}}
\newlabel{hier_normal_model}{{16}{24}{Poisson Log-Normal Model for Web Traffic Prediction}{equation.3.16}{}}
\newlabel{hier_model_proposal}{{17}{25}{Poisson Log-Normal Model for Web Traffic Prediction}{equation.3.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Trace and autocorrelation plots for DA and CDA in hierarchical normal model. }}{25}{figure.9}}
\newlabel{plot_hierarchical_normal}{{9}{25}{Trace and autocorrelation plots for DA and CDA in hierarchical normal model}{figure.9}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Traceplot for DA and CDA.}}}{25}{figure.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ACF for DA and CDA.}}}{25}{figure.9}}
\citation{johndrow2015approximations}
\@writefile{toc}{\contentsline {section}{\numberline {D}Calibrated Polya-Gamma Algorithm with Sub-sampling}{26}{section.4}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Proof of Theorem 1}{26}{section.5}}
\newlabel{eq:LB1}{{18}{27}{Poisson Log-Normal Model for Web Traffic Prediction}{equation.5.18}{}}
\bibdata{reference}
\bibcite{albert1993bayesian}{{1}{1993}{{Albert and Chib}}{{}}}
\bibcite{bardenet2017markov}{{2}{2017}{{Bardenet et~al.}}{{Bardenet, Doucet, and Holmes}}}
\bibcite{conrad2015accelerating}{{3}{2016}{{Conrad et~al.}}{{Conrad, Marzouk, Pillai, and Smith}}}
\bibcite{efron1978assessing}{{4}{1978}{{Efron and Hinkley}}{{}}}
\bibcite{hoff2009simulation}{{5}{2009}{{Hoff}}{{}}}
\bibcite{johndrow2015approximations}{{6}{2017}{{Johndrow et~al.}}{{Johndrow, Mattingly, Mukherjee, and Dunson}}}
\bibcite{johndrow2016inefficiency}{{7}{2018}{{Johndrow et~al.}}{{Johndrow, Smith, Pillai, and Dunson}}}
\bibcite{korattikara2014austerity}{{8}{2014}{{Korattikara et~al.}}{{Korattikara, Chen, and Welling}}}
\bibcite{liu1994collapsed}{{9}{1994{a}}{{Liu}}{{}}}
\bibcite{liu1994fraction}{{10}{1994{b}}{{Liu}}{{}}}
\bibcite{liu1999parameter}{{11}{1999}{{Liu and Wu}}{{}}}
\bibcite{maclaurin2014firefly}{{12}{2015}{{Maclaurin and Adams}}{{}}}
\bibcite{marcus2011informatics}{{13}{2011}{{Marcus et~al.}}{{Marcus, Harwell, Olsen, Hodge, Glasser, Prior, Jenkinson, Laumann, Curtiss, and Van~Essen}}}
\bibcite{meng1999seeking}{{14}{1999}{{Meng and Van~Dyk}}{{}}}
\bibcite{mengersen1996rates}{{15}{1996}{{Mengersen et~al.}}{{Mengersen, Tweedie, et~al.}}}
\bibcite{minsker2014robust}{{16}{2017}{{Minsker et~al.}}{{Minsker, Srivastava, Lin, and Dunson}}}
\bibcite{ngai2011application}{{17}{2011}{{Ngai et~al.}}{{Ngai, Hu, Wong, Chen, and Sun}}}
\bibcite{papaspiliopoulos2007general}{{18}{2007}{{Papaspiliopoulos et~al.}}{{Papaspiliopoulos, Roberts, and Sk{\"o}ld}}}
\bibcite{polson2013bayesian}{{19}{2013}{{Polson et~al.}}{{Polson, Scott, and Windle}}}
\bibcite{quiroz2016exact}{{20}{2018}{{Quiroz et~al.}}{{Quiroz, Kohn, Villani, and Tran}}}
\bibcite{roberts1994simple}{{21}{1994}{{Roberts and Smith}}{{}}}
\bibcite{rubin2004multiple}{{22}{2004}{{Rubin}}{{}}}
\bibcite{rudolf2018perturbation}{{23}{2018}{{Rudolf et~al.}}{{Rudolf, Schweizer, et~al.}}}
\bibcite{srivastava2015wasp}{{24}{2015}{{Srivastava et~al.}}{{Srivastava, Cevher, Tran-Dinh, and Dunson}}}
\bibcite{tanner1987calculation}{{25}{1987}{{Tanner and Wong}}{{}}}
\bibcite{tran2016adaptive}{{26}{2016}{{Tran et~al.}}{{Tran, Pitt, and Kohn}}}
\bibcite{wakefield2007disease}{{27}{2007}{{Wakefield}}{{}}}
\bibcite{wang2010click}{{28}{2010}{{Wang et~al.}}{{Wang, Li, Cui, Zhang, and Mao}}}
\bibcite{zhou2012lognormal}{{29}{2012}{{Zhou et~al.}}{{Zhou, Li, Dunson, and Carin}}}
\newlabel{LastPage}{{}{34}{}{page.34}{}}
\xdef\lastpage@lastpage{34}
\xdef\lastpage@lastpageHy{34}
